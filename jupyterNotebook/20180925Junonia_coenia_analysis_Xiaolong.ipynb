{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20180925 Junonia coenia analysis 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finish testing some programs, some more analysis.\n",
    "\n",
    "Plans:\n",
    "* gene trees\n",
    "* Structure\n",
    "* whole genome tree with 100 segments\n",
    "* Densitree\n",
    "* Protein correlates of antenna (Fst test, by trees of genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: most of the code will not be run within this notebook.\n",
    "\n",
    "frequently used constents and scripts\n",
    "```\n",
    "file_genome = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.scaffolds.fa'\n",
    "file_gff3 = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.gff3'\n",
    "file_detail = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.position.base.scf.scfpos.gene.rna.exon.cds'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check existence of files related with Junonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totally there are 308 samples sequenced. Time to fix this part and get all required data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sampe_prefix\n",
    "Totally 308 for 308 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique sample prefix 308\n"
     ]
    }
   ],
   "source": [
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "import pandas as pd\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check file existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of fastq files is 24045\n",
      "number of Junonia samples with fastq is 308\n",
      "[] is missing\n"
     ]
    }
   ],
   "source": [
    "l_junoniaPrefixes = sample_prefix\n",
    "import glob\n",
    "files_fastqs = glob.glob('/archive/butterfly/ready_fastq/*.*')\n",
    "print('total number of fastq files is',len(files_fastqs))\n",
    "files_fastqs = set(files_fastqs)\n",
    "l_junonia_withfastq = [e for e in l_junoniaPrefixes \\\n",
    "                       if '/archive/butterfly/ready_fastq/'+e+'_R1.fastq' in files_fastqs and \\\n",
    "                      '/archive/butterfly/ready_fastq/'+e+'_R2.fastq' in files_fastqs]\n",
    "print('number of Junonia samples with fastq is', len(l_junonia_withfastq))\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                       if '/archive/butterfly/ready_fastq/'+e+'_R1.fastq' not in files_fastqs or \\\n",
    "                      '/archive/butterfly/ready_fastq/'+e+'_R2.fastq' not in files_fastqs],'is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/SNP_results/debiased/7888_Lerema_accius_assembly_V1.1_withMito/', '/archive/butterfly/SNP_results/debiased/7412_Lerema_accius_assembly_V1.1_withMito/']\n",
      "number of vfc folders 7684\n",
      "number of Junonia samples with VFC is 308\n",
      "the following Junonia sample do not have a VFC file against Junonia_coenia_JC_v1.0.scaffolds\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "folders_vfc = glob.glob('/archive/butterfly/SNP_results/debiased/*/')\n",
    "print(folders_vfc[:2])\n",
    "folders_vfc = set(folders_vfc)\n",
    "print('number of vfc folders',len(folders_vfc))\n",
    "l_junoia_withVFC = [e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/SNP_results/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds/' in folders_vfc]\n",
    "print('number of Junonia samples with VFC is', len(l_junoia_withVFC))\n",
    "print('the following Junonia sample do not have a VFC file against Junonia_coenia_JC_v1.0.scaffolds')\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/SNP_results/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds/' not in folders_vfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/maps/debiased/3284_pxu_genome_snp_step2.map', '/archive/butterfly/maps/debiased/15109G07_3574_assembly_v1_withMito_snp_step2.map']\n",
      "number of map files 13333\n",
      "number of Junonia samples with Map file is 308\n",
      "the following Junonia sample do not have a map file against Junonia_coenia_JC_v1.0.scaffolds\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files_map = glob.glob('/archive/butterfly/maps/debiased/*.map')\n",
    "print(files_map[:2])\n",
    "files_map = set(files_map)\n",
    "print('number of map files',len(files_map))\n",
    "l_junoia_withMap = ['/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'\\\n",
    "                    for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' in files_map]\n",
    "print('number of Junonia samples with Map file is', len(l_junoia_withMap))\n",
    "print('the following Junonia sample do not have a map file against Junonia_coenia_JC_v1.0.scaffolds')\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' not in files_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/unbias_pipeline_info/step4_postprocessing/map2fasta/15101E01_phoebis_assembly_V1_snp_step2_m2s.fa', '/archive/butterfly/unbias_pipeline_info/step4_postprocessing/map2fasta/17114A12_Lerema_accius_assembly_V1.1_withMito_snp_step2_m2s.fa']\n",
      "number of map files 13531\n",
      "number of Junonia samples with fa file is 308\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files_fa = glob.glob('/archive/butterfly/unbias_pipeline_info/step4_postprocessing/map2fasta/*.fa')\n",
    "print(files_fa[:2])\n",
    "files_fa = set(files_fa)\n",
    "print('number of map files',len(files_fa))\n",
    "l_junoia_withfa = [e for e in  l_junoniaPrefixes\\\n",
    "                   if '/archive/butterfly/unbias_pipeline_info/step4_postprocessing/map2fasta/'\\\n",
    "                   +e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2_m2s.fa' in files_fa]\n",
    "print('number of Junonia samples with fa file is', len(l_junoia_withfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/6256_Calycopis_cecrops_assembly_V1.1_withMito_stat.report', '/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/5732_3614_assembly_v1_mitogenome_stat.report']\n",
      "number of files 7291\n",
      "number of Junonia samples with file is 308\n",
      "the following Junonia sample do not have a file against Junonia_coenia_JC_v1.0.scaffolds\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/*.report')\n",
    "print(files[:2])\n",
    "files = set(files)\n",
    "print('number of files',len(files))\n",
    "l_junoia_with_file = ['/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/'+e+\\\n",
    "                      '_Junonia_coenia_JC_v1.0.scaffolds_stat.report' for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/'+e+\\\n",
    "                      '_Junonia_coenia_JC_v1.0.scaffolds_stat.report' in files]\n",
    "print('number of Junonia samples with file is', len(l_junoia_with_file))\n",
    "print('the following Junonia sample do not have a file against Junonia_coenia_JC_v1.0.scaffolds')\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/'+e+\\\n",
    "                      '_Junonia_coenia_JC_v1.0.scaffolds_stat.report' not in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files to a folder\n",
    "import os\n",
    "for f in l_junoia_with_file:\n",
    "    os.system('mv '+f+' /home/xcao/w/20180905Junonia_coenia/20180919Info/alignment_final_stats/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summarize coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>reference</th>\n",
       "      <th>data_amount</th>\n",
       "      <th>map_read%(byReads)</th>\n",
       "      <th>map_read%(halfmap)</th>\n",
       "      <th>map_read%(byPosition)</th>\n",
       "      <th>expected_coverage</th>\n",
       "      <th>genome_coverage</th>\n",
       "      <th>coverage_mean</th>\n",
       "      <th>coverage_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S5473</td>\n",
       "      <td>Junonia_coenia_JC_v1.0.scaffolds</td>\n",
       "      <td>4.14Gbp</td>\n",
       "      <td>12.82%</td>\n",
       "      <td>3.50%</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.59%</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S5744</td>\n",
       "      <td>Junonia_coenia_JC_v1.0.scaffolds</td>\n",
       "      <td>2.04Gbp</td>\n",
       "      <td>50.87%</td>\n",
       "      <td>30.14%</td>\n",
       "      <td>27.73%</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.35%</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S5647</td>\n",
       "      <td>Junonia_coenia_JC_v1.0.scaffolds</td>\n",
       "      <td>9.41Mbp</td>\n",
       "      <td>35.20%</td>\n",
       "      <td>27.11%</td>\n",
       "      <td>23.00%</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S8278</td>\n",
       "      <td>Junonia_coenia_JC_v1.0.scaffolds</td>\n",
       "      <td>4.06Gbp</td>\n",
       "      <td>96.50%</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>80.86%</td>\n",
       "      <td>5.60</td>\n",
       "      <td>62.79%</td>\n",
       "      <td>6.82</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S6644</td>\n",
       "      <td>Junonia_coenia_JC_v1.0.scaffolds</td>\n",
       "      <td>5.79Gbp</td>\n",
       "      <td>97.40%</td>\n",
       "      <td>84.41%</td>\n",
       "      <td>78.79%</td>\n",
       "      <td>7.78</td>\n",
       "      <td>57.59%</td>\n",
       "      <td>10.20</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample                         reference data_amount map_read%(byReads)  \\\n",
       "0  S5473  Junonia_coenia_JC_v1.0.scaffolds     4.14Gbp             12.82%   \n",
       "1  S5744  Junonia_coenia_JC_v1.0.scaffolds     2.04Gbp             50.87%   \n",
       "2  S5647  Junonia_coenia_JC_v1.0.scaffolds     9.41Mbp             35.20%   \n",
       "3  S8278  Junonia_coenia_JC_v1.0.scaffolds     4.06Gbp             96.50%   \n",
       "4  S6644  Junonia_coenia_JC_v1.0.scaffolds     5.79Gbp             97.40%   \n",
       "\n",
       "  map_read%(halfmap) map_read%(byPosition) expected_coverage genome_coverage  \\\n",
       "0              3.50%                 5.19%              0.37           6.59%   \n",
       "1             30.14%                27.73%              0.97          10.35%   \n",
       "2             27.11%                23.00%              0.00           0.02%   \n",
       "3             87.71%                80.86%              5.60          62.79%   \n",
       "4             84.41%                78.79%              7.78          57.59%   \n",
       "\n",
       "  coverage_mean coverage_median  \n",
       "0          2.75            2.00  \n",
       "1          3.07            2.00  \n",
       "2          2.02            2.00  \n",
       "3          6.82            6.00  \n",
       "4         10.20            8.00  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summerize the stat file\n",
    "l_junoia_with_file = glob.glob('/home/xcao/w/20180905Junonia_coenia/20180919Info/alignment_final_stats/*_Junonia_coenia_JC_v1.0.scaffolds_stat.report')\n",
    "import pandas as pd\n",
    "df_align_stat = pd.concat([pd.read_csv(e,sep='\\t',dtype=str) for e in l_junoia_with_file],ignore_index=True)\n",
    "df_align_stat['sample'] =df_align_stat['sample'].apply(lambda x:'S'+x)\n",
    "print(df_align_stat.shape)\n",
    "df_align_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_align_stat.to_excel('/home/xcao/w/20180905Junonia_coenia/20180919Info/20181009junonia_align_stat.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get stats, 20190204\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20190130Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "ref_genome = 'Junonia_coenia_JC_v1.0.scaffolds'\n",
    "ls_file = ['/archive/butterfly/unbias_pipeline_info/step4_postprocessing/final_stats/' + e+'_'+ref_genome+'_stat.report' for e in sample_prefix]\n",
    "ls_file = [e for e in ls_file if os.path.exists(e)]\n",
    "df_align_stat = pd.concat([pd.read_csv(e,sep='\\t',dtype=str) for e in ls_file],ignore_index=True)\n",
    "df_align_stat['sample'] =df_align_stat['sample'].apply(lambda x:'S'+x)\n",
    "print(df_align_stat.shape)\n",
    "df_align_stat.head()\n",
    "df_align_stat.to_excel('/home/xcao/w/20180905Junonia_coenia/20180919Info/20190204junonia_align_stat.xlsx')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count gaps for map files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map file gap counts to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count gaps and store numpy array in disk for future usage. This will be make future work much faster.\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "def countgap(filename,strand=0, outfile = None):\n",
    "    '''\n",
    "    given a map file, return a numpy array of 1 or 0 showing whether a position is a gap\n",
    "    1 means gap, and 0 means 'ATCG'\n",
    "    strand = 0 or 2, the first or the second strand\n",
    "    if outfile is None, return the numpy array. Else, save the numpy array in disk\n",
    "    '''\n",
    "    if strand == 0:\n",
    "        strand = 0\n",
    "    elif strand == 1:\n",
    "        strand = 2\n",
    "    else:\n",
    "        print('wrong strand number!')\n",
    "        return None\n",
    "    mapFileGapCout = np.array([e[strand] not in 'ATCG' for e in open(filename)],dtype=np.int16)\n",
    "    if outfile is None:\n",
    "        return mapFileGapCout\n",
    "    #else store the variable in file\n",
    "    with open(outfile,'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(mapFileGapCout,f)\n",
    "        print('done')\n",
    "\n",
    "description = '''\n",
    "given a map file, return a numpy array of 1 or 0 showing whether a position is a gap\n",
    "1 means gap, and 0 means 'ATCG'\n",
    "strand = 0 or 1, the first or the second strand\n",
    "if outfile is None, return the numpy array. Else, save the numpy array in disk\n",
    "    '''\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    print(description)\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "    parser.add_argument('-i','--input', help = 'input location of .map files', required=True)\n",
    "    parser.add_argument('-o','--output',help = 'folder of numpy array of map gap count, default None', required = False, default = None)\n",
    "    parser.add_argument('-s','--strand', help = 'which strand to use. 0 or 1', default = 0, choices = [0,1], type=int)\n",
    "    f = parser.parse_args()\n",
    "    countgap(files_map=f.input,strand=f.strand, outfile=f.output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate running scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84285"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "outfolder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMap2GapCountsNumpyArray/'\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20180913scripts/20181005map2npInt8.cmds','w').write('\\n'.join('python3 /home/xcao/p/xiaolongTools/utils/countGapEachPositionFromOneMapFile2npInt8.py -i {folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map -o {outfolder}{sample_id}.npInt8'.format(folder_map=folder_map, sample_id = sample_id, outfolder=outfolder) for sample_id in sample_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python /home/xcao/p/xiaolongTools/multiThread.py 32 /home/xcao/w/20180905Junonia_coenia/20180913scripts/20181005map2npInt8.cmds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign chromosome numbers for Junonia scaffolds based on matching with Heliconius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got genome alignment of Junonia. Get the most likely chromosome order of Junonia based on the genome of Heliconius. Note, it is possible that this assignment is not accurate. e.g. `000012F` is about half aligned to chr1 and half to chr10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Junonia scaffolds to Heliconius chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename_coord = \"/home/xcao/w/20180905Junonia_coenia/Junonia2Heliconius.promer3.coords.qr\"\n",
    "\n",
    "filename = filename_coord\n",
    "l = open(filename).readlines()\n",
    "ls =[e.split() for e in l[5:]]\n",
    "\n",
    "df_coord = pd.DataFrame(ls)\n",
    "df_coord = df_coord[[0,1,3,4,6,7,9,10,11,13,14,16,17,19,20,21,22]].copy()\n",
    "df_coord[[0,1,3,4,6,7,9,10,11,13,14,16,17,19,20]] = df_coord[[0,1,3,4,6,7,9,10,11,13,14,16,17,19,20]].apply(pd.to_numeric)\n",
    "df_coord.columns = ['ref_start','ref_end','query_start','query_end','ref_match_len','query_match_len','identity','similarity','percent_stop_codon','ref_len','query_len','ref_coverage','query_coverage','ref_reading_frame','query_reading_frame','ref_id','query_id']\n",
    "df_coord_group_qr = df_coord.groupby(['query_id','ref_id'])\n",
    "\n",
    "df_qr = df_coord_group_qr['ref_match_len'].sum()\n",
    "df_summary = pd.DataFrame([ [e,df_qr[e].idxmax(),df_qr[e].max(),df_qr[e].sum(), df_qr[e].max()/df_qr[e].sum()] for e in df_qr.index.unique(0)])\n",
    "df_summary.columns = ['test_id','ref_id','scaffold_match_len','total_match_len','ratio_specific']\n",
    "\n",
    "\n",
    "for n in range(df_summary.shape[0]):\n",
    "    temp_df = df_coord_group_qr.get_group((df_summary['test_id'][n],df_summary['ref_id'][n]))\n",
    "    match_fragments = temp_df.shape[0]\n",
    "    df_summary.loc[n,'match_fragments'] = match_fragments\n",
    "    positions = []\n",
    "    for m in range(match_fragments):\n",
    "        start = temp_df.iloc[m]['ref_start']\n",
    "        end = temp_df.iloc[m]['ref_end']\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        positions += list(range(start, end))\n",
    "    position_avg = np.median(positions)\n",
    "    df_summary.loc[n,'position_avg'] = position_avg\n",
    "    \n",
    "\n",
    "df_summary['match_fragments'] = df_summary['match_fragments'].astype(np.int64)\n",
    "df_summary['position_avg'] = df_summary['position_avg'].astype(np.int64)\n",
    "\n",
    "def getHeml_chr_order(scaf_id):\n",
    "    '''\n",
    "    Hmel221001o, remove first 4 letters and last letter. the next two is chromosome number\n",
    "    if not 00, chr number. else, scaffold number\n",
    "    '''\n",
    "    scaf_id = scaf_id[5:-1]\n",
    "    return int(scaf_id)\n",
    "\n",
    "df_summary['Heml_chr_order'] = df_summary['ref_id'].apply(getHeml_chr_order)\n",
    "df_summary_ordered = df_summary.sort_values(by = ['Heml_chr_order','position_avg'])\n",
    "df_summary_ordered.to_csv('/home/xcao/w/genomes/Junonia_coenia/20181004Junonia2HeliconiaChr.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "order the scaffolds of junonia according the chromosome of Heliconius. The order will be chromsome 1-21, and all others. Generate 22 files, each with positions in the map files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order junonia chr according th Heliconius\n",
    "\n",
    "the outfile looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 length: 30322732\n",
      "chr2 length: 17488718\n",
      "chr3 length: 19873991\n",
      "chr4 length: 22305703\n",
      "chr5 length: 21479573\n",
      "chr6 length: 31788073\n",
      "chr7 length: 33385355\n",
      "chr8 length: 18689840\n",
      "chr9 length: 16484192\n",
      "chr10 length: 37804609\n",
      "chr11 length: 24833522\n",
      "chr12 length: 33464206\n",
      "chr13 length: 40426838\n",
      "chr14 length: 18809872\n",
      "chr15 length: 21603009\n",
      "chr16 length: 19437364\n",
      "chr17 length: 35729285\n",
      "chr18 length: 36020983\n",
      "chr19 length: 36775823\n",
      "chr20 length: 37437139\n",
      "chr21 length: 20560391\n",
      "chr0 length: 11272456\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"/home/xcao/w/genomes/Junonia_coenia/20181004Junonia2HeliconiaChr.csv\"\n",
    "file_scaflen = \"/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.scaffolds.fa.len\"\n",
    "\n",
    "\n",
    "df_ch_j2h = pd.read_csv(filename,sep='\\t')\n",
    "df_ch_j2h['Heml_chr'] = df_ch_j2h['Heml_chr_order'] // 1000\n",
    "\n",
    "tempdf = df_ch_j2h.groupby(['Heml_chr'])\n",
    "dc_ch_heml = {e:list(tempdf.get_group(e)['test_id']) for e in tempdf.groups.keys()}\n",
    "st_junonia_scf_with_heml = set(df_ch_j2h['test_id'])\n",
    "\n",
    "\n",
    "def getPositionsFromScfLen(file_scaflen):\n",
    "    '''\n",
    "    file_scaflen is a file with scaffold name and its length, in the order of the original scaffold file.\n",
    "    return a dictionary, with scaffold name as key, and start and end value of positions counting from 0 by aligning the scaffold sequences\n",
    "    '''\n",
    "    dc = {}\n",
    "    start = 0\n",
    "    l = open(file_scaflen).readlines()\n",
    "    for line in l:\n",
    "        scf, scflen = line.split()\n",
    "        scflen = int(scflen)\n",
    "        end = start+scflen\n",
    "        dc[scf] = (start, end)\n",
    "        start = end\n",
    "    return dc\n",
    "\n",
    "def run_fun_same_memory():\n",
    "    dc_junonia_scaf2pos = getPositionsFromScfLen(file_scaflen)\n",
    "\n",
    "    for e in dc_junonia_scaf2pos:\n",
    "        if e not in st_junonia_scf_with_heml:\n",
    "            dc_ch_heml[0].append(e)\n",
    "    \n",
    "    fout = open('/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.txt','w')\n",
    "    for e in list(range(1,22))+[0]:\n",
    "        chr_len = 0\n",
    "        fout.write(str(e)+':')\n",
    "        for scf in dc_ch_heml[e]:\n",
    "            fout.write('({scf}:{start},{end})'.format(scf=scf,start=dc_junonia_scaf2pos[scf][0],end=dc_junonia_scaf2pos[scf][1]))\n",
    "            fout.write(';')\n",
    "            chr_len += dc_junonia_scaf2pos[scf][1] - dc_junonia_scaf2pos[scf][0]\n",
    "        fout.write('\\n')\n",
    "        print('chr%d length:'%e, chr_len)\n",
    "    fout.close()\n",
    "    \n",
    "    \n",
    "run_fun_same_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change file to table `1\t000635F\t558391428\t558537911` with chromosome_id, scaffold_id, start and end in map file\n",
    "```\n",
    "file_in = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.txt'\n",
    "fout = open('/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.tab','w')\n",
    "for l in open(file_in):\n",
    "    chr_id, j_scfs = l.strip().split(':',1)\n",
    "    sites = []\n",
    "    for j_scf in j_scfs.split(';')[:-1]:\n",
    "        scf, start_end =j_scf[1:-1].split(':')\n",
    "        start,end = start_end.split(',')\n",
    "        fout.write(chr_id +'\\t'+scf+'\\t'+start+'\\t'+end+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split ordered junonia to 100kb pecies of different chromosome\n",
    "file indicating chromosome of junonia is like:\n",
    "```\n",
    "1:(000635F:558391428,558537911);(000793F:575740860,575815531);...;\n",
    "2:(000261F:437507228,438150665);(000248F:428842206,429533362);...;\n",
    "```\n",
    "output a file, with fragments and positions in map file. The output file looks like:\n",
    "\n",
    "```\n",
    "chr1_0:n01 n02 ...\n",
    "chr1_1:n11 n12 ...\n",
    "```\n",
    "chr1_1 means first chromosome, 0st fragment. split each chromosome to N parts, so that $\\frac{chr\\_len}{100kb} - 1 < N \\leqslant \\frac{chr\\_len}{100kb}$ , and fragment length will be just a little bit over 100kb\n",
    "\n",
    "generate files:  \n",
    "sitesKeep_2HmelChrOrder_fraglen10000  \n",
    "sitesKeep_2HmelChrOrder_fraglen20000  \n",
    "sitesKeep_2HmelChrOrder_fraglen50000  \n",
    "sitesKeep_2HmelChrOrder_fraglen100000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 length: 30322732, split to 303 fragments, the length of fragments is 100076\n",
      "chr2 length: 17488718, split to 174 fragments, the length of fragments is 100510\n",
      "chr3 length: 19873991, split to 198 fragments, the length of fragments is 100374\n",
      "chr4 length: 22305703, split to 223 fragments, the length of fragments is 100026\n",
      "chr5 length: 21479573, split to 214 fragments, the length of fragments is 100372\n",
      "chr6 length: 31788073, split to 317 fragments, the length of fragments is 100278\n",
      "chr7 length: 33385355, split to 333 fragments, the length of fragments is 100257\n",
      "chr8 length: 18689840, split to 186 fragments, the length of fragments is 100484\n",
      "chr9 length: 16484192, split to 164 fragments, the length of fragments is 100514\n",
      "chr10 length: 37804609, split to 378 fragments, the length of fragments is 100013\n",
      "chr11 length: 24833522, split to 248 fragments, the length of fragments is 100136\n",
      "chr12 length: 33464206, split to 334 fragments, the length of fragments is 100193\n",
      "chr13 length: 40426838, split to 404 fragments, the length of fragments is 100067\n",
      "chr14 length: 18809872, split to 188 fragments, the length of fragments is 100053\n",
      "chr15 length: 21603009, split to 216 fragments, the length of fragments is 100014\n",
      "chr16 length: 19437364, split to 194 fragments, the length of fragments is 100193\n",
      "chr17 length: 35729285, split to 357 fragments, the length of fragments is 100083\n",
      "chr18 length: 36020983, split to 360 fragments, the length of fragments is 100059\n",
      "chr19 length: 36775823, split to 367 fragments, the length of fragments is 100207\n",
      "chr20 length: 37437139, split to 374 fragments, the length of fragments is 100100\n",
      "chr21 length: 20560391, split to 205 fragments, the length of fragments is 100295\n",
      "chr0 length: 11272456, split to 112 fragments, the length of fragments is 100647\n"
     ]
    }
   ],
   "source": [
    "target_fragment_len = 100000\n",
    "file_in = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.txt'\n",
    "file_out = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen%d'%target_fragment_len\n",
    "fout = open(file_out,'w')\n",
    "for l in open(file_in):\n",
    "    chr_id, j_scfs = l.strip().split(':',1)\n",
    "    sites = []\n",
    "    for j_scf in j_scfs.split(';')[:-1]:\n",
    "        start,end = j_scf[1:-1].split(':')[1].split(',')\n",
    "        sites = sites + list(range(int(start), int(end)))\n",
    "    chr_len = len(sites)\n",
    "    N = chr_len // target_fragment_len\n",
    "    fragment_len = chr_len // N + 1\n",
    "    print('chr%s length: %d, split to %d fragments, the length of fragments is %d'%(chr_id, chr_len, N, fragment_len))\n",
    "    for i in range(N):\n",
    "        fout.write('chr%s_%d:'%(chr_id, i))\n",
    "        frag_sites = sites[i*fragment_len:(i+1)*fragment_len]\n",
    "        fout.write(' '.join(str(e) for e in frag_sites))\n",
    "        fout.write('\\n')\n",
    "del sites\n",
    "del l\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastqc, reads count, length and quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run fastqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58721"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfolder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/fastqc/'\n",
    "fastqc = '/home/xcao/p/fastqc/FastQC/fastqc'\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20180913scripts/20181007junonia_fastqc.cmds','w').write('\\n'.join('{fastqc} /archive/butterfly/ready_fastq/{sample}_R1.fastq /archive/butterfly/ready_fastq/{sample}_R2.fastq -o {outfolder} -t 1'.format(fastqc=fastqc,outfolder=outfolder,sample=sample) for sample in sample_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46762"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('/home/xcao/w/20180905Junonia_coenia/20180913scripts/20181007junonia_fastqc.cmds2','w').write('\\n'.join('{fastqc} /archive/butterfly/ready_fastq/{sample}_singleton.fastq -o {outfolder} -t 1'.format(fastqc=fastqc,outfolder=outfolder,sample=sample) for sample in sample_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20180913scripts/20181007junonia_fastqc.cmds```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/fastqc/'\n",
    "file_fastqc_data = glob.glob(folder+'**/fastqc_data.txt',recursive=True)\n",
    "print(len(file_fastqc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xcao/w/20180905Junonia_coenia/20180919Info/fastqc/7157_singleton_fastqc/fastqc_data.txt'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_fastqc_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S7157', 'singleton', 36.0, 24.444444444444443, 30.69423076848703)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "f = file_fastqc_data[0]\n",
    "def fastqc2readsinfo(filename):\n",
    "    l = open(f).readlines()\n",
    "    sample_id_full = os.path.basename(os.path.dirname(f))\n",
    "    sample_id = 'S' + sample_id_full.split('_')[0]\n",
    "    sample_reads_type = sample_id_full.split('_')[1]\n",
    "    reads_count = l[6].split()[2]\n",
    "    GC_percentage = l[9].split()[1]\n",
    "    n = 0\n",
    "    while n < len(l):\n",
    "        if '>>Sequence Length Distribution' in l[n]:\n",
    "            break\n",
    "        else:\n",
    "            n = n+1\n",
    "    n = n+2\n",
    "    reads_len_info = []\n",
    "    while n < len(l):\n",
    "        if l[n][0] != '>':\n",
    "            _reads_len_range, _reads_count = l[n].strip().split('\\t')\n",
    "            if '-' in _reads_len_range:\n",
    "                _rs,_re = _reads_len_range.split('-')\n",
    "                _rl = (float(_rs) + float(_re))/2\n",
    "            else:\n",
    "                _rl = float(_reads_len_range)\n",
    "            _reads_count = float(_reads_count)\n",
    "            reads_len_info.append([_rl,_reads_count])\n",
    "            n += 1\n",
    "        else:\n",
    "            break\n",
    "    reads_count = sum(e[1] for e in reads_len_info)\n",
    "    reads_len_mean = sum(e[0]*e[1] for e in reads_len_info)/reads_count\n",
    "    reads_len_sd = math.sqrt(sum(number*number*count for number, count in reads_len_info) / reads_count - reads_len_mean * reads_len_mean)\n",
    "    #print('total reads', reads_count,'reads length mean', reads_len_mean, 'reads length sd', reads_len_sd)\n",
    "    return sample_id, sample_reads_type, reads_count, reads_len_mean, reads_len_sd\n",
    "\n",
    "fastqc2readsinfo(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reads_count_singleton</th>\n",
       "      <th>reads_len_mean_singleton</th>\n",
       "      <th>reads_len_sd_singleton</th>\n",
       "      <th>reads_count_R1</th>\n",
       "      <th>reads_len_mean_R1</th>\n",
       "      <th>reads_len_sd_R1</th>\n",
       "      <th>reads_count_R2</th>\n",
       "      <th>reads_len_mean_R2</th>\n",
       "      <th>reads_len_sd_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S7157</th>\n",
       "      <td>36.0</td>\n",
       "      <td>24.444444</td>\n",
       "      <td>30.694231</td>\n",
       "      <td>19401694.0</td>\n",
       "      <td>102.673830</td>\n",
       "      <td>36.073033</td>\n",
       "      <td>19401694.0</td>\n",
       "      <td>102.497476</td>\n",
       "      <td>35.915343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5649</th>\n",
       "      <td>2422.0</td>\n",
       "      <td>123.154418</td>\n",
       "      <td>43.498703</td>\n",
       "      <td>18035201.0</td>\n",
       "      <td>112.553538</td>\n",
       "      <td>39.836017</td>\n",
       "      <td>18035201.0</td>\n",
       "      <td>112.236214</td>\n",
       "      <td>39.657888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5702</th>\n",
       "      <td>23079.0</td>\n",
       "      <td>91.652563</td>\n",
       "      <td>18.994251</td>\n",
       "      <td>12931875.0</td>\n",
       "      <td>82.230831</td>\n",
       "      <td>18.565457</td>\n",
       "      <td>12931875.0</td>\n",
       "      <td>82.389342</td>\n",
       "      <td>18.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5489</th>\n",
       "      <td>64.0</td>\n",
       "      <td>19.125000</td>\n",
       "      <td>17.273082</td>\n",
       "      <td>13866598.0</td>\n",
       "      <td>81.177647</td>\n",
       "      <td>36.035326</td>\n",
       "      <td>13866598.0</td>\n",
       "      <td>80.896868</td>\n",
       "      <td>35.627463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7087</th>\n",
       "      <td>87.0</td>\n",
       "      <td>16.201149</td>\n",
       "      <td>8.739019</td>\n",
       "      <td>21888219.0</td>\n",
       "      <td>74.505287</td>\n",
       "      <td>27.800787</td>\n",
       "      <td>21888219.0</td>\n",
       "      <td>74.356653</td>\n",
       "      <td>27.624737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reads_count_singleton  reads_len_mean_singleton  \\\n",
       "S7157                   36.0                 24.444444   \n",
       "S5649                 2422.0                123.154418   \n",
       "S5702                23079.0                 91.652563   \n",
       "S5489                   64.0                 19.125000   \n",
       "S7087                   87.0                 16.201149   \n",
       "\n",
       "       reads_len_sd_singleton  reads_count_R1  reads_len_mean_R1  \\\n",
       "S7157               30.694231      19401694.0         102.673830   \n",
       "S5649               43.498703      18035201.0         112.553538   \n",
       "S5702               18.994251      12931875.0          82.230831   \n",
       "S5489               17.273082      13866598.0          81.177647   \n",
       "S7087                8.739019      21888219.0          74.505287   \n",
       "\n",
       "       reads_len_sd_R1  reads_count_R2  reads_len_mean_R2  reads_len_sd_R2  \n",
       "S7157        36.073033      19401694.0         102.497476        35.915343  \n",
       "S5649        39.836017      18035201.0         112.236214        39.657888  \n",
       "S5702        18.565457      12931875.0          82.389342        18.700800  \n",
       "S5489        36.035326      13866598.0          80.896868        35.627463  \n",
       "S7087        27.800787      21888219.0          74.356653        27.624737  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_fastqc = pd.DataFrame()\n",
    "for f in file_fastqc_data:\n",
    "    sample_id, sample_reads_type, reads_count, reads_len_mean, reads_len_sd = fastqc2readsinfo(f)\n",
    "    df_fastqc.loc[sample_id,'reads_count_'+sample_reads_type] = reads_count\n",
    "    df_fastqc.loc[sample_id,'reads_len_mean_'+sample_reads_type] = reads_len_mean\n",
    "    df_fastqc.loc[sample_id,'reads_len_sd_'+sample_reads_type] = reads_len_sd\n",
    "print(df_fastqc.shape)\n",
    "df_fastqc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fastqc.to_excel('/home/xcao/w/20180905Junonia_coenia/20180919Info/20181008junonia_reads_info.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permanently remove 34 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totally 308 files. Remove 34 files, 12 of which is less than 1Gb, 32 with genome coverage less than 20%. \n",
    "\n",
    "Future analysis will be based on the 274 high quality ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change names\n",
    "\n",
    "change name of 7152 7166 7154 from 'Junonia zonalis michaelisi?' to 'Junonia zonalis michaelisi'. change 16106B05, 16106B06 16106B07 from 'Junonia nigrosuffusa (=melanina)' to 'Junonia nigrosuffusa'. \n",
    "\n",
    "15101E08 is a wrong sample. The total bases is good. but the align rate and mapped potion is too low. Removed in previous Step.\n",
    "5672 map ratio is low. Removed in previous step.\n",
    "\n",
    "change 5753 5754 5755 from 'Junonia evarete lima?divaricata' to 'Junonia divaricata divaricata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further remove some samples to increase the sites where all sample have bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique sample prefix 274\n"
     ]
    }
   ],
   "source": [
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "import pandas as pd\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "df_summary = df_summary[df_summary['permanet_remove'] == 0]\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMap2GapCountsNumpyArray/3935.npInt8'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "files_gap_npInt8 = {e:'/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMap2GapCountsNumpyArray/{e}.npInt8'.format(e=e) for e in sample_prefix}\n",
    "files_gap_npInt8[list(files_gap_npInt8.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "npInt8_gaps = {}\n",
    "for k, f in files_gap_npInt8.items():\n",
    "    npInt8_gaps[k] = pickle.load(open(f,'rb'))\n",
    "\n",
    "gapsum_all = np.zeros(len(npInt8_gaps[k]), dtype=np.int16)\n",
    "for v in npInt8_gaps.values():\n",
    "    gapsum_all += v\n",
    "\n",
    "#sites all not gap\n",
    "(gapsum_all == 0).sum() # output 36649\n",
    "\n",
    "# remove 1 sample and count sites all not gap\n",
    "to_remove = []\n",
    "keys = set(npInt8_gaps.keys())\n",
    "gapsum_temp = gapsum_all.copy()\n",
    "for dd in range(15):\n",
    "    templs = []\n",
    "    for k in keys:\n",
    "        v = npInt8_gaps[k]\n",
    "        templs.append((k, ((gapsum_temp - v) == 0).sum()))\n",
    "\n",
    "    templs.sort(key=lambda x:x[1])\n",
    "    to_remove.append(templs[-1])\n",
    "    print('remove one and remaining good sites', templs[-1])\n",
    "    keys.remove(templs[-1][0])\n",
    "    gapsum_temp = gapsum_temp - npInt8_gaps[templs[-1][0]]\n",
    "\n",
    "## run 15 rounds, each rounds remove one sample that will make the full-base sites reach maximum. \n",
    "## Then check the result to decide which one to remove.\n",
    "## after remove '5504', sites increase to 42226. But this one is the Junonia villida. \n",
    "## Remove the second best one, '15102E08', which is '15102E08'\n",
    "```\n",
    "\n",
    "the print out is like \n",
    "```\n",
    "remove one and remaining good sites ('5504', 42226)\n",
    "remove one and remaining good sites ('6661', 55020)\n",
    "remove one and remaining good sites ('15102E08', 65182)\n",
    "remove one and remaining good sites ('7578', 75933)\n",
    "remove one and remaining good sites ('16106B10', 84043)\n",
    "remove one and remaining good sites ('17109C10', 94347)\n",
    "remove one and remaining good sites ('5475', 103443)\n",
    "remove one and remaining good sites ('15101B04', 111072)\n",
    "remove one and remaining good sites ('7078', 118345)\n",
    "remove one and remaining good sites ('7169', 126585)\n",
    "remove one and remaining good sites ('5670', 133301)\n",
    "remove one and remaining good sites ('5663', 139890)\n",
    "remove one and remaining good sites ('7077', 146910)\n",
    "remove one and remaining good sites ('5710', 153787)\n",
    "remove one and remaining good sites ('6658', 161318)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5504, reomove, outer group and introduce many gaps  \n",
    "6661, remove, outer group and introduce many gaps   \n",
    "15102E08， remove, low quality and introduce many gaps  \n",
    "7578, remove, member of large group. low quality and introduce many gaps  \n",
    "16106B10, remove, member of large group. low quality and introduce many gaps  \n",
    "17109C10, remove, member of large group. low quality and introduce many gaps  \n",
    "5475, remove, member of large group. low quality and introduce many gaps  \n",
    "15101B04,  remove, member of large group. low quality and introduce many gaps  \n",
    "7078, remove, member of group of 4. low quality and introduce many gaps  \n",
    "7169, remove, member of 7. low quality and introduce many gaps  \n",
    "5670, remove, member of large group. low quality and introduce many gaps  \n",
    "\n",
    "the following one 5710 can be merged with other samples, and the sample group is not big, only with 7168 for sure. Stop here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the samples above and calculate 'gapCounts'\n",
    "```\n",
    "to_remove = ['5504', '6661', '15102E08', '7578', '16106B10', '17109C10', '5475', '15101B04', '7078', '7169', '5670']\n",
    "keys = set(npInt8_gaps.keys())\n",
    "for e in to_remove:\n",
    "    keys.remove(e)\n",
    "print(len(keys))\n",
    "\n",
    "gapsum_all_263 = np.zeros(len(npInt8_gaps[k]),dtype=np.int16)\n",
    "for e in keys:\n",
    "    gapsum_all_263 += npInt8_gaps[e]\n",
    "\n",
    "print((gapsum_all_263 == 0).sum())\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "gapCounts = Counter(gapsum_all_263)\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20180919Info/20181010mapFile263.GapCounts.statistics','w')\n",
    "l = list(gapCounts.items())\n",
    "l.sort(key=lambda x:x[0])\n",
    "for e in l:\n",
    "    fout.write('%d\\t%d\\n'%(e[0],e[1]))\n",
    "fout.close()\n",
    "\n",
    "import pickle\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20180919Info/20181010mapFile263.GapCounts.npInt16','wb') as f:\n",
    "    pickle.dump(gapsum_all_263,f)\n",
    "```\n",
    "\n",
    "gapCounts is stored as binary file with pickle. np.int16 format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get intron, intergene, UTR, CDS sites for Junonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Junonia genome info:\n",
    "\n",
    "| type | length | percentage |\n",
    "|--|:--:|:--|\n",
    "| CDS: | 22515575  |   0.0384 |\n",
    "| UTR: | 14638623  |   0.0250 |\n",
    "| intron: | 105194869  |   0.1795 |\n",
    "| intergene: | 443644607  |   0.7571 |\n",
    "| total: | 585993674 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "file_info = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.position.base.scf.scfpos.gene.rna.exon.cds'\n",
    "out_file_intron = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_intron'\n",
    "out_file_intergene = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_interGene'\n",
    "out_file_UTR = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_UTR'\n",
    "out_file_cds = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_CDS'\n",
    "fo = open(file_info)\n",
    "fout_intron = open(out_file_intron,'w')\n",
    "fout_CDS = open(out_file_cds,'w')\n",
    "fout_intergene = open(out_file_intergene, 'w')\n",
    "fout_UTR = open(out_file_UTR,'w')\n",
    "for line in fo:\n",
    "    es = line.split()\n",
    "    if es[-1]!='0':\n",
    "        fout_CDS.write(es[0]+'\\n')\n",
    "    else:\n",
    "        if es[4] == '0':\n",
    "            fout_intergene.write(es[0]+'\\n')\n",
    "        else:\n",
    "            if es[6] == '0':\n",
    "                fout_intron.write(es[0]+'\\n')\n",
    "            else:\n",
    "                fout_UTR.write(es[0]+'\\n')\n",
    "fout_intron.close()\n",
    "fout_intergene.close()\n",
    "fout_UTR.close()\n",
    "fout_CDS.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### species count\n",
    "```\n",
    "Junonia MEXICANspecies:31\n",
    "Junonia zonalis zonalis:28\n",
    "Junonia neildi:23\n",
    "Junonia coenia coenia:20\n",
    "Junonia nigrosuffusa:20\n",
    "Junonia coenia grisea:16\n",
    "Junonia neildiTX:15\n",
    "Junonia zonalis michaelisi:11\n",
    "Junonia nigrosuffusaTX:9\n",
    "Junonia wMex:9\n",
    "Junonia zonalis swifti:9\n",
    "Junonia divaricata divaricata:8\n",
    "Junonia evarete hilaris:6\n",
    "Junonia NSAm:6\n",
    "Junonia genoveva genoveva:5\n",
    "Junonia wahlbergi:5\n",
    "Junonia divaricata arenosa:4\n",
    "Junonia neildiTX-hybrid:4\n",
    "Junonia GRENADA:3\n",
    "Junonia litoralis:3\n",
    "Junonia vestina vestina:3\n",
    "Junonia coenia grisea-hybrid:2\n",
    "Junonia divaricata basifusca:2\n",
    "Junonia evarete:2\n",
    "Junonia evarete dougueti:2\n",
    "Junonia evarete fuscescens:2\n",
    "Junonia evarete oscura:2\n",
    "Junonia neildi?:2\n",
    "Junonia vestina livia:2\n",
    "Junonia coenia coenia X neildiTX:1\n",
    "Junonia coenia-hybrid:1\n",
    "Junonia divaricata:1\n",
    "Junonia genoveva:1\n",
    "Junonia genoveva vivida:1\n",
    "Junonia neildi-hybrid:1\n",
    "Junonia sp.:1\n",
    "Junonia sp.black-nudum:1\n",
    "Junonia vestina:1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert and store map file in numpy array, binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "outfolder1 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "outfolder2 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import pandas as pd\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "files = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "mapfileIO.read2Int8s(files, strand=0, changeGap=True, threads=32, outfolder=outfolder1)\n",
    "mapfileIO.read2Int8s(files, strand=2, changeGap=True, threads=32, outfolder=outfolder2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  convert to numpy array with new sequenced results 20190130\n",
    "```\n",
    "outfolder1 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "outfolder2 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20190130Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "files_processed = os.listdir(outfolder1)\n",
    "files_processed = [e for e in files_processed if e.split('_')[0] not in ['15102E08', '5325', '5330']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "files_all = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "files = [e for e in files_all if os.path.exists(e) and os.path.basename(e) not in files_processed]\n",
    "mapfileIO.read2Int8s(files, strand=0, changeGap=True, threads=32, outfolder=outfolder1)\n",
    "mapfileIO.read2Int8s(files, strand=2, changeGap=True, threads=32, outfolder=outfolder2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count gaps in files 20190131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change 5325.250 to 5325, remove the old one. remove 5325.rna.  \n",
    "count gaps in 324 map files of stand0\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gapCount324'\n",
    "outfileHist = outfile + '.hist'\n",
    "files = open('/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/20190131mapfiles.324').read().split()\n",
    "files = [folder + e for e in files]\n",
    "print('total map files', len(files))\n",
    "\n",
    "ls_mapInt8 = mapfileIO.loadMapBinaries(files, threads=32)#multithreading not working. use the slow method\n",
    "#ls_mapInt8 = mapfileIO.loadMapBinary(f) for f in files]\n",
    "temp = ls_mapInt8[0]\n",
    "maplen = len(temp)\n",
    "gapCount = np.zeros(maplen,dtype=np.int16)\n",
    "for e in ls_mapInt8:\n",
    "    gapCount += (e==45)\n",
    "\n",
    "with open(outfile,'wb') as f:\n",
    "    pickle.dump(gapCount,f)\n",
    "\n",
    "gapHist = np.unique(gapCount,return_counts=True)\n",
    "with open(outfileHist,'w') as f:\n",
    "    for c, p in zip(*gapHist):\n",
    "        f.write('{}\\t{}\\n'.format(c,p))\n",
    "\n",
    "#average gap per site\n",
    "gapAvg = gapCount.sum() / maplen\n",
    "print('average gap per site', gapAvg)\n",
    "\n",
    "```\n",
    "\n",
    "average gap per site 182.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get locations of mRNA, CDS and intron\n",
    "```\n",
    "file_gff = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.gff3'\n",
    "file_genome_len = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.scaffolds.fa.len'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df_scf = pd.read_csv(file_genome_len,sep='\\t', header=None)\n",
    "df_scf.columns = ['scf','scf_len']\n",
    "genomelen = df_scf['scf_len'].sum()\n",
    "\n",
    "df_scf['start'] = 0\n",
    "df_scf['end'] = df_scf['scf_len']\n",
    "for i in df_scf.index[1:]:\n",
    "    df_scf.loc[i,'start'] = df_scf.loc[i-1,'start'] + df_scf.loc[i-1, 'scf_len']\n",
    "    df_scf.loc[i,'end'] = df_scf.loc[i,'start'] + df_scf.loc[i,'scf_len']\n",
    "\n",
    "dc_scf2start = dict(zip(df_scf['scf'], df_scf['start']))\n",
    "dc_scf2end = dict(zip(df_scf['scf'], df_scf['end']))\n",
    "\n",
    "df_gff = pd.read_csv(file_gff,sep='\\t', header=None,comment='#')\n",
    "df_gff.columns = ['scf', 'program', 'type', 'start', 'end', 'special', 'strand','phase','transcript_id']\n",
    "df_gff = df_gff.set_index('scf')\n",
    "\n",
    "gffnp = np.zeros(genomelen,dtype=np.uint8)\n",
    "for scf, scf_gff in df_gff.iterrows():\n",
    "    start = scf_gff['start']-1\n",
    "    end = scf_gff['end']\n",
    "    start += dc_scf2start[scf]\n",
    "    end += dc_scf2start[scf]\n",
    "    if scf_gff['type'] == 'mRNA':\n",
    "        gffnp[range(start,end)] = 2\n",
    "#set exon UTR 14\n",
    "for scf, scf_gff in df_gff.iterrows():\n",
    "    start = scf_gff['start']-1\n",
    "    end = scf_gff['end']\n",
    "    start += dc_scf2start[scf]\n",
    "    end += dc_scf2start[scf]\n",
    "    if scf_gff['type'] == 'exon':\n",
    "        gffnp[range(start,end)] = 14\n",
    "\n",
    "#set CDS region to 6\n",
    "for scf, scf_gff in df_gff.iterrows():\n",
    "    start = scf_gff['start']-1\n",
    "    end = scf_gff['end']\n",
    "    start += dc_scf2start[scf]\n",
    "    end += dc_scf2start[scf]\n",
    "    if scf_gff['type'] == 'CDS':\n",
    "        gffnp[range(start,end)] = 6\n",
    "\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gff.0inter.2intron.6CDS.14UTR','wb') as f:\n",
    "    pickle.dump(gffnp,f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate a filter np.array file to extract z-chromosome sequences and zChrCDS\n",
    "```\n",
    "file_genomelen = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.scaffolds.fa.len'\n",
    "file_targetscf = '/home/xcao/w/20180905Junonia_coenia/20180913junonia_scaffolds_z_by_3species'\n",
    "file_gffnp = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gff.0inter.2intron.6CDS.14UTR'\n",
    "file_zChr = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilter'\n",
    "file_zChrCDS = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilterCDS'\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "sites = [] #True for keep, False not keep. The same length as the genome\n",
    "target_scfs = open(file_targetscf).read().split()\n",
    "target_scfs = set(target_scfs)\n",
    "ls_genomelen = open(file_genomelen).readlines() #remove mitochondrial line\n",
    "for line in ls_genomelen:\n",
    "    scf,scflen = line.split()\n",
    "    scflen = int(scflen)\n",
    "    if scf in target_scfs:\n",
    "        sites.append([True] * scflen)\n",
    "    else:\n",
    "        sites.append([False] * scflen)\n",
    "sites = list(itertools.chain.from_iterable(sites))\n",
    "npSites = np.array(sites)\n",
    "with open(file_zChr,'wb') as f:\n",
    "    pickle.dump(npSites,f)\n",
    "\n",
    "gffnp = pickle.load(open(file_gffnp,'rb'))\n",
    "npzChrCDS = ((gffnp==6) & npSites)\n",
    "with open(file_zChrCDS,'wb') as f:\n",
    "    pickle.dump(npzChrCDS,f)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate simiarity between each of the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import npMap\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "files = glob.glob(folder+'*')\n",
    "ls_names = [os.path.basename(e).split('_')[0] for e in files]\n",
    "ls_samples = ['S'+e for e in ls_names]\n",
    "similarities = npMap.calCommonNonGapRatio2D(files,threads =32)\n",
    "df = pd.DataFrame(similarities,index=ls_samples,columns=ls_samples)\n",
    "df.to_excel('/home/xcao/w/20180905Junonia_coenia/20180919Info/20181120junoniaLibrarySimilarityMatrix.xlsx')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create artificial outgroups\n",
    "\n",
    "create some map files as outgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine the 8 distant species, choose each non-gap site randomly, with lowest frequency, or with highest frequency\n",
    "['5462', '5469', '6661', '5467', '6660', '5468', '5504']\n",
    "\n",
    "scripts shown below. After merging, the coverage is 48.34%, still lower than the merged 4 species.  \n",
    "J_vestina, the coverage is 68.81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine 263 samples in use, choose each non-gap site with base with lowest frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import npMap\n",
    "\n",
    "sample_prefix = ['5462', '5469', '6661', '5467', '6660', '5468', '5504']\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "files = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "\n",
    "npMap.combineMap(files,outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileArtificialOutGroup/remote7random.map', method=0, threads =32)\n",
    "npMap.combineMap(files,outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileArtificialOutGroup/remote7high.map', method=2, threads =32)\n",
    "npMap.combineMap(files,outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileArtificialOutGroup/remote7low.map', method=3, threads =32)\n",
    "\n",
    "files = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location263.txt'\n",
    "npMap.combineMapSmallMem(files, outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileArtificialOutGroup/using263low.map', method=3)\n",
    "\n",
    "sample_prefix = ['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "files = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "\n",
    "npMap.combineMap(files,outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileArtificialOutGroup/J_vestina.map', method=2, threads =32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check which file is a good outgroup\n",
    "if it is a good outgroup, then if rooted with the outgroup, the chance is high that 6 merged genomes will be better grouped with neildi and zonalis together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract sequences\n",
    "```\n",
    "groups = '''remote8high remote8low remote8random using263low J_vestina'''.split()\n",
    "folder='/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample'\n",
    "commandline = 'python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i {folder}/mapfiles_{group} -t 8 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o {folder}/fraglen100000_{group}'\n",
    "for group in groups:\n",
    "    print(commandline.format(folder=folder,group=group))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "import glob\n",
    "groups = '''remote8high remote8low remote8random using263low J_vestina'''.split()\n",
    "folder='/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample'\n",
    "folders = [folder+'/fraglen100000_'+e+'/' for e in groups]\n",
    "outfolders = [e[:-1]+'_tree' for e in folders]\n",
    "outcommands = [e[:-1] + '.cmds' for e in folders]\n",
    "for group, folder, outfolder, outfile_cmds in zip(groups, folders, outfolders, outcommands):\n",
    "#changed to add bootstrap\n",
    "    files = glob.glob(folder+'*')\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename} -o {group} -x 234 -N 100 -f a && mv RAxML_bipartitions.{basename} {outfolder}/{basename} && rm RAxML*{basename} && rm {basename}.*'\n",
    "    open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e),group=group) for e in files))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect trees and count tree topologies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# RAxML collect data\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_remote8high_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_remote8low_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_remote8random_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_using263low_tree/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J_vestina as outgroup, 5, get topology of three and minBoots for each tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20181130 new sequenced libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the excel file of all sequences in Nick Lab, there are 329 libaries from Junonia, compared to 308 from the previous one. In the newest table with all samples, these three IDs exist twice, which are “NVG-15102E08 5325 5330”. I checked the /archive/butterfly/ready_fastq folder and found the fastq files are before 2018. I am not sure whether these are typos in the table.\n",
    "\n",
    "Note: 20181303, the duplicated IDs changed by Nick.  \n",
    "\n",
    "Normal new data:\n",
    "NVG-18081F08\n",
    "NVG-18081F09\n",
    "NVG-18081F10\n",
    "NVG-18081F11\n",
    "NVG-18083A01\n",
    "NVG-18083A02\n",
    "NVG-18083A03\n",
    "NVG-18083A04\n",
    "NVG-18083A05\n",
    "NVG-18083A06\n",
    "NVG-18083A07\n",
    "NVG-18083A08\n",
    "NVG-18083A09\n",
    "NVG-18083A10\n",
    "NVG-18083A11\n",
    "NVG-18083A12\n",
    "\n",
    "other new data:\n",
    "5325_250 (for assemblying genome? info from Jing)  \n",
    "5325_RNA (RNA-seq?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20181207 transfer some files to ls5 TACC for Qian Cong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transfer map files\n",
    "```\n",
    "import pandas as pd\n",
    "import os\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "files = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "\n",
    "file = '{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = '{'+','.join(sample_prefix) +'}')\n",
    "commandline = 'scp {file} ks2073@ls5.tacc.utexas.edu:/scratch/05920/ks2073/20181206Junonia/map/'.format(file=file)\n",
    "os.system(commandline)\n",
    "\n",
    "\n",
    "```\n",
    "#### transfer all analysis\n",
    "```\n",
    "scp -r /home/xcao/w/20180905Junonia_coenia/ ks2073@ls5.tacc.utexas.edu:/scratch/05920/ks2073/20181206Junonia/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treemix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use 6 big groups based on CDS tree\n",
    "\n",
    "the 6 species are 'JNG', 'JCG', 'JCC', 'JNX', 'NDX', 'NDI'. J. neildi are in two groups based on the tree.\n",
    "* JNG: Junonia nigrosuffusa\n",
    "* JCG: Junonia coenia grisea\n",
    "* JCC: Junonia coenia coenia\n",
    "* JNX: Junonia nigrosuffusaTX\n",
    "* NDX: Junonia neildiTX\n",
    "* NDI: Junonia neildi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get input files based on the grouping info. Use alignments from whole genome, CDS and z-chromosome.\n",
    "\n",
    "\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/115sample_pop_cdsTree6groups -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS -N 9 -G 0.8\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.treemix -root NDI\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/115sample_pop_cdsTree6groups -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z -N 9 -G 0.8\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.treemix -root NDI\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/115sample_pop_cdsTree6groups -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap -N 9 -G 0.8\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.treemix -root NDI\n",
    "```\n",
    "\n",
    "use -k 500 (the whole genome after filtering with about 3 million sites, split it to about 6000 groups)\n",
    "allow one migration, -m 1\n",
    "```\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.treemix.m1k500 -root NDI -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.treemix.m1k500 -root NDI -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.treemix.m1k500 -root NDI -m 1 -k 500\n",
    "```\n",
    "-m 2, -m 3\n",
    "```\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/115sample_pop_cdsTree6groups.CDS.treemix.m2k500 -root NDI -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/115sample_pop_cdsTree6groups.z.treemix.m2k500 -root NDI -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/115sample_pop_cdsTree6groups.whole_max30Gap.treemix.m2k500 -root NDI -m 2 -k 500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot with R\n",
    "\n",
    "```\n",
    "source('/home/xcao/p/treemix/treemix-1.13/src/plotting_funcs.R')\n",
    "library(glue)\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/115sample_pop_cdsTree6groups.pdf',width = 8,height = 6)\n",
    "\n",
    "for (key in c('CDS','z','whole_max30Gap')) {\n",
    "  #print(key)\n",
    "  filename.pre <- c('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/{key}/115sample_pop_cdsTree6groups.{key}.treemix')\n",
    "  filename.extends = c('','.m1k500','.m2k500','.m3k500')\n",
    "  for (filename.extend in filename.extends) {\n",
    "    filename <- paste0(glue(filename.pre), filename.extend)\n",
    "    # print(filename)\n",
    "    plot_tree(filename,maintitle = paste0(key,' ',filename.extend))\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use 9 samples 7 species "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/63sample_7species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS -N 4 -G 0.8 &\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/63sample_7species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z -N 4 -G 0.8 &\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/63sample_7species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap -N 4 -G 0.8 &\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/63sample_7species.CDS.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/63sample_7species.z.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/63sample_7species.whole_max30Gap.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot with R\n",
    "```\n",
    "source('/home/xcao/p/treemix/treemix-1.13/src/plotting_funcs.R')\n",
    "library(glue)\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/63sample_7species.pdf',width = 8,height = 6)\n",
    "\n",
    "for (key in c('CDS','z','whole_max30Gap')) {\n",
    "  #print(key)\n",
    "  filename.pre <- c('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/{key}/63sample_7species.{key}.treemix')\n",
    "  filename.extends = c('','.m1k500','.m2k500')\n",
    "  for (filename.extend in filename.extends) {\n",
    "    filename <- paste0(glue(filename.pre), filename.extend)\n",
    "    print(filename)\n",
    "    plot_tree(filename,maintitle = paste0(key,' ',filename.extend))\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use totally 80 samples 9 species\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/80sample_9species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS -N 4 -G 0.8 &\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/80sample_9species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z -N 4 -G 0.8 &\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/fasta2TreemixInput_withGroups.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap -g /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/80sample_9species -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap -N 4 -G 0.8 &\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.treemix -root J_z_zonalis\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.treemix.m1k500 -root J_z_zonalis -m 1 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/CDS/80sample_9species.CDS.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/z/80sample_9species.z.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "/home/xcao/p/treemix/treemix -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.gz -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/whole_max30Gap/80sample_9species.whole_max30Gap.treemix.m2k500 -root J_z_zonalis -m 2 -k 500\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "source('/home/xcao/p/treemix/treemix-1.13/src/plotting_funcs.R')\n",
    "library(glue)\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/80sample_9species.pdf',width = 8,height = 6)\n",
    "\n",
    "for (key in c('CDS','z','whole_max30Gap')) {\n",
    "  #print(key)\n",
    "  filename.pre <- c('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181021treemix/{key}/80sample_9species.{key}.treemix')\n",
    "  filename.extends = c('','.m1k500','.m2k500')\n",
    "  for (filename.extend in filename.extends) {\n",
    "    filename <- paste0(glue(filename.pre), filename.extend)\n",
    "    print(filename)\n",
    "    plot_tree(filename,maintitle = paste0(key,' ',filename.extend))\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mitochondrion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search ```((((010000[SLEN]:016600[SLEN]) AND mitochondrion[FILT]) AND \"insects\"[porgn:__txid6960]) AND \"moths\"[porgn:__txid7088]) AND \"butterflies\"[porgn:__txid37572] ``` in NCBI nucleotide. Got 467 sequences of mitochondrion longer than 10,000bp. 406 of them have the key word \"complete genome\"\n",
    "\n",
    "all 308 files have fastq files. Use MIRA to bait out mitochondrial reads, and assembly with spades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assemble mitochondrion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate commandlines with scripts below. \n",
    "\n",
    "split the commandlines to 3 parts `python /home/xcao/p/xiaolongTools/utils/splitFiles2Nparts.py -i /home/xcao/w/20180905Junonia_coenia/20180913scripts/2018junonia_assemble_mitochondria.cmds -N 3`\n",
    "\n",
    "Run the commandlines with `python /home/xcao/p/xiaolongTools/multiThread.py 24 /home/xcao/w/20180905Junonia_coenia/20180913scripts/2018junonia_assemble_mitochondria.cmds.split0/1/2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229546"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = '''cd /home/xcao/w/20180905Junonia_coenia/20181004mitochondrion && mkdir {name} && cd {name} && /home/xcao/p/mira/mira_4.0.2_linux-gnu_x86_64_static/bin/mirabait /home/xcao/w/genomes/mitochondrion/20181004mitochondrionGenome406butterflies.fasta /archive/butterfly/ready_fastq/{name}_R1.fastq {name}_R1.fastq && /home/xcao/p/mira/mira_4.0.2_linux-gnu_x86_64_static/bin/mirabait /home/xcao/w/genomes/mitochondrion/20181004mitochondrionGenome406butterflies.fasta /archive/butterfly/ready_fastq/{name}_R2.fastq {name}_R2.fastq && /home/xcao/p/SPAdes/SPAdes-3.12.0-Linux/bin/spades.py -k 33,55,77,99 -t 4 --s1 {name}_R1.fastq --s2 {name}_R2.fastq -o . && mv contigs.fasta ../{name}_mito.fa && rm -rf /home/xcao/w/20180905Junonia_coenia/20181004mitochondrion/{name}\n",
    "'''\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20180913scripts/2018junonia_assemble_mitochondria.cmds','w').write(''.join(txt.format(name = name) for name in sample_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene Tree of Junonia coenia\n",
    "\n",
    "Gene tree is defined as trees of many \"genes\". The definition of genes here can be genes, exons, transcripts, or some connected genes. First, use exons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "First, generate a file like:\n",
    "```\n",
    "exon_ID1:123 124 ... 125(line number in the .map file)\n",
    "exon_ID2:123 124 ... 125(line number in the .map file)\n",
    "...\n",
    "```\n",
    "\n",
    "Then, filter each base of the exon, only keep sites with certain gap_ratio (0.5 and 0.8)  \n",
    "Keep exons with sites longer than 50bp or 100bp  \n",
    "generate fasta files  \n",
    "run RAxML to generate the trees  \n",
    "use ASTRAL to combine the gene trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Tree of exons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get exon sites\n",
    "\n",
    "exons were named exon0, exon1, ...  \n",
    "The definition of exon here is a little different from in the gff3 file. Overlapping exons were merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "file_detail = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.position.base.scf.scfpos.gene.rna.exon.cds'\n",
    "file_out = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons'\n",
    "fo = open(file_detail)\n",
    "fout = open(file_out,'w')\n",
    "\n",
    "n = 0\n",
    "exon_id = 'exon{n}'.format(n=n)\n",
    "exon_scf = None\n",
    "exon_positions = []\n",
    "\n",
    "check_exon = 0\n",
    "check_cds = 0\n",
    "\n",
    "for e in fo:\n",
    "    es = e.split()\n",
    "    if es[6] != '0':\n",
    "        check_exon += 1\n",
    "    if es[7] != '0':\n",
    "        check_cds += 1\n",
    "    if es[6] != '0':\n",
    "        if exon_scf is None:\n",
    "            exon_scf = es[2]\n",
    "        if es[2] == exon_scf:\n",
    "            exon_positions.append(es[0])\n",
    "        else:\n",
    "            if exon_scf is not None:\n",
    "                fout.write(exon_id+':'+' '.join(exon_positions)+'\\n')\n",
    "                n += 1\n",
    "                exon_id = 'exon{n}'.format(n=n)\n",
    "                exon_scf = None\n",
    "                exon_positions = []\n",
    "    else:\n",
    "        if exon_scf is not None:\n",
    "            fout.write(exon_id+':'+' '.join(exon_positions)+'\\n')\n",
    "            n += 1\n",
    "            exon_id = 'exon{n}'.format(n=n)\n",
    "            exon_scf = None\n",
    "            exon_positions = []\n",
    "#     if n == 10:\n",
    "#         break\n",
    "if exon_scf is not None:\n",
    "    fout.write(exon_id+':'+' '.join(exon_positions)+'\\n')\n",
    "fout.close()\n",
    "fo.close()\n",
    "print('exon bases', check_exon, 'cds bases', check_cds)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons'\n",
    "import pandas as pd\n",
    "l = []\n",
    "for e in open(file_out):\n",
    "    es = e.split(':')\n",
    "    exon_id = es[0]\n",
    "    exon_len = es[1].count(' ') + 1\n",
    "    l.append([exon_id, exon_len])\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df.columns = ['exon_id','exon_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "exon_lens = np.array(df['exon_len'])\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(exon_lens, bins=np.logspace(np.log10(exon_lens.min()),np.log10(exon_lens.max()), 50))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of exons: 112037 \n",
      "min length of exons: 3 \n",
      "max length of exons: 20961\n"
     ]
    }
   ],
   "source": [
    "print('number of exons:', len(l),'\\nmin length of exons:', df['exon_len'].min(),'\\nmax length of exons:', df['exon_len'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get sites for different setting.\n",
    "```\n",
    "exon_min_lens = [50,100]\n",
    "non_gap_ratios = [0.5,0.8]\n",
    "import itertools\n",
    "dc_gapfilter = {}\n",
    "for non_gap_ratio in non_gap_ratios:\n",
    "    dc_gapfilter[non_gap_ratio] = set(open('/home/xcao/w/20180905Junonia_coenia/20180919wholeGenomeTree/sitesKeep_rmGap_{non_gap_ratio}'.format(non_gap_ratio=non_gap_ratio)).read().split())\n",
    "\n",
    "file_exons = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons'\n",
    "l = open(file_exons).readlines()\n",
    "\n",
    "for exon_min_len, non_gap_ratio in itertools.product(exon_min_lens, non_gap_ratios):\n",
    "    fout = open('/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons.rmGAP{non_gap_ratio}.minlen{exon_min_len}'.format(exon_min_len=exon_min_len,non_gap_ratio=non_gap_ratio),'w')\n",
    "    for e in l:\n",
    "        es = e.split(':')\n",
    "        exon_id = es[0]\n",
    "        exons = [i for i in es[1].split() if i in dc_gapfilter[non_gap_ratio]]\n",
    "        exon_len = len(exons)\n",
    "        if exon_len >= exon_min_len:\n",
    "            fout.write(exon_id+':'+' '.join(exons)+'\\n')\n",
    "    fout.close()\n",
    "```\n",
    "exons:  \n",
    "* 83927:sitesKeep_individual_exons.rmGAP0.5.minlen50  \n",
    "* 68550:sitesKeep_individual_exons.rmGAP0.5.minlen100  \n",
    "* 34440:sitesKeep_individual_exons.rmGAP0.8.minlen50  \n",
    "* 19262:sitesKeep_individual_exons.rmGAP0.8.minlen100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get fasta sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the linux scripts below\n",
    "\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180919mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons.rmGAP0.5.minlen50 -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50 -t 32\n",
    " \n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180919mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons.rmGAP0.5.minlen100 -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen100 -t 30\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180919mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons.rmGAP0.8.minlen50 -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50 -t 32\n",
    " \n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180919mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons.rmGAP0.5.minlen100 -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen100 -t 30\n",
    "\n",
    "```\n",
    "\n",
    "get exon sequences\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180919mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_individual_exons -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons -t 30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transfer the file to bioHPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compress files\n",
    "\n",
    "```\n",
    "cp -r /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen50/ /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen50/ && tar cf - /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen50/ |pigz >/home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen50.tar.gz && rm -rf /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen50/ \n",
    "cp -r /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen100/ /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen100/ && tar cf - /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen100/ |pigz >/home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen100.tar.gz && rm -rf /dev/shm/sitesKeep_individual_exons.rmGAP0.8.minlen100/ \n",
    "cp -r /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50/ /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen50/ && tar cf - /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen50/ |pigz >/home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50.tar.gz && rm -rf /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen50/ \n",
    "cp -r /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen100/ /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen100/ && tar cf - /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen100/ |pigz >/home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen100.tar.gz && rm -rf /dev/shm/sitesKeep_individual_exons.rmGAP0.5.minlen100/ \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scp\n",
    "```\n",
    "scp /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen100.tar.gz s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/geneTrees/\n",
    "scp /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.8.minlen50.tar.gz s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/geneTrees/\n",
    "scp /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen100.tar.gz s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/geneTrees/\n",
    "scp /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50.tar.gz s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/geneTrees/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only focus on the two groups with milen50.\n",
    "\n",
    "request a node to work interactively\n",
    "`salloc -p 256GB -t 12:00:00 srun --pty $SHELL`\n",
    "\n",
    "decompress the two target files\n",
    "```\n",
    "cd /work/biophysics/s185491/2018junonia/geneTrees\n",
    "tar xzf sitesKeep_individual_exons.rmGAP0.5.minlen50.tar.gz\n",
    "tar xzf sitesKeep_individual_exons.rmGAP0.8.minlen50.tar.gz\n",
    "mv /work/biophysics/s185491/2018junonia/geneTrees/dev/shm/* ./\n",
    "```\n",
    "\n",
    "get a full file paths for all exon files.\n",
    "```{python}\n",
    "import glob\n",
    "file_fas = glob.glob('/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP*.minlen50/exon*')\n",
    "print(len(file_fas))\n",
    "open('/work/biophysics/s185491/2018junonia/geneTrees/20180927fastaFiles.minlen50','w').write('\\n'.join(file_fas))\n",
    "```\n",
    "total number of alignments to run RAxML: 118367\n",
    "\n",
    "scripts to run RAxML and keep the `RAxML_bestTree` files. Works for single input. Python file stored in `/home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py`\n",
    "```\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "def runRAxMLForIndividualSmallFiles(filename):\n",
    "    '''\n",
    "    filename is the full path of a aligned fasta file\n",
    "    output and save the best tree in a folder by adding '.RAxMLbestTree' to the folder name.\n",
    "    filename = '/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.5.minlen50/exon78368'\n",
    "    bestTree is stored in '/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.5.minlen50.RAxMLbestTree/exon78368'\n",
    "    '''\n",
    "    folder = os.path.dirname(filename)\n",
    "    name = os.path.basename(filename)\n",
    "    outfolder = folder + '.RAxMLbestTree/'\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    \n",
    "    workFolder = '/dev/shm/'+name\n",
    "    if not os.path.exists(workFolder):\n",
    "        os.makedirs(workFolder)\n",
    "    os.chdir(workFolder)\n",
    "    shutil.copy(filename,'./')\n",
    "    commandline = '/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC -m GTRGAMMA -p 234 -s {name} -n {name}'.format(name=name)\n",
    "    os.system(commandline)\n",
    "    shutil.copy('RAxML_bestTree.' + name, outfolder + name)\n",
    "    shutil.rmtree(workFolder)\n",
    "    print('done for', name)\n",
    "\n",
    "description = '''\n",
    "    filename is the full path of a aligned fasta file\n",
    "    output and save the best tree in a folder by adding '.RAxMLbestTree' to the folder name.\n",
    "    filename = '/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.5.minlen50/exon78368'\n",
    "    bestTree is stored in '/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.5.minlen50.RAxMLbestTree/exon78368'\n",
    "    '''\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    print(description)\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "    parser.add_argument('-i','--input', help = 'input file storing the location of aligned fasta file', required=True)\n",
    "    f = parser.parse_args()\n",
    "    runRAxMLForIndividualSmallFiles(f.input)\n",
    "\n",
    "```\n",
    "\n",
    "test one:  \n",
    "`python3 /home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py -i /work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.5.minlen50/exon78369`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totally 118367 jobs. Split them to 40 jobs. Generate 40 cmd files and 40 qsub files. (note: there is a limitation for BioHPC. only 16 nodes can be used at a single time point)\n",
    "```\n",
    "files = open('/work/biophysics/s185491/2018junonia/geneTrees/20180927fastaFiles.minlen50').read().split()\n",
    "tasks = 40\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/geneTrees/20180927qsub_scripts/'\n",
    "file_cmd_names = [outfolder+'RAxML.cmds'+str(i) for i in range(tasks)]\n",
    "file_cmds = [open(f,'w') for f in file_cmd_names]\n",
    "for n, file in enumerate(files):\n",
    "    cmd = 'python3 /home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py -i '+file+'\\n'\n",
    "    file_cmds[n % tasks].write(cmd)\n",
    "\n",
    "for e in file_cmds:\n",
    "    e.close()\n",
    "\n",
    "qsub_txt = '''#!/bin/bash\n",
    "#SBATCH --job-name={jobname}\n",
    "#SBATCH --partition=super\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output={jobname}.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/python3 /home2/s185491/p/xiaolongTools/multiThread.py 32 {file_cmd}\n",
    "\n",
    "\n",
    "'''\n",
    "for n in range(tasks):\n",
    "    fout = open(outfolder+'qsub_RAxML.cmds'+str(n),'w')\n",
    "    fout.write(qsub_txt.format(jobname = 'qsub_RAxML.cmds'+str(n), file_cmd = file_cmd_names[n]))\n",
    "    fout.close()\n",
    "\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run ASTRUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the trees from `sitesKeep_individual_exons.rmGAP0.5.minlen50.RAxMLbestTree.all` to 1000 parts\n",
    "```\n",
    "python /home/xcao/p/xiaolongTools/utils/splitFiles2Nparts.py -i /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50.RAxMLbestTree.all -N 1000 -o /home/xcao/w/20180905Junonia_coenia/20180925geneTree/sitesKeep_individual_exons.rmGAP0.5.minlen50.RAxMLbestTreeSplit1000\n",
    "```\n",
    "run Astral\n",
    "```\n",
    "txt = '''java -Xmx3000m -jar /home2/s185491/p/Astral/astral.5.6.2.jar -i /work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.8.minlen50.RAxMLbestTree.split/sitesKeep_individual_exons.rmGAP0.8.minlen50.RAxMLbestTree.all.split{n} -o /work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.8.minlen50.RAxMLbestTree.split/sitesKeep_individual_exons.rmGAP0.8.minlen50.RAxMLbestTree.all.split{n}.sum\n",
    "'''\n",
    "fout = open('/work/biophysics/s185491/2018junonia/geneTrees/sitesKeep_individual_exons.rmGAP0.8.minlen50.RAxMLbestTree.split.cmd','w')\n",
    "for n in range(500):\n",
    "    fout.write(txt.format(n=n))\n",
    "fout.close()\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRUCTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first look at the result and test with some previous scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get input from aligned fasta files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/xcao/w/20180905Junonia_coenia/20180928STRUCTURE/\n",
    "python3 /home/xcao/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2STRUCTUREinput.py /home/xcao/w/20180905Junonia_coenia/20180919wholeGenomeTree/junonia_whole_genome_0_filter_CDS_noZ_rmGap0.85.fa\n",
    "\n",
    "cd /home/xcao/w/20180905Junonia_coenia/20180928STRUCTURE/\n",
    "python3 /home/xcao/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2STRUCTUREinput.py /home/xcao/w/20180905Junonia_coenia/20180919wholeGenomeTree/junonia_whole_genome_0_rmGap_0.9.fa \n",
    "\n",
    "cd /home/xcao/w/20180905Junonia_coenia/20180928STRUCTURE/\n",
    "python3 /home/xcao/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2STRUCTUREinput.py /home/xcao/w/20180905Junonia_coenia/20180913Zchromsome/20180913junonia_z_all_rmGap_0.8.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many of Wenlin's code go work with both strand of samples. e.g. Sample '5999', the sequence should include 5999_1 and 5999_2. That's why I cannot use some of the code directly, because I only have single stranded sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20181017 structure with CDS, z, and whole genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified the `/home/xcao/p/xiaolongTools/utils/fasta2STRUCTUREinput_haploid_nt.py` file to work with haploid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert files to structure input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default Ncut=4, gapcut=0.8\n",
    "```\n",
    "python /home/xcao/p/xiaolongTools/utils/fasta2STRUCTUREinput_haploid_nt.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS\n",
    "python /home/xcao/p/xiaolongTools/utils/fasta2STRUCTUREinput_haploid_nt.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap\n",
    "python /home/xcao/p/xiaolongTools/utils/fasta2STRUCTUREinput_haploid_nt.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z\n",
    "```\n",
    "numbers of snps that can be used by STRUCTURE\n",
    "* CDS: 1687546 (total bases 22515575, ratio 0.075)\n",
    "* whole_max30Gap: 10325979  (total bases 57707714, ratio 0.179)\n",
    "* z: 702392 (total bases 19003383, ratio 0.037)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract from strucure input and further filter\n",
    "\n",
    "\n",
    "the four key species, the sample count is\n",
    "```\n",
    "Junonia coenia coenia:20\n",
    "Junonia nigrosuffusa:20\n",
    "Junonia coenia grisea:16\n",
    "Junonia nigrosuffusaTX:9\n",
    "```\n",
    "and `Junonia MEXICANspecies:31`  \n",
    "filter with `Filter_7USA Filter4groups_withMX Filter_4groups`, with the second SNP greater than 9, gapcut 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def filter_eachPosition(x, gapcut = 0.9, second_snp_min = 9):\n",
    "    basecount = Counter(x)\n",
    "    totalcount = len(x)\n",
    "    if basecount[-9] <= totalcount * (1-gapcut):\n",
    "        basecount_sort = basecount.most_common()\n",
    "        basecount_sort = [e for e in basecount_sort if e[0]!= -9]\n",
    "        if len(basecount_sort) > 1:\n",
    "            if basecount_sort[1][1]>second_snp_min:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "outfolder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/'\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "df_summary['Number'] = sample_prefix\n",
    "print('unique sample prefix', len(set(df_summary['Number'])))\n",
    "df_summary = df_summary[df_summary['permanet_remove'] == 0]\n",
    "print('samples in use', len(set(df_summary['Number'])))\n",
    "\n",
    "filter_columns = 'Filter_7USA Filter4groups_withMX Filter_4groups'.split()\n",
    "files_STRUCTUREinput = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS.STRUCTURE\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap.STRUCTURE\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z.STRUCTURE\n",
    "'''.split()\n",
    "\n",
    "#sample_keep = list(df_summary[df_summary[filter_columns[0]] != 0]['Number'])\n",
    "#df_STRUCTURE = pd.read_csv(files_STRUCTUREinput[0],sep='\\t',index_col=0,header=None,low_memory=False)\n",
    "#df_STRUCTURE = df_STRUCTURE.astype(np.int8)\n",
    "#df_STRUCTURE_keep = df_STRUCTURE.loc[sample_keep]\n",
    "#df_STRUCTURE_keep_filter = df_STRUCTURE_keep.loc[:,df_STRUCTURE_keep.apply(filter_eachPosition,axis=0)]\n",
    "#outname = os.path.join(outfolder,os.path.basename('.'.join([files_STRUCTUREinput[0],filter_columns[0]])))\n",
    "#df_STRUCTURE_keep_filter.to_csv(outname,sep='\\t',header=None)\n",
    "\n",
    "\n",
    "for f_STRUCTUREinput in files_STRUCTUREinput:\n",
    "    df_STRUCTURE = pd.read_csv(f_STRUCTUREinput,sep='\\t',index_col=0,header=None,low_memory=False)\n",
    "    df_STRUCTURE = df_STRUCTURE.astype(np.int8)\n",
    "    for f_column in filter_columns:\n",
    "        sample_keep = list(df_summary[df_summary[f_column] != 0]['Number'])\n",
    "        df_STRUCTURE_keep = df_STRUCTURE.loc[sample_keep]\n",
    "        print('wroking on', f_STRUCTUREinput, f_column)\n",
    "        df_STRUCTURE_keep_filter =      df_STRUCTURE_keep.loc[:,df_STRUCTURE_keep.apply(filter_eachPosition,axis=0)]\n",
    "        outname = os.path.join(outfolder,os.path.basename('.'.join([f_STRUCTUREinput,f_column])))\n",
    "        df_STRUCTURE_keep_filter.to_csv(outname,sep='\\t',header=None)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run STRUCTURE\n",
    "```\n",
    "import os\n",
    "\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/test'\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/test/test.STRUCTURE'\n",
    "file_para = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/mainparams.ref'\n",
    "out_para = os.path.join(folder,'mainparams')\n",
    "\n",
    "num_population = 2\n",
    "num_burnin = 10000\n",
    "num_numreps = 20000\n",
    "infile = filename\n",
    "outfile = filename+'.result'\n",
    "num_individuals = 0\n",
    "num_loci = 0\n",
    "num_ploidy = 1\n",
    "\n",
    "\n",
    "with open(filename) as f:\n",
    "    fout = open(filename+'.1','w')\n",
    "    for e in f:\n",
    "        e1,e2 = e.split('\\t',1)\n",
    "        fout.write('\\t'.join([e1,'1',e2]))\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "mainparams = open(file_para).read()\n",
    "open(out_para,'w').write(mainparams.format(num_population=num_population, num_burnin=num_burnin, num_numreps=num_numreps, infile=infile, outfile=outfile, num_individuals=num_individuals, num_loci=num_loci, num_ploidy=num_ploidy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the large files to small ones and run\n",
    "\n",
    "```\n",
    "cd /work/biophysics/s185491/2018junonia/20181018STRUCTURE\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitSTRUCTUREinput2NPartsContinuingEqualStepOrRandom.py -i /work/biophysics/s185491/2018junonia/20181018STRUCTURE/20181010junoniaSeq_CDS.STRUCTURE.Filter_7USA -N 3 -m 1\n",
    "```\n",
    "\n",
    "```\n",
    "import os\n",
    "\n",
    "#folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/'\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181018STRUCTURE'\n",
    "\n",
    "files_STRUCTURE = '''20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_4groups.split3.0\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_4groups.split3.1\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_4groups.split3.2\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.0\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.1\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.2\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.3\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.4\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.5\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.6\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.7\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.8\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.9\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.10\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA.split12.11\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.0\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.1\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.2\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.3\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.4\n",
    "20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split6.5\n",
    "20181010junoniaSeq_CDS.STRUCTURE.Filter_7USA.split3.0\n",
    "20181010junoniaSeq_CDS.STRUCTURE.Filter_7USA.split3.1\n",
    "20181010junoniaSeq_CDS.STRUCTURE.Filter_7USA.split3.2\n",
    "'''.split()\n",
    "\n",
    "filename = 'test.STRUCTURE'\n",
    "file_para = os.path.join(folder,'mainparams.ref')\n",
    "file_paraExtra = os.path.join(folder,'extraparams.ref')\n",
    "#STRUCTURE = '/home/xcao/p/STRUCTURE/structure_kernel_src6/structure'\n",
    "STRUCTURE = '/home2/s185491/p/STRUCTURE/structure_kernel_src/structure'\n",
    "\n",
    "num_population = 2\n",
    "num_burnin = 10000\n",
    "num_numreps = 20000\n",
    "infile = filename\n",
    "outfile = filename+'.result'\n",
    "num_individuals = 0\n",
    "num_loci = 0\n",
    "num_ploidy = 1\n",
    "\n",
    "scripts = open(os.path.join(folder,'20181019scripts.txt'),'w')\n",
    "for filename in files_STRUCTURE:\n",
    "    workfolder = os.path.join(folder,filename+'.run')\n",
    "    if not os.path.exists(workfolder):\n",
    "        os.makedirs(workfolder)\n",
    "    mainparams = open(file_para).read()\n",
    "    num_individuals = 0\n",
    "    num_loci = 0\n",
    "    for line in open(os.path.join(folder,filename)):\n",
    "        num_individuals += 1\n",
    "    num_loci = line.count('\\t')\n",
    "    print(filename,'samples',num_individuals,'loci',num_loci)\n",
    "    for num_population in range(2,9):\n",
    "        for seed in [1000]:\n",
    "            outfile = os.path.join(workfolder,filename+'.pop'+str(num_population)+'.seed'+str(seed))\n",
    "            out_para = os.path.join(workfolder,'mainparams.pop'+str(num_population)+'.seed'+str(seed))\n",
    "            open(out_para,'w').write(mainparams.format(num_population=num_population, num_burnin=num_burnin, num_numreps=num_numreps, infile=os.path.join(folder, filename), outfile=outfile, num_individuals=num_individuals, num_loci=num_loci, num_ploidy=num_ploidy))\n",
    "            scripts.write('nohup {STRUCTURE} -e {file_paraExtra} -m {out_para} -D {seed} &\\n'.format(workfolder=workfolder, STRUCTURE=STRUCTURE, file_paraExtra=file_paraExtra, out_para=out_para,seed=seed))\n",
    "#    break\n",
    "scripts.close()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the file path is too long, STRUCTURE will run into problems. The 20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter4groups_withMX.split were re-runned, and changed to w30G.F40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181018STRUCTURE/'\n",
    "files = glob.glob(folder+'*/*_f')\n",
    "files_name = [os.path.basename(f) for f in files]\n",
    "\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20181018STRUCTURE/20181025result/'\n",
    "for f in files:\n",
    "    commandline = 'python3 /home2/s185491/p/xiaolongTools/utils/extractStructureTableFromCommandlineOutput.py -i {filein} -o {fileout}'.format(filein = f, fileout = os.path.join(outfolder, os.path.basename(f)))\n",
    "    os.system(commandline)\n",
    "\n",
    "#group them to three folders manually: 4groups, 4groups_withMX, 7USA\n",
    "#generate a tsv file with discription of files manually. copy the result from bioHPC to alea\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "library(readxl)\n",
    "library(tidyr)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "\n",
    "filename <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181017STRUCTURE/20181025result/files.txt'\n",
    "xdf_files = read.csv(filename,header = TRUE,sep='\\t',stringsAsFactors = FALSE)\n",
    "xdf_files <- xdf_files[order(xdf_files$Filter,xdf_files$baseType,xdf_files$population,xdf_files$splitParts,xdf_files$splitID,xdf_files$seed),]\n",
    "#xdf_filegroup <- split(xdf_files, list(xdf_files$Filter, xdf_files$baseType))\n",
    "#\"4goupMX\" \"4group\"  \"7USA\"   \n",
    "# \"CDS\"   \"whole\" \"z\"\n",
    "\n",
    "filename2 <- '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "xdf_info <- read_excel(filename2,sheet = 'all')\n",
    "v_id2Treename <- xdf_info$Treename\n",
    "names(v_id2Treename) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "v_id2species <- xdf_info$Species\n",
    "names(v_id2species) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "\n",
    "\n",
    "F.renameHead <- function(x){\n",
    "  x[1] = 'sample'\n",
    "  x[2:length(x)] = paste0('pop',1:(length(x)-1))\n",
    "  return(x)\n",
    "}\n",
    "\n",
    "F.plotOneline <- function(file,xtext = FALSE){\n",
    "  xdf_s <- read.csv(file$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "  colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "  xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "  xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "  xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species,-treename)\n",
    "  xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "  \n",
    "  p <- ggplot(xdf_s.clear,aes(x=treename,y=fraction,fill=pop)) + geom_bar(stat = 'identity') + ylab(paste0(file$Filter,'_s',file$seed,'_',file$baseType,'_K=',file$population,\" \",file$splitParts,'_',file$splitID))\n",
    "  if (xtext) p <- p + theme(legend.position=\"none\", axis.text.x= element_text(angle = 90, hjust = 1), axis.title.x = element_blank())\n",
    "  else p <- p <- p + theme(legend.position=\"none\", axis.text.x= element_blank(), axis.title.x = element_blank())\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "\n",
    "F.plotMultipleLines <- function(filelines){\n",
    "  p <- list()\n",
    "  for (i in 1:nrow(filelines)){\n",
    "    p[[i]] <- F.plotOneline(filelines[i,])\n",
    "  }\n",
    "  #p[[i+1]] <- F.plotOneline(filelines[i+1,], xtext = TRUE)\n",
    "  return(grid.arrange(grobs=p,ncol=1))\n",
    "}\n",
    "\n",
    "filelines = xdf_files[xdf_files$Filter == '4group' & xdf_files$baseType == 'CDS' & xdf_files$population<5,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '4group' & xdf_files$baseType == 'z' & xdf_files$population<5,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '4group' & xdf_files$baseType == 'whole' & xdf_files$population<5,]\n",
    "F.plotMultipleLines(filelines)\n",
    "\n",
    "filelines = xdf_files[xdf_files$Filter == '4goupMX' & xdf_files$baseType == 'CDS' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '4goupMX' & xdf_files$baseType == 'z' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '4goupMX' & xdf_files$baseType == 'whole' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "\n",
    "filelines = xdf_files[xdf_files$Filter == '7USA' & xdf_files$baseType == 'CDS' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '7USA' & xdf_files$baseType == 'z' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "filelines = xdf_files[xdf_files$Filter == '7USA' & xdf_files$baseType == 'whole' & xdf_files$population<8,]\n",
    "F.plotMultipleLines(filelines)\n",
    "\n",
    "file <- xdf_files[1,]\n",
    "xdf_s <- read.csv(file$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species,-treename)\n",
    "xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "\n",
    "ggplot(xdf_s.clear,aes(x=treename,y=fraction,fill=pop)) + geom_bar(stat = 'identity')+ theme(legend.position=\"none\", axis.text.x=element_blank(),axis.title.x = element_blank()) + ylab(paste0(file$Filter,'_s',file$seed,'_',file$baseType,'_K=',file$population,\" \",file$splitParts,'_',file$splitID))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run structure 7 USA 9 samples per group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select best samples\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all',dtype=str)\n",
    "df_summary['Number'] = df_summary['Number'].apply(lambda x:x.split('-')[-1])\n",
    "df_use = df_summary[df_summary['short_names'] != 'nan'].loc[:,['Number','short_names','genome_coverage']]\n",
    "df_use['genome_coverage'] = df_use['genome_coverage'].astype(float)\n",
    "df_use = df_use.sort_values(by = ['short_names','genome_coverage'], ascending=[True,False])\n",
    "species = list(df_use['short_names'].unique())\n",
    "df_use = df_use.reset_index()\n",
    "df_use.loc[0,'speciesID'] = 1\n",
    "for i in range(df_use.shape[0]-1):\n",
    "    if df_use.loc[i+1, 'short_names'] == df_use.loc[i, 'short_names']:\n",
    "        df_use.loc[i+1,'speciesID'] = df_use.loc[i,'speciesID'] + 1\n",
    "    else:\n",
    "        df_use.loc[i+1,'speciesID'] = 1\n",
    "df_use_keep = df_use[df_use['speciesID']<=9]\n",
    "df_use_keep.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181030STRUCTURE7USA/species', columns=['Number','short_names'], header=None, sep='\\t', index=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bioHPC\n",
    "```\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filename = '/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/species'\n",
    "df = pd.read_csv(filename,sep='\\t',header=None)\n",
    "samples = set(df[0])\n",
    "\n",
    "file_structures = '''/work/biophysics/s185491/2018junonia/20181018STRUCTURE/20181010junoniaSeq_CDS.STRUCTURE.Filter_7USA\n",
    "/work/biophysics/s185491/2018junonia/20181018STRUCTURE/20181010junoniaSeq_whole_max30Gap.STRUCTURE.Filter_7USA\n",
    "/work/biophysics/s185491/2018junonia/20181018STRUCTURE/20181010junoniaSeq_z.STRUCTURE.Filter_7USA\n",
    "'''.split()\n",
    "outfoler = '/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/'\n",
    "\n",
    "for f in file_structures:\n",
    "    outfile = os.path.join(outfoler,os.path.basename(f).split('Seq_')[1])\n",
    "    fout = open(outfile,'w')\n",
    "    for line in open(f):\n",
    "        if line.split('\\t',maxsplit=1)[0] in samples:\n",
    "            fout.write(line)\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run structure\n",
    "\n",
    "split whole_max30Gap to 6 parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "files_STRUCTURE = '''/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/CDS.STRUCTURE.Filter_7USA\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.0\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.1\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.2\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.3\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.4\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/whole_max30Gap.STRUCTURE.Filter_7USA.split6.5\n",
    "/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/z.STRUCTURE.Filter_7USA\n",
    "'''.split()\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181018STRUCTURE'\n",
    "filename = 'test.STRUCTURE'\n",
    "file_para = os.path.join(folder,'mainparams.ref')\n",
    "file_paraExtra = os.path.join(folder,'extraparams.ref')\n",
    "STRUCTURE = '/home2/s185491/p/STRUCTURE/structure_kernel_src/structure'\n",
    "\n",
    "num_population = 2\n",
    "num_burnin = 10000\n",
    "num_numreps = 20000\n",
    "infile = filename\n",
    "outfile = filename+'.result'\n",
    "num_individuals = 0\n",
    "num_loci = 0\n",
    "num_ploidy = 1\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA'\n",
    "scripts = open(os.path.join(folder,'20181030scripts.txt'),'w')\n",
    "for filename in files_STRUCTURE:\n",
    "    workfolder = os.path.join(folder,filename+'.run')\n",
    "    if not os.path.exists(workfolder):\n",
    "        os.makedirs(workfolder)\n",
    "    mainparams = open(file_para).read()\n",
    "    num_individuals = 0\n",
    "    num_loci = 0\n",
    "    for line in open(os.path.join(folder,filename)):\n",
    "        num_individuals += 1\n",
    "    num_loci = line.count('\\t')\n",
    "    print(filename,'samples',num_individuals,'loci',num_loci)\n",
    "    for num_population in range(2,9):\n",
    "        if 'split' in filename:\n",
    "            seeds = [1000]\n",
    "        else:\n",
    "            seeds = [1000,3000,5000]\n",
    "        for seed in seeds:\n",
    "            outfile = os.path.join(workfolder,filename+'.pop'+str(num_population)+'.seed'+str(seed))\n",
    "            out_para = os.path.join(workfolder,'mainparams.pop'+str(num_population)+'.seed'+str(seed))\n",
    "            open(out_para,'w').write(mainparams.format(num_population=num_population, num_burnin=num_burnin, num_numreps=num_numreps, infile=os.path.join(folder, filename), outfile=outfile, num_individuals=num_individuals, num_loci=num_loci, num_ploidy=num_ploidy))\n",
    "            scripts.write('nohup {STRUCTURE} -e {file_paraExtra} -m {out_para} -D {seed} &\\n'.format(workfolder=workfolder, STRUCTURE=STRUCTURE, file_paraExtra=file_paraExtra, out_para=out_para,seed=seed))\n",
    "#    break\n",
    "scripts.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect data\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/'\n",
    "files = glob.glob(folder+'**/*_f', recursive=True)\n",
    "files_name = [os.path.basename(f) for f in files]\n",
    "\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20181030STRUCTURE7USA/20181104result/'\n",
    "for f in files:\n",
    "    commandline = 'python3 /home2/s185491/p/xiaolongTools/utils/extractStructureTableFromCommandlineOutput.py -i {filein} -o {fileout}'.format(filein = f, fileout = os.path.join(outfolder, os.path.basename(f)))\n",
    "    os.system(commandline)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run structure 9 species 80 samples\n",
    "\n",
    "add J_Mexican and wMex species\n",
    "#### get input for STRUCTURE\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def filter_eachPosition(x, gapcut = 0.5, second_snp_min = 5):\n",
    "    basecount = Counter(x)\n",
    "    totalcount = len(x)\n",
    "    if basecount[-9] <= totalcount * (1-gapcut):\n",
    "        basecount_sort = basecount.most_common()\n",
    "        basecount_sort = [e for e in basecount_sort if e[0]!= -9]\n",
    "        if len(basecount_sort) > 1:\n",
    "            if basecount_sort[1][1]>second_snp_min:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/'\n",
    "filename = '/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/species'\n",
    "df = pd.read_csv(filename,sep='\\t',header=None)\n",
    "samples = list(df[0])\n",
    "\n",
    "files_STRUCTUREinput = '''/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/20181010junoniaSeq_CDS.STRUCTURE\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/20181010junoniaSeq_whole_max30Gap.STRUCTURE\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/20181010junoniaSeq_z.STRUCTURE\n",
    "'''.split()\n",
    "\n",
    "for f_STRUCTUREinput in files_STRUCTUREinput:\n",
    "    df_STRUCTURE = pd.read_csv(f_STRUCTUREinput,sep='\\t',index_col=0,header=None,low_memory=False)\n",
    "    df_STRUCTURE = df_STRUCTURE.astype(np.int8)\n",
    "    df_STRUCTURE_keep = df_STRUCTURE.loc[samples]\n",
    "    df_STRUCTURE_keep_filter =      df_STRUCTURE_keep.loc[:,df_STRUCTURE_keep.apply(filter_eachPosition,axis=0)]\n",
    "    outname = os.path.join(outfolder,os.path.basename(f_STRUCTUREinput).split('Seq_')[1]+'9')\n",
    "    df_STRUCTURE_keep_filter.to_csv(outname,sep='\\t',header=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split file. remove .STRUCTURE in filename.\n",
    "```\n",
    "cd /work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitSTRUCTUREinput2NPartsContinuingEqualStepOrRandom.py -i /work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9 -N 6 -m 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run (6 seeds now, may run up to 7 days.)\n",
    "```\n",
    "import os\n",
    "files_STRUCTURE = '''/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/CDS9\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.0\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.1\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.2\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.3\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.4\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/whole_max30Gap9.split6.5\n",
    "/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species/z9\n",
    "'''.split()\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181018STRUCTURE'\n",
    "file_para = os.path.join(folder,'mainparams.ref')\n",
    "file_paraExtra = os.path.join(folder,'extraparams.ref')\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181119STRUCTURE9Species'\n",
    "scripts = open(os.path.join(folder,'20181120scripts.txt'),'w')\n",
    "STRUCTURE = '/home2/s185491/p/STRUCTURE/structure_kernel_src/structure'\n",
    "num_population = 2\n",
    "num_burnin = 10000\n",
    "num_numreps = 20000\n",
    "num_individuals = 0\n",
    "num_loci = 0\n",
    "num_ploidy = 1\n",
    "\n",
    "for filename in files_STRUCTURE:\n",
    "    workfolder = os.path.join(folder,filename+'.run')\n",
    "    if not os.path.exists(workfolder):\n",
    "        os.makedirs(workfolder)\n",
    "    mainparams = open(file_para).read()\n",
    "    num_individuals = 0\n",
    "    num_loci = 0\n",
    "    for line in open(os.path.join(folder,filename)):\n",
    "        num_individuals += 1\n",
    "    num_loci = line.count('\\t')\n",
    "    print(filename,'samples',num_individuals,'loci',num_loci)\n",
    "    for num_population in range(2,9):\n",
    "        if 'split' in filename:\n",
    "            seeds = [1000]\n",
    "        else:\n",
    "            seeds = [1000,3000,5000,7000,9000,11000]\n",
    "        for seed in seeds:\n",
    "            outfile = filename+'.pop'+str(num_population)+'.seed'+str(seed)\n",
    "            out_para = os.path.join(workfolder,'mainparams.pop'+str(num_population)+'.seed'+str(seed))\n",
    "            open(out_para,'w').write(mainparams.format(num_population=num_population, num_burnin=num_burnin, num_numreps=num_numreps, infile=os.path.join(folder, filename), outfile=outfile, num_individuals=num_individuals, num_loci=num_loci, num_ploidy=num_ploidy))\n",
    "            scripts.write('nohup {STRUCTURE} -e {file_paraExtra} -m {out_para} -D {seed} &\\n'.format(workfolder=workfolder, STRUCTURE=STRUCTURE, file_paraExtra=file_paraExtra, out_para=out_para,seed=seed))\n",
    "#    break\n",
    "scripts.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test split to 100 parts and run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bioHPC, `python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NContinueingParts.py -i /work/biophysics/s185491/2018junonia/alignments/junonia_whole_genome_0_rmGap_0.85.fa -o /work/biophysics/s185491/2018junonia/alignments/junonia_whole_genome_0_rmGap_0.85.split100\n",
    "`, split to 100 parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate qsub jobs for 100 files.\n",
    "\n",
    "```\n",
    "qsub_txt = '''#!/bin/bash\n",
    "#SBATCH --job-name={file_id}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output={file_id}.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/python3 /home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py -i /work/biophysics/s185491/2018junonia/alignments/junonia_whole_genome_0_rmGap_0.85.split100/junonia_whole_genome_0_rmGap_0.85.fa.split100.{file_id} -T 32\n",
    "\n",
    "'''\n",
    "for n in range(100):\n",
    "    fout = open('/work/biophysics/s185491/2018junonia/alignments/junonia_whole_genome_0_rmGap_0.85.split100.jobs/'+str(n),'w')\n",
    "    fout.write(qsub_txt.format(file_id=n))\n",
    "    fout.close()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 263 high quality samples whole Genome, CDS, z-chromosmoe Trees, 100 parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "263 high quality samples. or sites info will be ordered according to the chromosome match with Heliconius   \n",
    "For genome Tree, only keep sites with less or equal to 30 gaps, which is equal to 9.85% (57,707,714bp) of the genome size.  \n",
    "For CDS Tree, all sites will be kept, which will be 3.84% (22,515,575) of the genome size\n",
    "For z-chromosome Trees, still use the scaffolds that agrees with 3 species, which is 3.24% (19,003,382) of the genome size  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "file_gapCounts = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181010mapFile263.GapCounts.npInt16'\n",
    "file_match2Heliconius = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.txt'\n",
    "file_CDS_sites = '/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_CDS'\n",
    "file_z_scaffolds = '/home/xcao/w/20180905Junonia_coenia/20180913junonia_scaffolds_z_by_3species'\n",
    "out_wholeGenome_max30Gap = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_whole_max30Gap'\n",
    "out_CDS_sites = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_CDS'\n",
    "out_z_sites = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_z'\n",
    "\n",
    "def iterateJunoniaMatch2Helicounius(filename):\n",
    "    '''\n",
    "    given the 20181005JunoniaPositionHeliconiusChrOrder\n",
    "    yield the chromosome_id, scaffold_id and base position each time\n",
    "    the file is like:\n",
    "    '1:(000635F:558391428,558537911);(000793F:575740860,575815531);...'\n",
    "    return 1,000635F,558391428\n",
    "    '''\n",
    "    l = open(filename).readlines()\n",
    "    l_chr = []\n",
    "    dc_chr = {}\n",
    "    dc_scf = {}\n",
    "    \n",
    "    for line in l:\n",
    "        chr_id, scfs_info = line.strip().split(':',1)\n",
    "        l_chr.append(chr_id)\n",
    "        dc_chr[chr_id] = []\n",
    "        for scf_info in scfs_info.strip(';').split(';'):\n",
    "            scf_id, start_end = scf_info[1:-1].split(':')\n",
    "            dc_chr[chr_id].append(scf_id)\n",
    "            start,end = start_end.split(',')\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            dc_scf[scf_id] =(start,end)\n",
    "    \n",
    "    for chr_id in l_chr:\n",
    "        for scf_id in dc_chr[chr_id]:\n",
    "            start, end = dc_scf[scf_id]\n",
    "            for location in range(start,end):\n",
    "                yield chr_id, scf_id, location\n",
    "        \n",
    "\n",
    "with open(file_gapCounts,'rb') as f:\n",
    "    gapCounts = pickle.load(f)\n",
    "\n",
    "sites_max30Gap = np.where(gapCounts <= 30)[0]\n",
    "sites_max30Gap = set(sites_max30Gap)\n",
    "\n",
    "fout = open(out_wholeGenome_max30Gap,'w')\n",
    "for location in iterateJunoniaMatch2Helicounius(file_match2Heliconius):\n",
    "    if location[2] in sites_max30Gap:\n",
    "        fout.write(str(location[2])+'\\n')\n",
    "fout.close()\n",
    "\n",
    "sites_CDS = open(file_CDS_sites).read().split()\n",
    "sites_CDS = [int(e) for e in sites_CDS]\n",
    "sites_CDS = set(sites_CDS)\n",
    "fout = open(out_CDS_sites,'w')\n",
    "for location in iterateJunoniaMatch2Helicounius(file_match2Heliconius):\n",
    "    if location[2] in sites_CDS:\n",
    "        fout.write(str(location[2])+'\\n')\n",
    "fout.close()\n",
    "\n",
    "z_scf = open(file_z_scaffolds).read().split()\n",
    "z_scf = set(z_scf)\n",
    "fout = open(out_z_sites,'w')\n",
    "for location in iterateJunoniaMatch2Helicounius(file_match2Heliconius):\n",
    "    if location[1] in z_scf:\n",
    "        fout.write(str(location[2])+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get map file locations\n",
    "```\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "import pandas as pd\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "df_summary = df_summary[df_summary['permanet_remove'] == 0]\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "\n",
    "out_mapfile_locations = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location263.txt'\n",
    "fout = open(out_mapfile_locations,'w')\n",
    "for e in sample_prefix:\n",
    "    fout.write('/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map\\n')\n",
    "fout.close()\n",
    "```\n",
    "extract sequences\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location263.txt -b /home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_CDS -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_CDS -t 32 -s 0\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location263.txt -b /home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_whole_max30Gap -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_whole_max30Gap -t 32 -s 0\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location263.txt -b /home/xcao/w/20180905Junonia_coenia/20181010Trees/sitesKeep_ordered_z -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010junoniaSeq_z -t 32 -s 0\n",
    "\n",
    "```\n",
    "\n",
    "note: getSequencesFromMapFilesInSites_small_memory will significantly reduces memory consumption, but also runs slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split files and run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split files\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -i /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_whole_max30Gap -o /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_whole_max30Gap_split -N 100 -b 10000 -s 0 -m 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate qsub files, an example\n",
    "```\n",
    "qsub_txt = '''#!/bin/bash\n",
    "#SBATCH --job-name={file_id}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output={file_id}.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/python3 /home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py -i /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split/20181010junoniaSeq_CDS.split100.{file_id} -T 32\n",
    "\n",
    "'''\n",
    "for n in range(100):\n",
    "    fout = open('/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaJobs/CDS'+str(n),'w')\n",
    "    fout.write(qsub_txt.format(file_id=n))\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine and sumtrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine\n",
    "```\n",
    "cat /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree/* >/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.all\n",
    "```\n",
    "sumtrees\n",
    "```\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.all >/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.sum\n",
    "```\n",
    "change format \n",
    "```{python3}\n",
    "import dendropy\n",
    "mle = dendropy.Tree.get(path=\"/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.sum\", schema=\"nexus\")\n",
    "mle.write(path=\"/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.sum.newick\", schema=\"newick\")\n",
    "```\n",
    "rename\n",
    "```\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.sum.newick\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the sequences with walking step method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen10000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/fraglen10000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen20000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/fraglen20000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen50000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/fraglen50000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106All263sample/fraglen100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run MrBayes/ExaML to get trees from whole Genome, CDS, z-chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, use the splitted sequences to speed up.\n",
    "#### filter sequences, only keep sites with information to tell the differences\n",
    "baseKeepMinCount=4, minNonGapRatio=0.8. Apply the same filter parameter to the whole genome  \n",
    "baseKeepMinCount=2, minNonGapRatio=0.2. Apply the same filter parameter to CDS and z\n",
    "\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/fastaAlignmentFilter.py -i /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS -o /work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_CDS.filter -t 48 -b 2 -g 0.2\n",
    "python /home2/s185491/p/xiaolongTools/utils/fastaAlignmentFilter.py -i /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_whole_max30Gap -o /work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_whole_max30Gap.filter -t 48 -b 4 -g 0.8\n",
    "python /home2/s185491/p/xiaolongTools/utils/fastaAlignmentFilter.py -i /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_z -o /work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_z.filter -t 48 -b 2 -g 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run ExaML\n",
    "MyBayes will be too slow. Run ExaML instead. Note: use 256GP CPUs for whole_max30Gap\n",
    "```\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=10\n",
    "#SBATCH --ntasks=320\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/utils/runExaML_withFastaForLargeFiles.py -i {filename} -r {referenceTree} -T 320\n",
    "'''\n",
    "\n",
    "files = '''/work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_CDS.filter\n",
    "/work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_whole_max30Gap.filter\n",
    "/work/biophysics/s185491/2018junonia/20181118MrBayes/20181010junoniaSeq_z.filter\n",
    "'''.split()\n",
    "referenceTrees = '''/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.sum.newick\n",
    "/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_whole_max30Gap_split.RAxMLbestTree.sum.newick\n",
    "/work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_z_split.RAxMLbestTree.sum.newick\n",
    "'''.split()\n",
    "\n",
    "import os\n",
    "for filename, referenceTree in zip(files,referenceTrees):\n",
    "    fout = open('/work/biophysics/s185491/2018junonia/20181118MrBayes/'+'qsub.'+os.path.basename(filename),'w')\n",
    "    fout.write(txt_qsub.format(filename=filename, referenceTree=referenceTree,n=os.path.basename(filename)))\n",
    "    fout.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum trees with astral, 20190222\n",
    "\n",
    "```\n",
    "txt = '''java -Xmx3000m -jar /home2/s185491/p/Astral/astral.5.6.2.jar -i {f} -o {f}.astral '''\n",
    "\n",
    "files = '''/work/biophysics/s185491/2018junonia/20181010Tree/20181010junoniaSeq_CDS_split.RAxMLbestTree.all\n",
    "/work/biophysics/s185491/2018junonia/20181010Tree/20181010junoniaSeq_whole_max30Gap_split.RAxMLbestTree.all\n",
    "/work/biophysics/s185491/2018junonia/20181010Tree/20181010junoniaSeq_z_split.RAxMLbestTree.all\n",
    "'''.split()\n",
    "\n",
    "for f in files:\n",
    "    print(txt.format(f=f))\n",
    "\n",
    "for f in files:\n",
    "    print('/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py  ' + f +'.astral')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gene tree, 4 groups with Neldi, totally 10 samples\n",
    "\n",
    "select the best genome coverage for these groups, each with 2 samples, totally 10 samples.\n",
    "10385 and 5322 are very different. 5322 is close to that 4 groups. Change to 5322 and 10385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### each group with 2 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique sample prefix 10\n",
      "['8215', '5256', '6049', '6704', '10385', '5332', '15117D12', '5950', '5397', '5438']\n"
     ]
    }
   ],
   "source": [
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "import pandas as pd\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "df_summary = df_summary[df_summary['Filter_4groups_Keep2_neildi'] == 1]\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "\n",
    "out_mapfile_locations = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location4groupswithNeldi.txt'\n",
    "fout = open(out_mapfile_locations,'w')\n",
    "for e in sample_prefix:\n",
    "    fout.write('/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map\\n')\n",
    "fout.close()\n",
    "print(sample_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract sequences\n",
    "```\n",
    "python /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181010mapfile_location4groupswithNeldi.txt -t 10 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181011_4groupswithNeldi_100kb -s 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if all files with 10 sequences # Yes, all files with 10 sequences.  \n",
    "generate scripts to run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181011_4groupswithNeldi_100kb/'\n",
    "outfolder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181011_4groupswithNeldi_100kb_tree'\n",
    "outfile_cmds = '/home/xcao/w/20180905Junonia_coenia/20180913scripts/20181011_4groupswithNeldi_100kb_tree'\n",
    "files = glob.glob(folder+'*')\n",
    "countline = lambda x: len(open(x).readlines())\n",
    "file_lines = [countline(f) for f in files]\n",
    "print(Counter(file_lines))\n",
    "\n",
    "commandline = 'cd /dev/shm && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {fullname} -n {basename} -o 10385,5332 && mv RAxML_bestTree.{basename} {outfolder}/{basename} && rm RAxML_*.{basename}'\n",
    "open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "```\n",
    "\n",
    "run commands\n",
    "```\n",
    "python /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20180913scripts/20181011_4groupswithNeldi_100kb_tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### each group with 1 sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "selected = {'8215',  '6049',  '10385',  '5950', '5397'}\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import glob\n",
    "folder_ori = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181011_4groupswithNeldi_100kb/'\n",
    "files_ori = glob.glob(folder_ori+'*')\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb/'\n",
    "outfolder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree/'\n",
    "outfile_cmds = '/home/xcao/w/20180905Junonia_coenia/20180913scripts/20181012_4groupswithNeldi5sample_100kb_tree'\n",
    "\n",
    "def extractSeq(filename,outfolder,selected):\n",
    "    l = SeqIO.parse(filename,'fasta')\n",
    "    fout = open(os.path.join(outfolder,os.path.basename(filename)),'w')\n",
    "    for s in l:\n",
    "        if s.id in selected:\n",
    "            fout.write('>'+s.id+'\\n'+str(s.seq)+'\\n')\n",
    "    fout.close()\n",
    "for f in files_ori:\n",
    "    if not f.endswith('reduced'):\n",
    "        extractSeq(f,outfolder=folder, selected=selected)\n",
    "\n",
    "#changed to add bootstrap\n",
    "files = glob.glob(folder+'*')\n",
    "commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename} -o 10385 -x 234 -N 100 -f a && mv RAxML_bipartitions.{basename} {outfolder}/{basename} && rm *{basename}*'\n",
    "open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "```\n",
    "\n",
    "run commands\n",
    "```\n",
    "python /home/xcao/p/xiaolongTools/multiThread.py 32 /home/xcao/w/20180905Junonia_coenia/20180913scripts/20181012_4groupswithNeldi5sample_100kb_tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum up the result\n",
    "```\n",
    "import glob\n",
    "from ete3 import Tree\n",
    "import os\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree'\n",
    "files = glob.glob(folder+'/*')\n",
    "print('total number of trees',len(files))\n",
    "\n",
    "def getSimplifiedTrees(filename,dc_leafname):\n",
    "    '''\n",
    "    tree file with info like\n",
    "        '((8215:0.025,(5397:0.022,(5950:0.017,6049:0.025):0.003):0.003):0.010,10385:0.010);'\n",
    "    change label name according to dc_leafname\n",
    "    sort leaves\n",
    "    ouput topology only\n",
    "        '((C,((G,N),X)),O);'\n",
    "    '''\n",
    "    t = Tree(filename)\n",
    "    for leaf in t.iter_leaves():\n",
    "        leaf.name = dc_leafname[leaf.name]\n",
    "    t.sort_descendants()\n",
    "    return t.write(format=9)\n",
    "\n",
    "dc_leafname = {'8215':'C','6049':'G','10385':'O','5950':'N','5397':'X'}\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree.topology.all'\n",
    "fout = open(outfile,'w')\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    t = getSimplifiedTrees(f,dc_leafname)\n",
    "    fout.write(basename+'\\t'+t+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot and save result in figure.\n",
    "```\n",
    "\n",
    "filename = r\"C:\\Users\\K\\OneDrive\\Lab\\UTSW\\2018JunoniaProject_Xiaolong\\20180927geneTree\\20181012_4groupswithNeldi5sample_100kb_tree\\20181012_4groupswithNeldi5sample_100kb_tree.topology.all\"\n",
    "\n",
    "chromosomes_order = list(range(1,22)) + [0] # chromosome_id\n",
    "dc_chromosomes = {k:[] for k in chromosomes_order} # key: chromsome_id, value, list of fragment_id\n",
    "dc_trees = {} #key: (chromosome_id, fragment_id), \n",
    "#((15, 60), '((((C,G),N),X),O);')\n",
    "lis = open(filename).readlines()\n",
    "for line in lis:\n",
    "    _chromosome_frag, _tree = line.split()\n",
    "    _chr,_fragid = _chromosome_frag[3:].split('_')\n",
    "    _chr = int(_chr)\n",
    "    _fragid = int(_fragid)\n",
    "    dc_trees[(_chr,_fragid)] = _tree\n",
    "    dc_chromosomes[_chr].append(_fragid)\n",
    "\n",
    "for _chr in dc_chromosomes:#store the number of fragments. All fragments exist in the tree\n",
    "    dc_chromosomes[_chr] = len(dc_chromosomes[_chr])\n",
    "\n",
    "start = 0\n",
    "dc_chromosomes_plot = {}\n",
    "for _chr in chromosomes_order:\n",
    "    end = start + dc_chromosomes[_chr]\n",
    "    dc_chromosomes_plot[_chr] = [start,end]\n",
    "    start = end\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "uniTreeCount =Counter(dc_trees.values())\n",
    "uniTreeCount = uniTreeCount.most_common() # count of trees. Totally 15 possibilities. All exist\n",
    "dc_uniTreeOrder = {}\n",
    "for n,_tree in enumerate(uniTreeCount):\n",
    "    dc_uniTreeOrder[_tree[0]] = n\n",
    "\n",
    "\n",
    "# plot all 15 trees in chromsome\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,200)\n",
    "\n",
    "#import matplotlib\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "l = len(chromosomes_order)\n",
    "if l % 2 != 0:\n",
    "    l += 1\n",
    "colors = get_cmap(l)\n",
    "colors_int = []\n",
    "for n in range(l//2):\n",
    "    colors_int += [n,n+len(chromosomes_order)//2]\n",
    "colors_use = [colors(i) for i in colors_int]\n",
    "\n",
    "\n",
    "#plt.vlines(1,ymin=0,ymax=1)\n",
    "#matplotlib.pyplot.ion()\n",
    "#matplotlib.pyplot.ioff()\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "ymax = 0\n",
    "for n,chr_id in enumerate(chromosomes_order):\n",
    "    plt.vlines(x=0,ymin= -dc_chromosomes_plot[chr_id][1],ymax= -dc_chromosomes_plot[chr_id][0],colors=colors_use[n])\n",
    "    plt.text(x=0,y= -sum(dc_chromosomes_plot[chr_id])/2,s='chr%d'%chr_id, horizontalalignment='right')\n",
    "\n",
    "nnn = 0\n",
    "for tree_key,tree in dc_trees.items():\n",
    "    tree_id = dc_uniTreeOrder[tree]\n",
    "    y = -(dc_chromosomes_plot[tree_key[0]][0] + tree_key[1])\n",
    "    plt.hlines(y=y,xmin=tree_id+0.1,xmax=tree_id+1,colors=colors_use[tree_id])\n",
    "    plt.text(x=tree_id+0.55, y=1, s=tree+'count:%d'%uniTreeCount[tree_id][1], rotation=90, horizontalalignment='center', verticalalignment='bottom', name='Arial')\n",
    "    nnn += 1\n",
    "#    if nnn == 1000:\n",
    "#        break\n",
    "description = '''\n",
    "C: Junonia coenia coenia\n",
    "G: Junonia coenia grisea\n",
    "N: Junonia nigrosuffusa\n",
    "X: Junonia nigrosuffusaTX\n",
    "O: Outgroup, Junonia neildi\n",
    "'''\n",
    "plt.text(x = 0, y = -dc_chromosomes_plot[0][1], s = description,horizontalalignment='left', verticalalignment='top')\n",
    "#plt.show()\n",
    "plt.ylim(-5900,100)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\Users\\K\\Downloads\\figure.pdf')\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ion() # turn one interactive mode for plt\n",
    "#plt.plot([1.6, 2.7])\n",
    "#plt.ioff() # turn off interactive mode for plt\n",
    "\n",
    "## summerize dcTrees\n",
    "from collections import defaultdict,Counter\n",
    "import pandas as pd\n",
    "dc_chromosomes_trees = defaultdict(list)\n",
    "for tree_key, tree in dc_trees.items():\n",
    "    dc_chromosomes_trees[tree_key[0]].append(tree)\n",
    "ls_chromosome_trees=[]\n",
    "for chr_id, trees in dc_chromosomes_trees.items():\n",
    "    chr_trees = Counter(trees)\n",
    "    chr_trees['chr_id'] = chr_id\n",
    "    ls_chromosome_trees.append(chr_trees)\n",
    "df_chromosome_trees = pd.DataFrame(ls_chromosome_trees)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum up the result, with bootstrap\n",
    "```\n",
    "import glob\n",
    "from ete3 import Tree\n",
    "import os\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree'\n",
    "files = glob.glob(folder+'/*')\n",
    "print('total number of trees',len(files))\n",
    "\n",
    "def getSimplifiedTreesOnlyChangeName(filename,dc_leafname):\n",
    "    '''\n",
    "    tree file with info like\n",
    "        '((8215:0.025,(5397:0.022,(5950:0.017,6049:0.025):0.003):0.003):0.010,10385:0.010);'\n",
    "    change label name according to dc_leafname\n",
    "    sort leaves\n",
    "    \n",
    "        '((C,((G,N),X)),O);'\n",
    "    '''\n",
    "    t = Tree(filename)\n",
    "    for leaf in t.iter_leaves():\n",
    "        leaf.name = dc_leafname[leaf.name]\n",
    "    t.sort_descendants()\n",
    "    return t.write(format=0)\n",
    "\n",
    "dc_leafname = {'8215':'C','6049':'G','10385':'O','5950':'N','5397':'X'}\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181013_4groupswithNeldi5sample_100kb_tree.topology.all.bootstrap'\n",
    "fout = open(outfile,'w')\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    t = getSimplifiedTreesOnlyChangeName(f,dc_leafname)\n",
    "    fout.write(basename+'\\t'+t+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the trees along the chromosome\n",
    "```\n",
    "\n",
    "filename = r\"C:\\Users\\K\\OneDrive\\Lab\\UTSW\\2018JunoniaProject_Xiaolong\\20180927geneTree\\20181012_4groupswithNeldi5sample_100kb_tree\\20181013_4groupswithNeldi5sample_100kb_tree.topology.all.bootstrap\"\n",
    "\n",
    "chromosomes_order = list(range(1,22)) + [0] # chromosome_id\n",
    "dc_chromosomes = {k:[] for k in chromosomes_order} # key: chromsome_id, value, list of fragment_id\n",
    "dc_trees = {} #key: (chromosome_id, fragment_id), \n",
    "#((15, 60), '((((C,G),N),X),O);')\n",
    "lis = open(filename).readlines()\n",
    "for line in lis:\n",
    "    _chromosome_frag, _tree = line.split()\n",
    "    _chr,_fragid = _chromosome_frag[3:].split('_')\n",
    "    _chr = int(_chr)\n",
    "    _fragid = int(_fragid)\n",
    "    dc_trees[(_chr,_fragid)] = _tree\n",
    "    dc_chromosomes[_chr].append(_fragid)\n",
    "\n",
    "from ete3 import Tree\n",
    "def getSupportValues(t):\n",
    "    '''\n",
    "    t = '((C:0.0251469,((G:0.0254845,N:0.0175748)91:0.0032768,X:0.0221408)89:0.00343486)1:0.0107784,O:0.0107784);'\n",
    "    return '((C,((G,N),X)),O);', (91,89)\n",
    "    '''\n",
    "    tree = Tree(t)\n",
    "    boots = []\n",
    "    for leaf in tree.iter_descendants():\n",
    "        if leaf.support > 1:\n",
    "            boots.append(leaf.support)\n",
    "    tree = tree.write(format=9)\n",
    "    return tree, boots\n",
    "\n",
    "dc_trees_boots = {}\n",
    "for key in dc_trees:\n",
    "    dc_trees[key], dc_trees_boots[key] = getSupportValues(dc_trees[key])\n",
    "\n",
    "for _chr in dc_chromosomes:#store the number of fragments. All fragments exist in the tree\n",
    "    dc_chromosomes[_chr] = len(dc_chromosomes[_chr])\n",
    "\n",
    "start = 0\n",
    "dc_chromosomes_plot = {}\n",
    "for _chr in chromosomes_order:\n",
    "    end = start + dc_chromosomes[_chr]\n",
    "    dc_chromosomes_plot[_chr] = [start,end]\n",
    "    start = end\n",
    "\n",
    "\n",
    "# filter dc_trees based on bootstrap value\n",
    "# only these with 2 boots larger than 80%\n",
    "min_boots = 80\n",
    "dc_trees_filter = {}\n",
    "for key in dc_trees:\n",
    "    if all(e > min_boots for e in dc_trees_boots[key]):\n",
    "        dc_trees_filter[key] = dc_trees[key]\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "uniTreeCount =Counter(dc_trees.values())\n",
    "uniTreeCount = uniTreeCount.most_common() # count of trees. Totally 15 possibilities. All exist\n",
    "dc_uniTreeOrder = {}\n",
    "for n,_tree in enumerate(uniTreeCount):\n",
    "    dc_uniTreeOrder[_tree[0]] = n\n",
    "\n",
    "uniTreeCount_filter =Counter(dc_trees_filter.values())\n",
    "uniTreeCount_filter = uniTreeCount_filter.most_common() # count of trees. Totally 15 possibilities. All exist\n",
    "dc_uniTreeOrder_filter = {}\n",
    "for n,_tree in enumerate(uniTreeCount_filter):\n",
    "    dc_uniTreeOrder_filter[_tree[0]] = n\n",
    "\n",
    "# plot all 15 trees in chromsome\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,200)\n",
    "\n",
    "#import matplotlib\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "l = len(chromosomes_order)\n",
    "if l % 2 != 0:\n",
    "    l += 1\n",
    "colors = get_cmap(l)\n",
    "colors_int = []\n",
    "for n in range(l//2):\n",
    "    colors_int += [n,n+len(chromosomes_order)//2]\n",
    "colors_use = [colors(i) for i in colors_int]\n",
    "\n",
    "\n",
    "#plt.vlines(1,ymin=0,ymax=1)\n",
    "#matplotlib.pyplot.ion()\n",
    "#matplotlib.pyplot.ioff()\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "ymax = 0\n",
    "for n,chr_id in enumerate(chromosomes_order):\n",
    "    plt.vlines(x=0,ymin= -dc_chromosomes_plot[chr_id][1],ymax= -dc_chromosomes_plot[chr_id][0],colors=colors_use[n])\n",
    "    plt.text(x=0,y= -sum(dc_chromosomes_plot[chr_id])/2,s='chr%d'%chr_id, horizontalalignment='right')\n",
    "\n",
    "nnn = 0\n",
    "for tree_key,tree in dc_trees.items():\n",
    "    tree_id = dc_uniTreeOrder_filter[tree]\n",
    "    y = -(dc_chromosomes_plot[tree_key[0]][0] + tree_key[1])\n",
    "    if tree_key in dc_trees_filter:\n",
    "        linestyle = 'solid'\n",
    "    else:\n",
    "        linestyle = 'dotted'\n",
    "    plt.hlines(y=y,xmin=tree_id+0.1,xmax=tree_id+1,colors=colors_use[tree_id], linestyle=linestyle)\n",
    "    plt.text(x=tree_id+0.55, y=1, s=tree+'count:%d'%uniTreeCount[tree_id][1], rotation=90, horizontalalignment='center', verticalalignment='bottom', name='Arial')\n",
    "    nnn += 1\n",
    "#    if nnn == 1000:\n",
    "#        break\n",
    "description = '''\n",
    "C: Junonia coenia coenia\n",
    "G: Junonia coenia grisea\n",
    "N: Junonia nigrosuffusa\n",
    "X: Junonia nigrosuffusaTX\n",
    "O: Outgroup, Junonia neildi\n",
    "'''\n",
    "plt.text(x = 0, y = -dc_chromosomes_plot[0][1], s = description,horizontalalignment='left', verticalalignment='top')\n",
    "#plt.show()\n",
    "plt.ylim(-5900,100)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\Users\\K\\Downloads\\figure.pdf')\n",
    "plt.close()\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ion() # turn one interactive mode for plt\n",
    "#plt.plot([1.6, 2.7])\n",
    "#plt.ioff() # turn off interactive mode for plt\n",
    "\n",
    "## summerize dcTrees\n",
    "from collections import defaultdict,Counter\n",
    "import pandas as pd\n",
    "dc_chromosomes_trees = defaultdict(list)\n",
    "for tree_key, tree in dc_trees.items():\n",
    "    dc_chromosomes_trees[tree_key[0]].append(tree)\n",
    "ls_chromosome_trees=[]\n",
    "for chr_id, trees in dc_chromosomes_trees.items():\n",
    "    chr_trees = Counter(trees)\n",
    "    chr_trees['chr_id'] = chr_id\n",
    "    ls_chromosome_trees.append(chr_trees)\n",
    "df_chromosome_trees = pd.DataFrame(ls_chromosome_trees)\n",
    "\n",
    "dc_chromosomes_trees = defaultdict(list)\n",
    "for tree_key, tree in dc_trees_filter.items():\n",
    "    dc_chromosomes_trees[tree_key[0]].append(tree)\n",
    "ls_chromosome_trees=[]\n",
    "for chr_id, trees in dc_chromosomes_trees.items():\n",
    "    chr_trees = Counter(trees)\n",
    "    chr_trees['chr_id'] = chr_id\n",
    "    ls_chromosome_trees.append(chr_trees)\n",
    "df_chromosome_filter_trees = pd.DataFrame(ls_chromosome_trees)\n",
    "df_chromosome_filter_trees.index = df_chromosome_filter_trees['chr_id']\n",
    "df_chromosome_filter_trees =  df_chromosome_filter_trees.drop(labels=['chr_id'],axis=1)\n",
    "df_chromosome_filter_trees = df_chromosome_filter_trees.loc[chromosomes_order,list(dc_uniTreeOrder_filter)]\n",
    "df_chromosome_filter_trees.to_excel(r\"C:\\Users\\K\\Downloads\\temp.xlsx\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate T1, T2 based on Tree topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for trees in folder `/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree`, the samples are {'8215',  '6049',  '10385',  '5950', '5397'}\n",
    "* 8215: Junonia coenia coenia, JCC, C\n",
    "* 6049: Junonia coenia grisea, JCG, G,\n",
    "* 10385: Junonia neildi, NDI, D,\n",
    "* 5950: Junonia nigrosuffusa, JNG, N\n",
    "* 5397: Junonia nigrosuffusaTX, JNX, X\n",
    "\n",
    "Generate a table. set `D` as outgroup. Get bootstrap values, and get topology of with `CGND`,`CGXD`,`GNXD`,`CNXD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect data\n",
    "```\n",
    "import glob\n",
    "from ete3 import Tree\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils')\n",
    "import calculateT1T2FromMultipleFastaAlignment\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb_tree/'\n",
    "folder_fasta = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb/'\n",
    "\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "file = files[0]\n",
    "\n",
    "def getInfoFromTrees(file):\n",
    "    t = Tree(open(file).read())\n",
    "    # set '10385' as outgroup. it is already the outgroup. just to make sure\n",
    "    t.set_outgroup('10385')\n",
    "    samples = ['8215', '6049', '5950', '5397']\n",
    "    dc_samples = dict(zip(['8215',  '6049',  '10385',  '5950', '5397'],'CGDNX'))\n",
    "    bootstraps = []\n",
    "    for node in t.traverse(strategy='levelorder'):\n",
    "        if node.support > 1:\n",
    "            bootstraps.append(node.support)\n",
    "    #print(bootstraps)\n",
    "    bootstraps = ','.join(str(e) for e in bootstraps)\n",
    "    samples_orders = []\n",
    "    for sample in samples:\n",
    "        t_working = t.copy()\n",
    "        for leaf in t_working.iter_leaves():\n",
    "            if leaf.name == sample:\n",
    "                leaf.delete(preserve_branch_length=True)\n",
    "        #print(t_working)\n",
    "        t_working.sort_descendants()\n",
    "        sample_orders = []\n",
    "        for node in t_working.traverse(strategy='levelorder'):\n",
    "            if node.is_leaf():\n",
    "                sample_orders.append(node.name)\n",
    "        sample_orders.reverse()\n",
    "        T1T2=calculateT1T2FromMultipleFastaAlignment.calculateT1T2FromFastaAlignment(os.path.join(folder_fasta,os.path.basename(file)), orders = sample_orders,minlen= 5000)\n",
    "        if T1T2 is not None:\n",
    "            T1, T2 = T1T2\n",
    "        else:\n",
    "            T1,T2 = 0,0\n",
    "        sample_letters = ''.join(dc_samples[e] for e in sample_orders)\n",
    "        sample_orders = ','.join(sample_orders)\n",
    "        sample_t1t2 = str(T1)+','+str(T2)\n",
    "        samples_orders.append(sample_orders)\n",
    "        samples_orders.append(sample_letters)\n",
    "        samples_orders.append(sample_t1t2)\n",
    "    to_write = '\\t'.join([os.path.basename(file)]+[bootstraps] + samples_orders)\n",
    "    return to_write\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(getInfoFromTrees,files)\n",
    "pool.close()\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb.summary','w').write('\\n'.join(results))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each line contains 14 elements\n",
    "```\n",
    "chr15_60: fragment name\n",
    "100.0,100.0: bootstrap values\n",
    "6049,5950,5397,10385: tree order\n",
    "GNXD: tree order coded in single letter\n",
    "0.037763884605680004,0.026353416131654938: T1T2\n",
    "8215,5950,5397,10385: tree order\n",
    "CNXD: tree order coded in single letter\n",
    "0.03910747219583077,0.028181411725255685: T1T2\n",
    "6049,8215,5397,10385: tree order\n",
    "GCXD: tree order coded in single letter\n",
    "0.038698151730066625,0.02421824629271438: T1T2\n",
    "6049,8215,5950,10385: tree order\n",
    "GCND: tree order coded in single letter\n",
    "0.033613020622725434,0.023402749696724626: T1T2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot with R\n",
    "```\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(tidyr)\n",
    "\n",
    "filename <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181012_4groupswithNeldi5sample_100kb.summary'\n",
    "\n",
    "x.df <- read.csv(filename,header = FALSE,sep = '\\t',stringsAsFactors = FALSE)\n",
    "\n",
    "\n",
    "\n",
    "getNthElementFromStr <- function(x, split='_', N=1) strsplit(x,split = split)[[1]][N]\n",
    "\n",
    "x.dw <- data.frame(chr = sapply(x.df$V1, getNthElementFromStr, split='_',N=1), chr_loc = as.numeric(sapply(x.df$V1, getNthElementFromStr, split='_',N=2)))\n",
    "x.dw$bootstrap1 = as.numeric(sapply(x.df$V2, getNthElementFromStr,split=',',N=1))\n",
    "x.dw$bootstrap2 = as.numeric(sapply(x.df$V2, getNthElementFromStr,split=',',N=2))\n",
    "x.dw$GNXD <- x.df$V4\n",
    "x.dw$GNXD.T1 <- as.numeric(sapply(x.df$V5, getNthElementFromStr,split=',',N=1))\n",
    "x.dw$GNXD.T2 <- as.numeric(sapply(x.df$V5, getNthElementFromStr,split=',',N=2))\n",
    "x.dw$CNXD <- x.df$V7\n",
    "x.dw$CNXD.T1 <- as.numeric(sapply(x.df$V8, getNthElementFromStr,split=',',N=1))\n",
    "x.dw$CNXD.T2 <- as.numeric(sapply(x.df$V8, getNthElementFromStr,split=',',N=2))\n",
    "x.dw$CGXD <- x.df$V10\n",
    "x.dw$CGXD.T1 <- as.numeric(sapply(x.df$V11, getNthElementFromStr,split=',',N=1))\n",
    "x.dw$CGXD.T2 <- as.numeric(sapply(x.df$V11, getNthElementFromStr,split=',',N=2))\n",
    "x.dw$CGND <- x.df$V13\n",
    "x.dw$CGND.T1 <- as.numeric(sapply(x.df$V14, getNthElementFromStr,split=',',N=1))\n",
    "x.dw$CGND.T2 <- as.numeric(sapply(x.df$V14, getNthElementFromStr,split=',',N=2))\n",
    "x.dw$isZ <- sapply(x.dw$chr, function(x) { if (x == 'chr21') 'Z' else 'auto'})\n",
    "\n",
    "x.dw.boots80 <- x.dw[x.dw$bootstrap1 >80 & x.dw$bootstrap2>80,] #filter both bootstrap greater than 80\n",
    "\n",
    "#plot GNXD\n",
    "x.dw.GNXD <- x.dw.boots80[x.dw.boots80$GNXD.T1 !=0 & x.dw.boots80$GNXD.T2!=0,] # remove bad alignments\n",
    "x.dw.GNXD <- x.dw.GNXD[,c('GNXD',\"GNXD.T1\",'GNXD.T2','isZ')]\n",
    "x.dw.GNXD <- gather(x.dw.GNXD,key='T1T2',value = 'Height',-GNXD, -isZ)\n",
    "print(unique(x.dw.GNXD$GNXD))\n",
    "\n",
    "ggplot(x.dw.GNXD,aes(x=isZ,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "ggplot(x.dw.GNXD[x.dw.GNXD$isZ != 'Z',],aes(x=GNXD,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "\n",
    "#plot CNXD\n",
    "x.dw.CNXD <- x.dw.boots80[x.dw.boots80$CNXD.T1 !=0 & x.dw.boots80$CNXD.T2!=0,] # remove bad alignments\n",
    "x.dw.CNXD <- x.dw.CNXD[,c('CNXD',\"CNXD.T1\",'CNXD.T2','isZ')]\n",
    "x.dw.CNXD <- gather(x.dw.CNXD,key='T1T2',value = 'Height',-CNXD, -isZ)\n",
    "print(unique(x.dw.CNXD$CNXD))\n",
    "\n",
    "ggplot(x.dw.CNXD,aes(x=isZ,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "ggplot(x.dw.CNXD[x.dw.CNXD$isZ != 'Z',],aes(x=CNXD,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "\n",
    "#plot CGXD\n",
    "x.dw.CGXD <- x.dw.boots80[x.dw.boots80$CGXD.T1 !=0 & x.dw.boots80$CGXD.T2!=0,] # remove bad alignments\n",
    "x.dw.CGXD <- x.dw.CGXD[,c('CGXD',\"CGXD.T1\",'CGXD.T2','isZ')]\n",
    "x.dw.CGXD <- gather(x.dw.CGXD,key='T1T2',value = 'Height',-CGXD, -isZ)\n",
    "print(unique(x.dw.CGXD$CGXD))\n",
    "\n",
    "ggplot(x.dw.CGXD,aes(x=isZ,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "ggplot(x.dw.CGXD[x.dw.CGXD$isZ != 'Z',],aes(x=CGXD,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "\n",
    "\n",
    "#plot CGND\n",
    "x.dw.CGND <- x.dw.boots80[x.dw.boots80$CGND.T1 !=0 & x.dw.boots80$CGND.T2!=0,] # remove bad alignments\n",
    "x.dw.CGND <- x.dw.CGND[,c('CGND',\"CGND.T1\",'CGND.T2','isZ')]\n",
    "x.dw.CGND <- gather(x.dw.CGND,key='T1T2',value = 'Height',-CGND, -isZ)\n",
    "print(unique(x.dw.CGND$CGND))\n",
    "\n",
    "ggplot(x.dw.CGND,aes(x=isZ,y=Height)) + geom_boxplot() + facet_wrap(~T1T2) + stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "ggplot(x.dw.CGND[x.dw.CGND$isZ != 'Z',],aes(x=CGND,y=Height)) + geom_boxplot() + facet_wrap(~T1T2) + stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5)))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gene tree from merged samples 6 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make gene trees. with 6 groups. samples were put into groups based on trees made from CDS and z-chromosome  \n",
    "J_c_coenia\n",
    "J_neildi\n",
    "J_z_zonalis\n",
    "J_nigrosuffusa\n",
    "J_c_grisea\n",
    "J_nigrosuffusaTX\n",
    "\n",
    "For each group, merge the map files. First keep the one with the highest coverage, than the second best coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get merged sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils')\n",
    "from mapFile import mapfileIO\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20181003Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all',dtype=str)\n",
    "df_summary['Number'] = df_summary['Number'].apply(lambda x:x.split('-')[-1])\n",
    "df_use = df_summary[df_summary['Filter_4groups_withNeildiZonalis'] != 'nan'].loc[:,['Number','Filter_4groups_withNeildiZonalis','genome_coverage']]\n",
    "df_use['genome_coverage'] = df_use['genome_coverage'].astype(float)\n",
    "df_use['mapfile'] = df_use['Number'].apply(lambda x:'/archive/butterfly/maps/debiased/'+x+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map')\n",
    "df_use = df_use.sort_values(by = ['Filter_4groups_withNeildiZonalis','genome_coverage'], ascending=[True,False])\n",
    "\n",
    "ls_int8 = mapfileIO.read2Int8s(list(df_use['mapfile']),threads=32)\n",
    "df_use['int8'] = ls_int8\n",
    "\n",
    "species = list(df_use['Filter_4groups_withNeildiZonalis'].unique())\n",
    "\n",
    "def getFirstATCG(elements):\n",
    "    for i in elements:\n",
    "        if chr(i) in 'ATCG':\n",
    "            return chr(i)\n",
    "    return '-'\n",
    "def mergeSeqs(seqs):\n",
    "    '''\n",
    "    seqs is a list of sequences, sort by coverange\n",
    "    '''\n",
    "    return ''.join(getFirstATCG(e) for e in zip(*seqs))\n",
    "\n",
    "def mergeSeqsThread(seqs,threads=32):\n",
    "    '''\n",
    "    seqs is a list of sequences, sort by coverange\n",
    "    '''\n",
    "    seqlen = len(seqs[0])\n",
    "    import numpy as np\n",
    "    seqs_s = []\n",
    "    step = int(np.ceil(seqlen/threads))\n",
    "    for i in range(0,seqlen,step):\n",
    "        seqs_s.append([e[i:i+step] for e in seqs])\n",
    "    from multiprocessing import Pool\n",
    "    pool = Pool(threads)\n",
    "    results = pool.map(mergeSeqs, seqs_s)\n",
    "    pool.close()\n",
    "    return ''.join(results)\n",
    "\n",
    "import pickle\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/ls_int8','wb') as f:\n",
    "    pickle.dump(ls_int8,f)\n",
    "\n",
    "dc_seqs = dict.fromkeys(species)\n",
    "for s in dc_seqs:\n",
    "    dc_seqs[s] = mergeSeqsThread(list(df_use[df_use['Filter_4groups_withNeildiZonalis'] == s]['int8']))\n",
    "\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/6species.fa','w')\n",
    "for k,v in dc_seqs.items():\n",
    "    fout.write('>'+str(k)+'\\n'+str(v)+'\\n')\n",
    "fout.close()\n",
    "\n",
    "#write to map files\n",
    "for k in dc_seqs:\n",
    "    fout = open(os.path.join('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample', k+'.map'),'w')\n",
    "    fout.write('\\n'.join(list(dc_seqs[k])))\n",
    "    fout.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "completeness changed to  \n",
    "J_c_coenia 0.9426292953462839  \n",
    "J_c_grisea 0.9088682414684224  \n",
    "J_neildi 0.8628496013422835  \n",
    "J_nigrosuffusa 0.9337466294900651  \n",
    "J_nigrosuffusaTX 0.8962087413250812  \n",
    "J_z_zonalis 0.8886533269982024  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sequences\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen10000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen20000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen20000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen50000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen50000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen100000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run RAxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate scripts\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folders = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen20000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen50000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen100000/\n",
    "'''.split()\n",
    "outfolders = [e[:-1]+'_tree' for e in folders]\n",
    "outcommands = [e[:-1] + '.cmds' for e in folders]\n",
    "\n",
    "for folder, outfolder, outfile_cmds in zip(folders, outfolders, outcommands):\n",
    "#changed to add bootstrap\n",
    "    files = glob.glob(folder+'*')\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename} -o J_neildi,J_z_zonalis -x 234 -N 100 -f a && mv RAxML_bipartitions.{basename} {outfolder}/{basename} && rm *{basename}*'\n",
    "    open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "```\n",
    "\n",
    "Run\n",
    "\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen20000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen50000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen100000.cmds\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect trees\n",
    "\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen20000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen50000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen100000_tree/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### outgroup not together ratio vs bootstrap values\n",
    "data process suggested by Nick\n",
    "\n",
    "For each tree, get the minimum bootstrap value, and plot the proportion of trees that two outgroups is not together. \n",
    "\n",
    "```\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum'\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "\n",
    "#split minBoots to 20 fractions, count the number of False in outgorupTogether column\n",
    "bins = [[i*5,(i+1)*5] for i in range(20)]\n",
    "bincounts = [0 for i in bins]\n",
    "binFalseOut = [0 for i in bins]\n",
    "for n, line in df_tree.iterrows():\n",
    "    minBoots = line['minBoots']\n",
    "    bin_loc = int(np.ceil(minBoots /5)) -1\n",
    "    bincounts[bin_loc] += 1\n",
    "    if not line['outgroupTogether']:\n",
    "        binFalseOut[bin_loc] += 1\n",
    "df_FalseOut = pd.DataFrame()\n",
    "df_FalseOut['bins'] = bins\n",
    "df_FalseOut['counts'] = bincounts\n",
    "df_FalseOut['OutgroupNotTogether'] = binFalseOut\n",
    "df_FalseOut['ratio_outNotTogether'] = df_FalseOut['OutgroupNotTogether'] / df_FalseOut['counts']\n",
    "\n",
    "import pickle\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.memory.df_tree','wb') as f:\n",
    "    pickle.dump(df_tree, f)\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.memory.df_FalseOut','wb') as f:\n",
    "    pickle.dump(df_FalseOut, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f88817dd668>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.memory.df_FalseOut','rb') as f:\n",
    "    df_FalseOut = pickle.load(f)\n",
    "%matplotlib inline\n",
    "df_FalseOut.plot.bar(x='bins',y='ratio_outNotTogether')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minBoots (there are 3 bootstrap values. the minimum bootstrap value) 75 is a good points that the ratio two outgroup not together is not together. Below that, it is is possibly due to no phylogenetic resolution (not enough informative positions) and the tree is random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot distribution of relative distance between two outgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def getRelativeDistance(tree,node1 = 'J_neildi',node2='J_z_zonalis'):\n",
    "    '''\n",
    "    get the relative distance of node1 and node2 in a tree\n",
    "    '''\n",
    "    total_distance = 0\n",
    "    for i in tree.traverse():\n",
    "        total_distance += i.dist\n",
    "    #print(total_distance)\n",
    "    outgroup_distance = tree.get_distance('J_neildi','J_z_zonalis')\n",
    "    outgroup_relative_distance = outgroup_distance / total_distance\n",
    "    return outgroup_relative_distance\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(32)\n",
    "df_tree['outgroup_relative_distance'] = pool.map(getRelativeDistance, df_tree['tree'])\n",
    "pool.close()\n",
    "\n",
    "df_tree.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.csv',columns=['chr','chrN','minBoots','outgroupTogether','outgroup_relative_distance'],sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code below run with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "library(ggplot2)\n",
    "\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.csv'\n",
    "xdf = read.csv(filename,sep = '\\t',stringsAsFactors = FALSE)\n",
    "xdf$outgroupTogether <- xdf$outgroupTogether =='True'\n",
    "xdf$Boots75 <-  xdf$minBoots >= 75\n",
    "\n",
    "xdf$minBootsRange <- cut(xdf$minBoots,seq(0,100,20))\n",
    "#ggplot(xdf,aes(x=minBootsRange)) + geom_boxplot(aes(y=outgroup_relative_distance))\n",
    "ggplot(xdf) + geom_histogram(aes(x=outgroup_relative_distance))\n",
    "ggplot(xdf) + geom_histogram(aes(x=outgroup_relative_distance)) + facet_grid(outgroupTogether~.)\n",
    "ggplot(xdf) + geom_histogram(aes(x=outgroup_relative_distance)) + facet_grid(Boots75~.)\n",
    "ggplot(xdf) + geom_histogram(aes(x=outgroup_relative_distance)) + facet_grid(outgroupTogether~minBootsRange)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count and plot good tree topologies\n",
    "minBoots>75, outgorupTogether is True\n",
    "\n",
    "```\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)]\n",
    "df_tree_b75_O2 = df_tree_b75_O2.reset_index().copy()\n",
    "def getTopology(tree):\n",
    "    tree = tree.copy()\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_z_zonalis','J_neildi'))\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "from collections import Counter\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.b75o2.names','w')\n",
    "for chromosome, chrN in zip(df_tree_b75_O2['chr'],df_tree_b75_O2['chrN']):\n",
    "    fout.write('chr'+str(chromosome)+'_'+str(chrN)+'\\n')\n",
    "fout.close() #store filenames of trees with minBoots >=75 and outgroupTogether\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.sum.b75o2.xlsx') #store the tree toplogy count in each chromosome\n",
    "\n",
    "from ete3 import TreeStyle,TextFace,NodeStyle,Tree\n",
    "\n",
    "for n,t in enumerate(txt):\n",
    "    tt,tn = t.split(':')\n",
    "    tt = Tree(tt)\n",
    "    ts = TreeStyle()\n",
    "    ts.title.add_face(TextFace('count:'+tn, fsize=20),column=0)\n",
    "    ts.show_branch_length = False\n",
    "    ts.show_scale = False\n",
    "    nstyle = NodeStyle()\n",
    "    nstyle[\"size\"] = 0\n",
    "    nstyle['hz_line_width'] = 1\n",
    "    nstyle['vt_line_width'] = 1\n",
    "    for node in tt.traverse():\n",
    "       node.set_style(nstyle)\n",
    "    tt.render('20181029Trees_'+str(n)+'.pdf',tree_style=ts)\n",
    "    #break\n",
    "\n",
    "files = ['20181029Trees_'+str(n)+'.pdf' for n in range(15)]\n",
    "from PyPDF2 import PdfFileMerger\n",
    "merger = PdfFileMerger()\n",
    "for pdf in files:\n",
    "    merger.append(open(pdf, 'rb'))\n",
    "with open('result.pdf', 'wb') as fout:\n",
    "    merger.write('20181029Trees.pdf')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate T1T2 for outgroup together and bootsMin>=75 trees. \n",
    "13050 out of 58534 trees are good.\n",
    "```\n",
    "import glob\n",
    "from ete3 import Tree\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils')\n",
    "import calculateT1T2FromMultipleFastaAlignment\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree/'\n",
    "folder_fasta = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000/'\n",
    "\n",
    "files = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.b75o2.names').read().split()\n",
    "\n",
    "file = files[0]\n",
    "\n",
    "\n",
    "treeTXT='((J_nigrosuffusaTX:0.02653114204746282248,((J_c_grisea:0.02850490624017898780,J_c_coenia:0.02532914758048187068)95:0.00397364234452561322,J_nigrosuffusa:0.02831559295854962910)99:0.00439717379237309063)100:0.00397298745329234797,(J_neildi:0.02513506551944225814,J_z_zonalis:0.02562707214028403513)100:0.00397298745329234797);'\n",
    "file_alignment = r\"C:\\Users\\K\\Downloads\\chr0_4\"\n",
    "tree = Tree(treeTXT)\n",
    "\n",
    "def processTree(file):\n",
    "    file_tree = os.path.join(folder,file)\n",
    "    file_alignment = os.path.join(folder_fasta,file)\n",
    "    tree = Tree(open(file_tree).read())\n",
    "    chromosome, chromosomeN = file.split('_')\n",
    "    ls_outinfo = []\n",
    "    def addNodeNames(tree):\n",
    "        '''\n",
    "        the internal nodes have no names. add unique names like node1, node2 to them\n",
    "        '''\n",
    "        n = 1\n",
    "        for node in tree.traverse():\n",
    "            if not node.is_leaf():\n",
    "                node.name = 'node' + str(n)\n",
    "                n += 1\n",
    "        return None\n",
    "    addNodeNames(tree)\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_neildi','J_z_zonalis'))\n",
    "    outgroups = ['J_neildi','J_z_zonalis']\n",
    "    testgroups = ['J_nigrosuffusaTX', 'J_c_grisea', 'J_c_coenia', 'J_nigrosuffusa']\n",
    "    for node in outgroups:\n",
    "        for testnode in testgroups:\n",
    "            dc = {}\n",
    "            dc['chr'] = chromosome\n",
    "            dc['chrN'] = chromosomeN\n",
    "            t2 = tree.copy()\n",
    "            for leaf in t2.iter_leaves():\n",
    "                if leaf.name == node or leaf.name == testnode:\n",
    "                    leaf.delete(preserve_branch_length=True)\n",
    "            #print(t2)\n",
    "            t2.sort_descendants()\n",
    "            sample_orders = []\n",
    "            for n2 in t2.traverse(strategy='levelorder'):\n",
    "                if n2.is_leaf():\n",
    "                    sample_orders.append(n2.name)\n",
    "            sample_orders.reverse()\n",
    "            #print(t2.write(format=9))\n",
    "            #print(calculateT1T2FromFastaAlignment(file_alignment,sample_orders))\n",
    "            T1T2=calculateT1T2FromMultipleFastaAlignment.calculateT1T2FromFastaAlignment(file_alignment, orders = sample_orders,minlen= 1000)\n",
    "            if T1T2 is not None:\n",
    "                T1, T2 = T1T2\n",
    "                dc['T1'] = T1\n",
    "                dc['T2'] = T2\n",
    "                dc['tree'] = t2.write(format=9)\n",
    "            ls_outinfo.append(dc)\n",
    "    return ls_outinfo\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(processTree,files)\n",
    "pool.close()\n",
    "df_final = pd.DataFrame([j for i in results for j in i])\n",
    "df_final.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.b75o2.summary',sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot T1,T2 based on different tree structures with R\n",
    "```\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(tidyr)\n",
    "library(scales)\n",
    "library(dplyr)\n",
    "\n",
    "filename <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/fraglen10000_tree.b75o2.summary'\n",
    "\n",
    "x.df <- read.csv(filename,sep = '\\t',stringsAsFactors = FALSE,row.names = 1)\n",
    "x.df$Z <- sapply(x.df$chr,function(x) if (x=='chr21') return ('Z') else return('auto'))\n",
    "x.df <- x.df[x.df$tree !='',]\n",
    "x.df$tree <- gsub('J_z_zonalis|J_neildi','O',x.df$tree)\n",
    "\n",
    "groupXTrees <- function(tree_topology,outgroups = c('J_z_zonalis','J_neildi','O')){\n",
    "  # tree topology lookes like '(((J_c_coenia,J_c_grisea),J_nigrosuffusa),J_z_zonalis);'\n",
    "  # return \"J_c_grisea,J_nigrosuffusa,J_nigrosuffusaTX\"\n",
    "  a <- strsplit(tree_topology,split = '\\\\(|\\\\)| |,|;')\n",
    "  a <- a[[1]]\n",
    "  a <- a[a!='']\n",
    "  a <- a[!(a %in% outgroups)]\n",
    "  a <- sort(a)\n",
    "  return(paste(a,collapse = ','))\n",
    "}\n",
    "\n",
    "x.df$group <- sapply(x.df$tree,groupXTrees)\n",
    "# x.df$outgroup <- sapply(x.df$tree,getOutgroupXtrees)\n",
    "x.trees <- unique(x.df$tree)\n",
    "x.groups <- unique(x.df$group)\n",
    "# x.outgroups <- unique(x.df$outgroup)\n",
    "x.dfm <- x.df %>% group_by(chr, chrN, tree, Z, group) %>% summarise(T1=mean(T1),T2=mean(T2))\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/20181104mergedSamplesT1T2.pdf',width = 18,height = 9)\n",
    "\n",
    "\n",
    "workgroup <- x.groups[1]\n",
    "for (workgroup in x.groups){\n",
    "  x.df.1 <- x.dfm[x.dfm$group==workgroup,]\n",
    "  x.df.2 <- gather(x.df.1, key='T1T2',value = 'Height',-chr, -chrN, -tree, -Z,-group)\n",
    "  tvalue.T1 <- t.test(as.data.frame(x.df.2[x.df.2$T1T2 == 'T1' & x.df.2$Z =='auto','Height']), as.data.frame(x.df.2[x.df.2$T1T2 == 'T1' & x.df.2$Z =='Z','Height']))\n",
    "  tvalue.T2 <- t.test(as.data.frame(x.df.2[x.df.2$T1T2 == 'T2' & x.df.2$Z =='auto','Height']), as.data.frame(x.df.2[x.df.2$T1T2 == 'T2' & x.df.2$Z =='Z','Height']))\n",
    "  dat_text <- data.frame(label=paste0('pvalue: ',c(scientific(tvalue.T1$p.value,3), scientific(tvalue.T2$p.value,3))),T1T2=c('T1','T2'))\n",
    "  p1 <- ggplot(x.df.2, aes(x=Z,y=Height)) + geom_boxplot() + facet_wrap(~T1T2)+ stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5))) + geom_text(data=dat_text,mapping = aes(x = -Inf, y = -Inf, label=label), hjust = -0.1, vjust = -1,color='red') + ggtitle(workgroup)\n",
    "  \n",
    "  \n",
    "  p2 <- ggplot(x.df.2, aes(x=tree,y=Height)) + geom_boxplot() + facet_wrap(~T1T2) + stat_summary(fun.y=mean, colour=\"red\", geom=\"point\", size=3) + stat_summary(fun.y=mean, colour=\"blue\", geom=\"text\", vjust=-0.7, aes( label=round(..y.., digits=5))) + ggtitle(workgroup) + theme(axis.text.x = element_text(angle = 10))\n",
    "  x.df.3 <- x.df.2 %>% filter(T1T2=='T1')\n",
    "  tvalue.T1p <- pairwise.t.test(x.df.3$Height, x.df.3$tree,p.adjust.method = 'BH')\n",
    "  a <- tvalue.T1p$p.value\n",
    "  x.txt1 <- c(paste0(rownames(a)[1], colnames(a)[1],': ', scientific(a[rownames(a)[1],colnames(a)[1]],3)), paste0(rownames(a)[2], colnames(a)[1],': ', scientific(a[rownames(a)[2],colnames(a)[1]],3)),paste0(rownames(a)[2], colnames(a)[2],': ', scientific(a[rownames(a)[2],colnames(a)[2]],3)))\n",
    "  x.df.4 <- x.df.2 %>% filter(T1T2=='T2')\n",
    "  tvalue.T2p <- pairwise.t.test(x.df.4$Height, x.df.3$tree,p.adjust.method = 'BH')\n",
    "  a <- tvalue.T2p$p.value\n",
    "  x.txt2 <- c(paste0(rownames(a)[1], colnames(a)[1],': ', scientific(a[rownames(a)[1],colnames(a)[1]],3)), paste0(rownames(a)[2], colnames(a)[1],': ', scientific(a[rownames(a)[2],colnames(a)[1]],3)),paste0(rownames(a)[2], colnames(a)[2],': ', scientific(a[rownames(a)[2],colnames(a)[2]],3)))\n",
    "  dat_text2 <- data.frame(label=c(paste(x.txt1,collapse = '\\n'), paste(x.txt2, collapse = '\\n')),T1T2=c('T1','T2'))\n",
    "  p2 <- p2 + geom_text(data=dat_text2,mapping = aes(x = -Inf, y = -Inf, label=label), hjust = 0, vjust = -1,color='red')\n",
    "  print(workgroup)\n",
    "  print('T1 T2 comparison between Z and auto chromosomes')\n",
    "  print(p1)\n",
    "  print('T1 T2 comparison of different topology in auto chromosomes')\n",
    "  print(p2)\n",
    "  print('T1 pvalue')\n",
    "  print(x.txt1)\n",
    "  print(\"T2 pvalue\")\n",
    "  print(x.txt2)\n",
    "}\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count outgroup confidently not together\n",
    "```\n",
    "df_tree_b99_notO2 = df_tree[(~ df_tree['outgroupTogether']) & (df_tree['minBoots']>99)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gene tree from merged samples 6 group with BPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run BPP for one aligned fasta file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run BPP for one aligned fasta file\n",
    "```\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "BPP = '/home2/s185491/p/bpp/bpp-master/bpp'\n",
    "TREE1 = '(((J_c_coenia,(J_c_grisea,J_nigrosuffusa)),J_nigrosuffusaTX),(J_neildi,J_z_zonalis));'\n",
    "TREE2 = '((((J_c_coenia,J_c_grisea),J_nigrosuffusa),J_nigrosuffusaTX),(J_neildi,J_z_zonalis));'\n",
    "TREE3 = '(((J_c_coenia,J_nigrosuffusaTX),(J_c_grisea,J_nigrosuffusa)),(J_neildi,J_z_zonalis));'\n",
    "TREES = [TREE1,TREE2,TREE3]\n",
    "FOLDER_WORKING = '/work/biophysics/s185491/2018junonia/20181105bpp/temp/'\n",
    "FOLDER_OUTPUT1 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP1/'\n",
    "FOLDER_OUTPUT2 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP2/'\n",
    "FOLDER_OUTPUT3 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP3/'\n",
    "FOLDER_OUTPUTS = [FOLDER_OUTPUT1,FOLDER_OUTPUT2,FOLDER_OUTPUT3]\n",
    "IMAPFILE = '/work/biophysics/s185491/2018junonia/20181105bpp/Imap'\n",
    "THETA_A = 3\n",
    "THETA_B = 0.008\n",
    "TAU_A = 3\n",
    "TAU_B = 0.04\n",
    "\n",
    "A01 = '''\n",
    "          seed =  {SEED}\n",
    "\n",
    "       seqfile = {SEQFILE}\n",
    "      Imapfile = {IMAPFILE}\n",
    "       outfile = {OUTFILE}\n",
    "      mcmcfile = {MCMCFILE}\n",
    "\n",
    "  speciesdelimitation = 0 * fixed species tree\n",
    "         speciestree = 1  0.4 0.2 0.1   * speciestree pSlider ExpandRatio ShrinkRatio\n",
    "\n",
    "   speciesmodelprior = 1  * 0: uniform LH; 1:uniform rooted trees; 2: uniformSLH; 3: uniformSRooted\n",
    "\n",
    "  species&tree = 6  J_c_coenia  J_c_grisea  J_neildi  J_nigrosuffusa J_nigrosuffusaTX J_z_zonalis\n",
    "                    1  1  1  1  1  1\n",
    "                 {TREE}\n",
    "        diploid =   0  0  0  0  0  0\n",
    "   \n",
    "       usedata = 1  * 0: no data (prior); 1:seq like\n",
    "         nloci = 1  * number of data sets in seqfile\n",
    "\n",
    "     cleandata = 0    * remove sites with ambiguity data (1:yes, 0:no)?\n",
    "\n",
    "    thetaprior = {THETA_A} {THETA_B}   # invgamma(a, b) for theta\n",
    "      tauprior = {TAU_A} {TAU_B}    # invgamma(a, b) for root tau & Dirichlet(a) for other tau's\n",
    "\n",
    "      finetune =  1: 5 0.001 0.001  0.001 0.3 0.33 1.0  # finetune for GBtj, GBspr, theta, tau, mix, locusrate, seqerr\n",
    "\n",
    "         print = 1 0 0 0   * MCMC samples, locusrate, heredityscalars, Genetrees\n",
    "        burnin = 40000\n",
    "      sampfreq = 10\n",
    "       nsample = 200000\n",
    "\n",
    "'''\n",
    "\n",
    "def alignmentFasta2PhylipBPP(filename):\n",
    "    '''\n",
    "    filename is a filename of alignments in fasta format\n",
    "    if outfilename is None, write to filename+'.phylip'\n",
    "    '''\n",
    "    basename = os.path.basename(filename)\n",
    "    outfilename = os.path.join(FOLDER_WORKING,basename + '.phylip')\n",
    "    fout = open(outfilename,'w')\n",
    "    \n",
    "    seqNum = 0\n",
    "    seqIDs = []\n",
    "    for s in SeqIO.parse(filename,'fasta'):\n",
    "        seqNum += 1\n",
    "        seqIDs.append(s.id)\n",
    "    \n",
    "    nameLen = max(len(e) for e in seqIDs) + 3\n",
    "    \n",
    "    seqLen = len(s.seq)\n",
    "    fout.write('%d %d\\n'%(seqNum,seqLen))\n",
    "    \n",
    "    for s in SeqIO.parse(filename,'fasta'):\n",
    "        fout.write(s.id + '^'+ s.id +' '*(nameLen - len(s.id))+str(s.seq)+'\\n')\n",
    "    fout.close()\n",
    "    print('Format converting Finished!')\n",
    "    \n",
    "    # make A01 file and get commandline\n",
    "    commandlines = []\n",
    "    for n in range(3):\n",
    "        outfile_A01 = os.path.join(FOLDER_WORKING,basename+'.A01_%d'%(n+1))\n",
    "        outfile_result = os.path.join(FOLDER_WORKING, basename+'.out_%d'%(n+1))\n",
    "        outfile_mcmc = os.path.join(FOLDER_WORKING, basename+'.mcmc_%d'%(n+1))\n",
    "        fout = open(outfile_A01,'w')\n",
    "        fout.write(A01.format(SEED = -1, SEQFILE=outfilename, IMAPFILE=IMAPFILE, OUTFILE = outfile_result, MCMCFILE = outfile_mcmc, TREE = TREES[n],THETA_A = THETA_A, THETA_B = THETA_B, TAU_A = TAU_A, TAU_B = TAU_B))\n",
    "        fout.close()\n",
    "        commandline = '{BPP} --cfile {outfile_A01} && cp -f {outfile_result} {FOLDER_OUTPUT}{basename}'.format(BPP=BPP, outfile_A01 = outfile_A01, outfile_result=outfile_result, FOLDER_OUTPUT = FOLDER_OUTPUTS[n], basename=basename)\n",
    "        commandlines.append(commandline)\n",
    "    \n",
    "    commandlines.append('wait')\n",
    "    os.system(' & '.join(commandlines))\n",
    "    print('Finish BPP with tree')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='input a fasta file of sequence alignments, run BPP for 6 merged genome fragments 100kb, each with 3 individual runs.')\n",
    "    parser.add_argument('-i','--input', help = 'input file fasta file', required=True)\n",
    "    f = parser.parse_args()\n",
    "    alignmentFasta2PhylipBPP(filename=f.input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate cmds and qsub\n",
    "\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181105bpp/cmds/cmds'\n",
    "cmds = ['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runBPP_6mergedGenome_fraglen100000.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 12 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge results from three runs to one\n",
    "```\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000/'\n",
    "folders_results = ['/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP%d/'%n for n in [1,2,3]]\n",
    "folder_out = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP/'\n",
    "files_input = glob.glob(folder+'*')\n",
    "files_results = []\n",
    "for f in folders_results:\n",
    "     files_results += glob.glob(f+'*')\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_input:\n",
    "     dc_results[os.path.basename(f)] = []\n",
    "for f in files_results:\n",
    "     dc_results[os.path.basename(f)].append(f)\n",
    "\n",
    "Counter([len(e) for e in dc_results.values()])\n",
    "\n",
    "l_parameters = []\n",
    "for k in dc_results:\n",
    "     l_parameters.append([dc_results[k], 'J_neildi', 0.3, os.path.join(folder_out,k)])\n",
    "\n",
    "#test1\n",
    "f1 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP1.chr16_126'\n",
    "f2 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP2.chr16_126'\n",
    "f3 = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP3.chr16_126'\n",
    "resultA1 = bpp.processBPPresultPartA(f1)\n",
    "resultA2 = bpp.processBPPresultPartA(f2)\n",
    "resultA3 = bpp.processBPPresultPartA(f3)\n",
    "resultA12 = bpp.combine2BPPresultPartA_List(resultA1,resultA2)\n",
    "resultA = bpp.combineBPPresultPartAOne([f1,f2,f3])\n",
    "bpp.combineBPPresultPartAOne(*l_parameters[0])\n",
    "\n",
    "# run all\n",
    "pool = Pool(32)\n",
    "results = pool.starmap(bpp.combineBPPresultPartAOne, l_parameters)\n",
    "pool.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the 5849 gene trees, 45 do not have consistent results. discard them.\n",
    "Analyze the result\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000BPP/'\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "def processResultA(file):\n",
    "     '''\n",
    "     return a dc with tree as key and prob as value\n",
    "     '''\n",
    "     dc = {}\n",
    "     basename = os.path.basename(file)\n",
    "     dc['tree_id'] = basename\n",
    "     dc['chr'] = int(basename[3:].split('_')[0])\n",
    "     dc['chrN'] = int(basename[3:].split('_')[1])\n",
    "     for line in open(file):\n",
    "          tree, prob = line.strip().split('\\t')\n",
    "          prob = float(prob)\n",
    "          dc[tree] = prob\n",
    "     return dc\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(processResultA,files)\n",
    "pool.close()\n",
    "df_all = pd.DataFrame(results)\n",
    "df_all = df_all.fillna(0)\n",
    "columns = ['chr', 'chrN', 'tree_id'] + list(df_all.columns)[:-3]\n",
    "df_all = df_all.loc[:,columns]\n",
    "df_all = df_all.sort_values(by = ['chr','chrN'])\n",
    "df_all.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000.bpp.all',sep='\\t',index=None)\n",
    "\n",
    "df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "df_sum = df_sum.drop(columns=['chrN'])\n",
    "df_sum = df_sum.T\n",
    "df_sum['sum'] = df_sum.sum(axis=1)\n",
    "df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "df_sum = df_sum.drop(index=['chr'])\n",
    "df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "\n",
    "df_all2 = df_all.copy()\n",
    "trees = list(df_all.columns[3:])\n",
    "#for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "def processLine(row):\n",
    "     dc = {}\n",
    "     dc['chr'] = row['chr']\n",
    "     dc['chrN'] = row['chrN']\n",
    "     dc['tree_id'] = row['tree_id']\n",
    "     row_trees = row[trees]\n",
    "     row_trees = row_trees.sort_values(ascending=False)\n",
    "     dc['max_prob'] = row_trees[0]\n",
    "     dc['max_tree'] = row_trees.index[0]\n",
    "     dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "     return dc\n",
    "\n",
    "l = df_all.apply(processLine, axis=1)\n",
    "l = list(l)\n",
    "df_sum2 = pd.DataFrame(l)\n",
    "df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000.bpp.bestTree',sep='\\t',index=None)\n",
    "\n",
    "df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000.bpp.bestTree.sum',sep='\\t')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run BPP by grouping samples to coding and non-conding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split chromosome to coding and non-coding fragments. For coding fragments, discard those shorter than 100 bp. For non-coding fragments, discard those shorter than 100bp and those longer than 2000bp. For each chromosome, group the fragments so that each group have just above 100 fragments.   \n",
    "789 groups for coding  \n",
    "688 groups for non-coding\n",
    "coding sequences is about 3.84% of the genome  \n",
    "non-coding is about twice of coding.  \n",
    "totally about 7.26% of the genome. 2018 Anopheles paper, 7.8%\n",
    "```\n",
    "file_sitesInfo = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.position.base.scf.scfpos.gene.rna.exon.cds'\n",
    "file_scfInfo = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.txt'\n",
    "\n",
    "dc_scaf2chr = {} #key: scaf_id, value: chromosome and order in chromosome\n",
    "f = open(file_scfInfo)\n",
    "for line in f:\n",
    "     chromosome, scafs = line.strip().split(':',maxsplit=1)\n",
    "     scafs = scafs.strip(';').split(';')\n",
    "     scafs = [e[1:-1].split(':')[0] for e in scafs]\n",
    "     for n, scaf in enumerate(scafs):\n",
    "          dc_scaf2chr[scaf] = [chromosome,n]\n",
    "f.close()\n",
    "\n",
    "dc_scf2coding = {} #key: scaf_id, value, list of start and end of coding ids\n",
    "dc_scf2noncoding = {} #key: scaf_id, value, list of start and end of non-coding ids\n",
    "f = open(file_sitesInfo)\n",
    "for line in f:\n",
    "     position, base, scf, scfpos, gene, rna, exon, cds = line.split()\n",
    "     position = int(position)\n",
    "     if scf not in dc_scf2coding:\n",
    "          dc_scf2coding[scf] = []\n",
    "     if scf not in dc_scf2noncoding:\n",
    "          dc_scf2noncoding[scf] = []\n",
    "     if cds == '1':\n",
    "          dc_scf2coding[scf].append(position)\n",
    "     else:\n",
    "          dc_scf2noncoding[scf].append(position)\n",
    "#     break\n",
    "f.close()\n",
    "\n",
    "\n",
    "dc_scf2codingLoc = {}\n",
    "for scf, positions in dc_scf2coding.items():\n",
    "     if len(positions) == 0:\n",
    "          continue\n",
    "     frags = []\n",
    "     frag = [positions[0],positions[0]+1]\n",
    "     for p in positions[1:]:\n",
    "          if p == frag[-1]:\n",
    "               frag[-1] += 1\n",
    "          else:\n",
    "               frags.append(frag)\n",
    "               frag = [p,p+1]\n",
    "     frags.append(frag)\n",
    "     dc_scf2codingLoc[scf] = frags\n",
    "\n",
    "dc_scf2noncodingLoc = {}\n",
    "for scf, positions in dc_scf2noncoding.items():\n",
    "     if len(positions) == 0:\n",
    "          continue\n",
    "     frags = []\n",
    "     frag = [positions[0],positions[0]+1]\n",
    "     for p in positions[1:]:\n",
    "          if p == frag[-1]:\n",
    "               frag[-1] += 1\n",
    "          else:\n",
    "               frags.append(frag)\n",
    "               frag = [p,p+1]\n",
    "     frags.append(frag)\n",
    "     dc_scf2noncodingLoc[scf] = frags\n",
    "\n",
    "#dc_scf2codingLoc['000475F']\n",
    "ls_frag = [] # a list of dictionary, with type(coding or noncoding), chr, scf, start, end\n",
    "for scf, loc in dc_scf2codingLoc.items():\n",
    "     for start, end in loc:\n",
    "          dc = {}\n",
    "          dc['start'] = start\n",
    "          dc['end'] = end\n",
    "          dc['scf'] = scf\n",
    "          dc['type'] = 'c'\n",
    "          dc['chr'] = dc_scaf2chr[scf][0]\n",
    "          dc['chrN'] = dc_scaf2chr[scf][1]\n",
    "          ls_frag.append(dc)\n",
    "for scf, loc in dc_scf2noncodingLoc.items():\n",
    "     for start, end in loc:\n",
    "          dc = {}\n",
    "          dc['start'] = start\n",
    "          dc['end'] = end\n",
    "          dc['scf'] = scf\n",
    "          dc['type'] = 'n'\n",
    "          dc['chr'] = dc_scaf2chr[scf][0]\n",
    "          dc['chrN'] = dc_scaf2chr[scf][1]\n",
    "          ls_frag.append(dc)\n",
    "\n",
    "import pandas as pd\n",
    "df_frag = pd.DataFrame(ls_frag)\n",
    "df_frag = df_frag.sort_values(by=['chr','chrN','start'])\n",
    "df_frag.to_csv('/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder_coding.txt',sep='\\t',index=None,columns=['chr','chrN','scf','type','start','end'])\n",
    "\n",
    "df_frag['len'] = df_frag['end'] - df_frag['start']\n",
    "df_frag_c = df_frag[(df_frag['type']=='c') & (df_frag['len'] >= 100)]\n",
    "df_frag_n = df_frag[(df_frag['type']=='n') & (df_frag['len'] >= 100) & (df_frag['len'] <= 2000)]\n",
    "\n",
    "import numpy as np\n",
    "#frag in each chromosome, split to N fragments so that each fragments have just over 100 frags\n",
    "chromosome = list(df_frag_c['chr'].unique())\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_coding_100frag','w')\n",
    "for c in chromosome:\n",
    "     df_temp = df_frag_c[df_frag_c['chr'] == c]\n",
    "     #break\n",
    "\n",
    "     n = df_temp.shape[0]\n",
    "     num_group = n // 100\n",
    "     step = int(np.ceil(n/num_group))\n",
    "     print(n,step)\n",
    "     for fragn, i in enumerate(list(range(0,n,step))):\n",
    "          df_temp_group = df_temp.iloc[i:i+step,]\n",
    "          name = 'chr'+c+'_c_'+str(fragn)\n",
    "          fout.write(name+':')\n",
    "          positions = []\n",
    "          for start, end in zip(df_temp_group['start'],df_temp_group['end']):\n",
    "               positions = positions + list(range(start,end))\n",
    "          fout.write(' '.join(str(p) for p in positions))\n",
    "          fout.write('\\n')\n",
    "          print(name,len(positions))\n",
    "fout.close()\n",
    "\n",
    "chromosome = list(df_frag_n['chr'].unique())\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_noncoding_100frag','w')\n",
    "for c in chromosome:\n",
    "     df_temp = df_frag_n[df_frag_n['chr'] == c]\n",
    "     #break\n",
    "\n",
    "     n = df_temp.shape[0]\n",
    "     num_group = n // 100\n",
    "     step = int(np.ceil(n/num_group))\n",
    "     print(n,step)\n",
    "     for fragn, i in enumerate(list(range(0,n,step))):\n",
    "          df_temp_group = df_temp.iloc[i:i+step,]\n",
    "          name = 'chr'+c+'_n_'+str(fragn)\n",
    "          fout.write(name+':')\n",
    "          positions = []\n",
    "          for start, end in zip(df_temp_group['start'],df_temp_group['end']):\n",
    "               positions = positions + list(range(start,end))\n",
    "          fout.write(' '.join(str(p) for p in positions))\n",
    "          fout.write('\\n')\n",
    "          print(name,len(positions))\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get sequences. Combine outputs to same folder after job finished\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_coding_100frag -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_noncoding_100frag -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run BPP\n",
    "compared to merged sample frag100000, increase the mcmc runs 10 times. change sample frequency from 10 to 20.\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181105bpp/cmds/cmds'\n",
    "cmds = ['python /home2/s185491/p/xiaolongTools/project/Junonia/runBPP_6mergedGenome_frags100codingNonecoding.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 12 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarized the result was further summarized in one excel file. coding, non-coding, and all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge three runs\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp/'\n",
    "folders_results = ['/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bppBPP%d/'%n for n in [1,2,3]]\n",
    "folder_out = '/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bppBPP/'\n",
    "files_input = glob.glob(folder+'*')\n",
    "files_results = []\n",
    "for f in folders_results:\n",
    "     files_results += glob.glob(f+'*')\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_input:\n",
    "     dc_results[os.path.basename(f)] = []\n",
    "for f in files_results:\n",
    "     dc_results[os.path.basename(f)].append(f)\n",
    "\n",
    "Counter([len(e) for e in dc_results.values()])\n",
    "\n",
    "l_parameters = []\n",
    "for k in dc_results:\n",
    "     l_parameters.append([dc_results[k], 'J_neildi', 0.3, os.path.join(folder_out,k)])\n",
    "\n",
    "# run all\n",
    "pool = Pool(32)\n",
    "results = pool.starmap(bpp.combineBPPresultPartAOne, l_parameters)\n",
    "pool.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bppBPP/'\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "def processResultA(file):\n",
    "     '''\n",
    "     return a dc with tree as key and prob as value\n",
    "     '''\n",
    "     dc = {}\n",
    "     basename = os.path.basename(file)\n",
    "     dc['tree_id'] = basename\n",
    "     dc['chr'] = int(basename[3:].split('_')[0])\n",
    "     dc['type'] = basename[3:].split('_')[1]\n",
    "     dc['chrN'] = int(basename[3:].split('_')[2])\n",
    "     for line in open(file):\n",
    "          tree, prob = line.strip().split('\\t')\n",
    "          prob = float(prob)\n",
    "          dc[tree] = prob\n",
    "     return dc\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(processResultA,files)\n",
    "pool.close()\n",
    "df_all = pd.DataFrame(results)\n",
    "df_all = df_all.fillna(0)\n",
    "columns = ['chr', 'chrN', 'tree_id', 'type'] + list(df_all.columns)[:-4]\n",
    "df_all = df_all.loc[:,columns]\n",
    "df_all = df_all.sort_values(by = ['chr','type','chrN'])\n",
    "df_all.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.all',sep='\\t',index=None)\n",
    "\n",
    "df_all_ori = df_all.copy()\n",
    "df_all = df_all_ori.drop(columns=['type'])\n",
    "df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "df_sum = df_sum.drop(columns=['chrN'])\n",
    "df_sum = df_sum.T\n",
    "df_sum['sum'] = df_sum.sum(axis=1)\n",
    "df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "df_sum = df_sum.drop(index=['chr'])\n",
    "df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "df_sum.to_excel('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.xlsx')\n",
    "\n",
    "df_all2 = df_all.copy()\n",
    "trees = list(df_all.columns[3:])\n",
    "#for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "def processLine(row):\n",
    "     dc = {}\n",
    "     dc['chr'] = row['chr']\n",
    "     dc['chrN'] = row['chrN']\n",
    "     dc['tree_id'] = row['tree_id']\n",
    "     row_trees = row[trees]\n",
    "     row_trees = row_trees.sort_values(ascending=False)\n",
    "     dc['max_prob'] = row_trees[0]\n",
    "     dc['max_tree'] = row_trees.index[0]\n",
    "     dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "     return dc\n",
    "\n",
    "l = df_all.apply(processLine, axis=1)\n",
    "l = list(l)\n",
    "df_sum2 = pd.DataFrame(l)\n",
    "df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.bestTree',sep='\\t',index=None)\n",
    "\n",
    "df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.bestTree.sum',sep='\\t')\n",
    "\n",
    "#result for coding\n",
    "types = ['c','n']\n",
    "for t in types:\n",
    "     df_all = df_all_ori[df_all_ori['type']==t].drop(columns=['type'])\n",
    "     df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "     df_sum = df_sum.drop(columns=['chrN'])\n",
    "     df_sum = df_sum.T\n",
    "     df_sum['sum'] = df_sum.sum(axis=1)\n",
    "     df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "     df_sum = df_sum.drop(index=['chr'])\n",
    "     df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "     df_sum.to_excel('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.{t}.xlsx'.format(t=t))\n",
    "     \n",
    "     df_all2 = df_all.copy()\n",
    "     trees = list(df_all.columns[3:])\n",
    "     #for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "     def processLine(row):\n",
    "          dc = {}\n",
    "          dc['chr'] = row['chr']\n",
    "          dc['chrN'] = row['chrN']\n",
    "          dc['tree_id'] = row['tree_id']\n",
    "          row_trees = row[trees]\n",
    "          row_trees = row_trees.sort_values(ascending=False)\n",
    "          dc['max_prob'] = row_trees[0]\n",
    "          dc['max_tree'] = row_trees.index[0]\n",
    "          dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "          return dc\n",
    "     \n",
    "     l = df_all.apply(processLine, axis=1)\n",
    "     l = list(l)\n",
    "     df_sum2 = pd.DataFrame(l)\n",
    "     df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.bestTree.{t}'.format(t=t),sep='\\t',index=None)\n",
    "     \n",
    "     df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "     df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "     df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "     df_sum4 = df_sum4.fillna(0)\n",
    "     df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "     df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "     df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "     df_sum4 = df_sum4.astype(int)\n",
    "     df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp.bpp.bestTree.sum.{t}'.format(t=t),sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run RAxML with frag100_bpp\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folders = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp/\n",
    "'''.split()\n",
    "outfolders = [e[:-1]+'_tree' for e in folders]\n",
    "outcommands = [e[:-1] + '.cmds' for e in folders]\n",
    "\n",
    "for folder, outfolder, outfile_cmds in zip(folders, outfolders, outcommands):\n",
    "#changed to add bootstrap\n",
    "    files = glob.glob(folder+'*')\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename} -o J_neildi,J_z_zonalis -x 234 -N 1000 -f a && mv RAxML_bipartitions.{basename} {outfolder}/{basename} && rm RAxML*{basename} && rm {basename}.*'\n",
    "    open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "```\n",
    "\n",
    "run\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp.cmds\n",
    "```\n",
    "\n",
    "collect tree\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process result\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp_tree/'\n",
    "dirname = os.path.dirname(folder)\n",
    "outfilename = dirname+'.sum'\n",
    "files = glob.glob(folder +'*')\n",
    "fout = open(outfilename,'w')\n",
    "for f in files:\n",
    "     tree = open(f).read()\n",
    "     fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "fout.close()\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp_tree.sum'\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    \n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_type, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['type'] = tree_type\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','type','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "df_tree = df_tree.drop(columns='index')\n",
    "\n",
    "def getTopology(tree):\n",
    "    tree = tree.copy()\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_z_zonalis','J_neildi'))\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=35) & (df_tree['outgroupTogether']==True)].copy()\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp_tree.sum.b75o2.xlsx') #store the tree toplogy count in each chromosome\n",
    "\n",
    "for tree_type in 'cn':\n",
    "    df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=35) & (df_tree['outgroupTogether']==True) & (df_tree['type'] == tree_type)].copy()\n",
    "    df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "    treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "    treeTopology = {}\n",
    "    for chromosome in df_tree_b75_O2.chr.unique():\n",
    "        treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "    df_treeTopology = pd.DataFrame(treeTopology)\n",
    "    df_treeTopology = df_treeTopology.fillna(0)\n",
    "    df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "    df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "    df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "    df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MergeSample/100frag_bpp_tree.sum.b75o2.{tree_type}.xlsx'.format(tree_type=tree_type)) #store the tree toplogy count in each chromosome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run MrBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181105bpp/cmds/cmds'\n",
    "cmds = ['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bpp/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181105bpp/cmds/cmds'\n",
    "cmds = ['python /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'a').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 4 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MrBayes result\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/work/biophysics/s185491/2018junonia/20181105bpp/100frag_bppNewick/\n",
    "/work/biophysics/s185491/2018junonia/20181105bpp/fraglen100000Newick/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree+'\\n')\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp'\n",
    "basename = 'fraglen100000Newick.sum'\n",
    "filename = os.path.join(folder,basename)\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)]\n",
    "df_tree_b75_O2 = df_tree_b75_O2.reset_index().copy()\n",
    "def getTopology(tree):\n",
    "    tree = tree.copy()\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_z_zonalis','J_neildi'))\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.xlsx')) #store the tree toplogy count in each chromosome\n",
    "\n",
    "# '100frag_bpp'\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181105bpp'\n",
    "basename = '100frag_bppNewick.sum'\n",
    "filename = os.path.join(folder,basename)\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_type, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['type'] = tree_type\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','type','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "df_tree = df_tree.drop(columns='index')\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)].copy()\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.xlsx')) #store the tree toplogy count in each chromosome\n",
    "\n",
    "for tree_type in 'cn':\n",
    "    df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True) & (df_tree['type'] == tree_type)].copy()\n",
    "    df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "    treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "    treeTopology = {}\n",
    "    for chromosome in df_tree_b75_O2.chr.unique():\n",
    "        treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "    df_treeTopology = pd.DataFrame(treeTopology)\n",
    "    df_treeTopology = df_treeTopology.fillna(0)\n",
    "    df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "    df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "    df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "    df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.{tree_type}.xlsx'.format(tree_type=tree_type))) #store the tree toplogy count in each chromosome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all samples in each of 6 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sequences\n",
    "an example codes shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen10000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026MultiSampe6groups/fraglen10000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select best 5 sequences from each species\n",
    "\n",
    "6 species. for each fragmement, each species only keep 5 samples with the least number of gaps. the max allowed gaps in each fragments is 70%. in each group, each species have at least 3 species.  \n",
    "before filter: 58580. after fileter: 52536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "\n",
    "keepNum = 5\n",
    "file_sample2species = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/id2species'\n",
    "\n",
    "dc_sample2species = {}\n",
    "for line in open(file_sample2species):\n",
    "    k,v = line.split()\n",
    "    dc_sample2species[k] = v\n",
    "\n",
    "def filterSeq(filename, outputfolder, gap_ratio_max=0.7, keepNumMax=5, keepNumMin=3):\n",
    "    '''\n",
    "    remove sequences with gap_ratio greater than gap_ratio_max\n",
    "    for each fragment, keep if all species have at least 3 sequences\n",
    "    for each species, keep at most 5 sequences\n",
    "    write the output to outputfolder. filename is the same as input filename\n",
    "    '''\n",
    "    fileout = os.path.join(outputfolder,os.path.basename(filename))\n",
    "    ls_fastas = list(SeqIO.parse(filename,'fasta'))#read in input\n",
    "    seqlen = len(ls_fastas[0].seq)\n",
    "    max_gap = seqlen * gap_ratio_max\n",
    "    ls_fastas = [e for e in ls_fastas if str(e.seq).count('-') <max_gap]#remove sequences with too many gaps\n",
    "    \n",
    "    dc_speces2seq = {}# assign each sequences to species\n",
    "    for fasta in ls_fastas:\n",
    "        species = dc_sample2species[fasta.id]\n",
    "        if species not in dc_speces2seq:\n",
    "            dc_speces2seq[species] = []\n",
    "        dc_speces2seq[species].append(fasta)\n",
    "    \n",
    "    if len(dc_speces2seq) < 6:#return None if not all 6 species identified\n",
    "        print('less than 6 species in file', os.path.basename(filename))\n",
    "        return None\n",
    "    for k,v in dc_speces2seq.items():#return None if any of the 6 species have less than keepNumMin samples\n",
    "        if len(v) < keepNumMin:\n",
    "            print(os.path.basename(filename),'less than 3 samples of species', k)\n",
    "            return None\n",
    "    to_write = []\n",
    "    for v in dc_speces2seq.values():\n",
    "        v.sort(key=lambda x:str(x.seq).count('-'))\n",
    "        for i in range(min(keepNum,len(v))):\n",
    "            to_write.append('>'+v[i].id+'\\n'+str(v[i].seq)+'\\n')\n",
    "    fout = open(fileout,'w')\n",
    "    fout.write(''.join(to_write))\n",
    "    fout.close()\n",
    "\n",
    "files = glob.glob('/dev/shm/temp/fraglen10000/*')\n",
    "outputfolder = '/dev/shm/temp/fraglen10000.keep5'\n",
    "pool = Pool(32)\n",
    "pool.starmap(filterSeq,[(e,outputfolder) for e in files])\n",
    "pool.close()\n",
    "\n",
    "#filterSeq('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000/chr2_676','/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no bootstrap testing\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folders = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5/\n",
    "'''.split()\n",
    "outfolders = [e[:-1]+'_tree' for e in folders]\n",
    "outcommands = [e[:-1] + '.cmds' for e in folders]\n",
    "\n",
    "for folder, outfolder, outfile_cmds in zip(folders, outfolders, outcommands):\n",
    "#changed to add bootstrap\n",
    "    files = glob.glob(folder+'*')\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename}   && mv RAxML_bestTree.{basename} {outfolder}/{basename} && rm {basename} && rm RAxML*{basename} && rm {basename}.*'\n",
    "    open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "    \n",
    "    \n",
    "folder2 = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree/'\n",
    "files2 = glob.glob(folder2+'*')\n",
    "files2 = set([os.path.basename(e) for e in files2])\n",
    "print(len(files2))\n",
    "files = [e for e in files if os.path.basename(e) not in files2]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect trees\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get trees where samples from same species are together\n",
    "\n",
    "```\n",
    "\n",
    "from ete3 import Tree\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#filename = r\"C:\\Users\\K\\Downloads\\chr10_871\"\n",
    "#tree = Tree(open(filename).read())\n",
    "\n",
    "def addNodeNames(tree):\n",
    "    '''\n",
    "    the internal nodes have no names. add unique names like node1, node2 to them\n",
    "    '''\n",
    "    n = 1\n",
    "    for node in tree.traverse():\n",
    "        if not node.is_leaf():\n",
    "            node.name = 'node' + str(n)\n",
    "            n += 1\n",
    "    return None\n",
    "\n",
    "def add_species(tree):\n",
    "    '''\n",
    "    add speces based on name for leaves\n",
    "    '''\n",
    "    for leaf in tree.iter_leaves():\n",
    "        if leaf.name in dc_sample2species:\n",
    "            leaf.add_features(species=dc_sample2species[leaf.name])\n",
    "    return None\n",
    "\n",
    "def get_leaf_species(tree):\n",
    "    '''\n",
    "    get the species names for a tree node\n",
    "    '''\n",
    "    return([e.species for e in tree.get_leaves()])\n",
    "\n",
    "def checkIfWith6monoTrees(tree):\n",
    "    '''\n",
    "    given a tree, check if there are 6 branches with 5 samples from the same species\n",
    "    '''\n",
    "    if len(tree.get_leaves()) != 30:\n",
    "        return False\n",
    "    tree = tree.copy()\n",
    "    addNodeNames(tree)\n",
    "    add_species(tree)\n",
    "    tr_species = set()\n",
    "    tree.unroot()\n",
    "    for node in tree.children:\n",
    "        if not node.is_leaf():\n",
    "            if len(node.get_leaves()) == 5:\n",
    "                leave_species = get_leaf_species(node)\n",
    "                if len(set(leave_species)) == 1:\n",
    "                    tr_species.add(leave_species[0])\n",
    "    for node in tree.iter_descendants():\n",
    "        tree2 = tree.copy()\n",
    "        tree2.set_outgroup(node.name)\n",
    "        for node in tree2.children:\n",
    "            if not node.is_leaf():\n",
    "                if len(node.get_leaves()) == 5:\n",
    "                    leave_species = get_leaf_species(node)\n",
    "                    if len(set(leave_species)) == 1:\n",
    "                        tr_species.add(leave_species[0])\n",
    "    if len(tr_species) == 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    addNodeNames(tree)\n",
    "    dc ={}\n",
    "    dc['tree_id']=tree_id\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['tree'] = tree\n",
    "    dc['6monoTrees'] = checkIfWith6monoTrees(tree)\n",
    "    return dc\n",
    "\n",
    "\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree.sum'\n",
    "file_sample2species = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/id2species'\n",
    "\n",
    "dc_sample2species = {}\n",
    "for line in open(file_sample2species):\n",
    "    k,v = line.split()\n",
    "    dc_sample2species[k] = v\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "df_good = df_tree[df_tree['6monoTrees']][['chr','chrN']]\n",
    "print(list(df_good['tree_id']))\n",
    "print(df_good.shape)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot trees with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "library(ggtree)\n",
    "library(ggrepel)\n",
    "library(ggplot2)\n",
    "\n",
    "tree <- read.tree('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree/chr10_20')\n",
    "\n",
    "xdf.sample2species = read.csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/id2species',sep = '\\t',stringsAsFactors = FALSE,header = FALSE,colClasses = 'character')\n",
    "colnames(xdf.sample2species) <- c('sample','species')\n",
    "\n",
    "p <- ggtree(tree,layout = 'daylight') \n",
    "p %<+% xdf.sample2species + geom_tiplab2(aes(angle=angle),hjust = 0.5,vjust=2) + geom_tippoint(aes(size=5,color=species,shape=species),alpha=0.7) + scale_shape_manual(values=c(15,16,17,15,16,17))+ theme(legend.position=\"right\")+ guides(colour = guide_legend(override.aes = list(size=5))) + ggtitle('chr10_20')\n",
    "\n",
    "folder <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree/'\n",
    "files = c('chr10_871', 'chr11_1841', 'chr11_2043', 'chr11_2097', 'chr12_89', 'chr13_2360', 'chr13_2797', 'chr13_633', 'chr13_881', 'chr15_1020', 'chr15_1255', 'chr16_1019', 'chr18_1196', 'chr18_2020', 'chr19_1715', 'chr4_314', 'chr5_1641', 'chr8_525', 'chr9_466', 'chr9_623')\n",
    "\n",
    "files <- c('chr21_744', 'chr21_1446', 'chr21_1852', 'chr21_1861', 'chr21_1905', 'chr21_614', 'chr21_1896', 'chr21_1804', 'chr21_1430', 'chr21_1380', 'chr21_1894', 'chr21_669', 'chr21_1930', 'chr21_816', 'chr21_1429', 'chr21_1436', 'chr21_610', 'chr21_1867', 'chr21_1903', 'chr21_661', 'chr16_202', 'chr21_733', 'chr21_1906', 'chr21_621', 'chr21_1434', 'chr21_626', 'chr21_599', 'chr21_1458', 'chr21_1234', 'chr21_558', 'chr21_1819', 'chr21_1878', 'chr21_678', 'chr21_630', 'chr21_660', 'chr21_1907', 'chr21_1854', 'chr21_1926', 'chr21_1923', 'chr21_1882', 'chr21_1452', 'chr21_1447', 'chr21_708', 'chr21_718', 'chr21_1432', 'chr21_1922', 'chr21_1868', 'chr21_613', 'chr21_721', 'chr21_1890', 'chr21_668', 'chr21_1855', 'chr21_688', 'chr21_1378', 'chr21_1398', 'chr21_1286', 'chr21_1383', 'chr21_1880')\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen10000.keep5_tree.good58trees.pdf',width = 15, height = 9)\n",
    "for (t in files){\n",
    "  tree <- paste0(folder,t)\n",
    "  tree <- read.tree(tree)\n",
    "  p <- ggtree(tree,layout = 'daylight') \n",
    "  p <- p %<+% xdf.sample2species + geom_tiplab2(aes(angle=angle),hjust = 0.5,vjust=2) + geom_tippoint(aes(size=5,color=species,shape=species),alpha=0.7) + scale_shape_manual(values=c(15,16,17,15,16,17))+ theme(legend.position=\"right\")+ guides(colour = guide_legend(override.aes = list(size=5))) + ggtitle(t)\n",
    "  print(p)\n",
    "}\n",
    "dev.off()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "folder <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen20000.keep5_tree/'\n",
    "files <- c('chr21_699', 'chr21_360', 'chr21_382', 'chr21_649', 'chr21_431', 'chr21_940', 'chr21_930', 'chr21_681', 'chr21_346', 'chr21_953', 'chr21_939', 'chr21_315', 'chr21_637', 'chr21_140', 'chr21_715', 'chr21_728', 'chr21_951', 'chr21_366', 'chr21_299', 'chr21_362', 'chr21_716', 'chr21_723', 'chr16_101', 'chr21_372', 'chr21_279', 'chr21_343', 'chr21_725', 'chr21_948', 'chr21_714', 'chr21_617', 'chr21_640', 'chr21_638', 'chr21_330', 'chr21_717', 'chr21_313', 'chr21_927', 'chr21_689', 'chr21_408', 'chr21_724', 'chr21_704', 'chr21_929', 'chr21_928', 'chr21_961', 'chr21_691')\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen20000.keep5_tree.good44trees.pdf',width = 15, height = 9)\n",
    "```\n",
    "\n",
    "```\n",
    "folder <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen50000.keep5_tree/'\n",
    "files <- c('chr21_289', 'chr21_376', 'chr21_267', 'chr21_123', 'chr21_286', 'chr21_381', 'chr21_115', 'chr21_291', 'chr21_163', 'chr21_56', 'chr21_138', 'chr21_280', 'chr21_285', 'chr21_173', 'chr21_144', 'chr21_370', 'chr21_372', 'chr21_361', 'chr21_257', 'chr21_279', 'chr21_259', 'chr21_373', 'chr21_111', 'chr21_125', 'chr21_268', 'chr21_143', 'chr21_383', 'chr21_384', 'chr21_148', 'chr21_377', 'chr21_375', 'chr21_126', 'chr21_124', 'chr21_275', 'chr21_122', 'chr21_146', 'chr16_40')\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen50000.keep5_tree.good37trees.pdf',width = 15, height = 9)\n",
    "```\n",
    "\n",
    "```\n",
    "folder <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen100000.keep5_tree/'\n",
    "files <- c('chr21_71', 'chr21_145', 'chr21_86', 'chr21_127', 'chr21_129', 'chr21_73', 'chr21_192', 'chr21_187', 'chr21_133', 'chr21_28', 'chr21_81', 'chr21_55', 'chr21_139', 'chr21_180', 'chr21_103', 'chr21_190', 'chr21_144', 'chr21_128', 'chr21_185', 'chr21_72', 'chr21_137', 'chr21_69', 'chr21_61', 'chr21_142', 'chr21_191', 'chr16_20', 'chr21_62', 'chr21_131', 'chr21_122', 'chr21_189', 'chr21_60', 'chr21_67')\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/fraglen100000.keep5_tree.good32trees.pdf',width = 15, height = 9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get SAF (simple alignment format for 10kb chr16_202\n",
    "\n",
    "```\n",
    "\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "filename = r\"C:\\Users\\K\\OneDrive\\Lab\\UTSW\\2018JunoniaProject_Xiaolong\\20180927geneTree\\20181031bestSample6species\\fraglen10000.chr16_202.263samples.txt\"\n",
    "file_summary = r\"C:\\Users\\K\\OneDrive\\Lab\\UTSW\\2018JunoniaProject_Xiaolong\\summary\\20181003Junonia-all-sample-summary.xlsx\"\n",
    "\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all',dtype=str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "df_summary['Number'] = sample_prefix\n",
    "print('unique sample prefix', len(set(sample_prefix)))\n",
    "dc_id2shortname = dict(zip(df_summary['Number'], df_summary['Treename']))\n",
    "\n",
    "ls_seqs = list(SeqIO.parse(filename,'fasta'))\n",
    "for s in ls_seqs:\n",
    "    s.id = dc_id2shortname[s.id]\n",
    "ls_seqs = sorted(ls_seqs,key=lambda x:x.id)\n",
    "\n",
    "fout = open(filename[:-4]+'SAF.txt','w')\n",
    "for s in ls_seqs:\n",
    "    fout.write('{key:<60}  {seq}\\n'.format(key=s.id,seq=str(s.seq)))\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 species, merge each site to chose the most common nt\n",
    "for selected samples in each species, for each nt in each position, choose the most common nt. If there more than one most common nts, choose one of them randomly. If all gap, choose gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences\n",
    "```\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "file_id2species = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181026AllSample6groups/id2species'\n",
    "\n",
    "dc_speciesMapFiles = {}\n",
    "for line in open(file_id2species):\n",
    "     sample_id, species = line.split()\n",
    "     if species not in dc_speciesMapFiles:\n",
    "          dc_speciesMapFiles[species] = []\n",
    "     file_sample = '/archive/butterfly/maps/debiased/'+sample_id+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'\n",
    "     dc_speciesMapFiles[species].append(file_sample)\n",
    "     #print(sample_id, os.path.exists(file_sample))\n",
    "\n",
    "def processSpecies(species):\n",
    "     files = dc_speciesMapFiles[species]\n",
    "     ls_openfiles = [open(e) for e in files]\n",
    "     fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/'+species+'.map','w')\n",
    "     counts_empty = 0\n",
    "     counts_common = 0\n",
    "     counts_dominant = 0\n",
    "     counts_equalChance = 0\n",
    "     for lines in zip(*ls_openfiles):\n",
    "          nts = ''.join(lines).split()\n",
    "          nts = [e for e in nts if e in \"ATCG\"]\n",
    "          if len(nts) == 0:\n",
    "               fout.write('-\\n')\n",
    "               counts_empty += 1\n",
    "          else:\n",
    "               nt_counts = Counter(nts).most_common()\n",
    "               if len(nt_counts) == 1:\n",
    "                    counts_common += 1\n",
    "                    fout.write(nt_counts[0][0]+'\\n')\n",
    "               else:\n",
    "                    counts_max = nt_counts[0][1]\n",
    "                    nt_most = [e[0] for e in nt_counts if e[1] == counts_max]\n",
    "                    if len(nt_most) == 1:\n",
    "                         fout.write(nt_most[0]+'\\n')\n",
    "                         counts_dominant += 1\n",
    "                    else:\n",
    "                         nt_choice = random.choice(nt_most)\n",
    "                         counts_equalChance += 1\n",
    "                         fout.write(nt_choice+'\\n')\n",
    "     fout.close()\n",
    "     print('counts_empty counts_common counts_dominant counts_equalChance',counts_empty, counts_common, counts_dominant,counts_equalChance)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "p = Pool(10)\n",
    "p.map(processSpecies,list(dc_speciesMapFiles.keys()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "counts_empty counts_common counts_dominant counts_equalChance 60821021 469006931 53157054 3008668\n",
    "counts_empty counts_common counts_dominant counts_equalChance 53402634 470786095 59682412 2122533\n",
    "counts_empty counts_common counts_dominant counts_equalChance 80369266 449562308 53992328 2069772\n",
    "counts_empty counts_common counts_dominant counts_equalChance 33618870 455829136 95018906 1526762\n",
    "counts_empty counts_common counts_dominant counts_equalChance 38824056 452883294 92510565 1775759\n",
    "counts_empty counts_common counts_dominant counts_equalChance 65248446 438670511 80211101 1863616\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get informative sites\n",
    "for aligned sequences of 6 species, for each site if there are <= 2 gaps and more than 1 kind of bases, this site is considered informative\n",
    "```\n",
    "\n",
    "file_mapfiles = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles'\n",
    "\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dc_species = {}\n",
    "files = open(file_mapfiles).read().split()\n",
    "ls_species_names = [os.path.basename(f).split('.')[0] for f in files]\n",
    "ls_species_seqs = mapfileIO.read2Int8s(files,threads=6)\n",
    "\n",
    "def checkInformative(sitesInt):\n",
    "     '''\n",
    "     informative sites are sites where there is at most 2 gap and other than gaps, there are at least two different kind of bases\n",
    "     return True or False\n",
    "     '''\n",
    "     bases = set()\n",
    "     gap = 0\n",
    "     for site in sitesInt:\n",
    "          if site == 45:\n",
    "               gap += 1\n",
    "          else:\n",
    "               bases.add(site)\n",
    "     if gap > 2:\n",
    "          return False\n",
    "     if len(bases) == 1:\n",
    "          return False\n",
    "     return True\n",
    "\n",
    "seqlen = len(ls_species_seqs[0])\n",
    "\n",
    "def checkInformative_multipleSites(sitesInts):\n",
    "     return list(map(checkInformative,sitesInts))\n",
    "\n",
    "def checkInformative_np(test):\n",
    "     return  np.apply_along_axis(checkInformative,axis=0, arr=test)\n",
    "\n",
    "ls_seq2d = np.array(ls_species_seqs)\n",
    "step = seqlen // 32 + 1\n",
    "ls_seq2d_ls = [ls_seq2d[:,i:i+step] for i in range(0,seqlen,step)]\n",
    "\n",
    "#sitesInts = list(zip(*ls_species_seqs))\n",
    "#sitesInts_ls = np.array_split(sitesInts,32)\n",
    "pool = Pool(32)\n",
    "informative_sites = pool.map(checkInformative_np,ls_seq2d_ls)\n",
    "print(seqlen,len(informative_sites))\n",
    "pool.close()\n",
    "\n",
    "sites = np.array([i for j in informative_sites for i in j])\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/sitesNonIdentical_6MostCommon','w')\n",
    "fout.write('\\n'.join(str(n) for n,e in enumerate(sites) if e))\n",
    "fout.close()\n",
    "```\n",
    "\n",
    "Totally 25917829 sites, 4.42% were considered informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order informative sites according to location in the chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "filename = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.tab'\n",
    "dc_scfinfo = {}\n",
    "dc_chrinfo = {}\n",
    "for line in open(filename):\n",
    "     chr_id, scf, start, end = line.split()\n",
    "     dc_scfinfo[scf] = (chr_id, int(start), int(end))\n",
    "     if chr_id not in dc_chrinfo:\n",
    "          dc_chrinfo[chr_id] = []\n",
    "     dc_chrinfo[chr_id].append((scf, int(start), int(end)))\n",
    "\n",
    "file_sitesNonIdentical = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/sitesNonIdentical_6MostCommon'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sites = np.loadtxt(file_sitesNonIdentical,dtype=int)\n",
    "\n",
    "df_scf = pd.read_csv(filename,sep='\\t',dtype={0:str,1:str,2:int,3:int},header=None)\n",
    "df_scf.columns = ['chr','scf','start','end']\n",
    "df_scf = df_scf.sort_values(by='start')\n",
    "scf_starts = np.array(df_scf['start'])\n",
    "scf_ends = np.array(df_scf['end'])\n",
    "scf_ids = np.array(df_scf['scf'])\n",
    "\n",
    "def getscf(site):\n",
    "     '''\n",
    "     given a site, return its scf\n",
    "     '''\n",
    "     n = np.argmax(scf_ends>site)\n",
    "     return scf_ids[n]\n",
    "\n",
    "ls_scf = np.vectorize(getscf)(sites)\n",
    "\n",
    "dc_scf_sitesNoeIdentical = {k:[] for k in dc_scfinfo}\n",
    "for site, scf in zip(sites, ls_scf):\n",
    "     dc_scf_sitesNoeIdentical[scf].append(site)\n",
    "\n",
    "dc_chr_sitesNoeIdentical = {k:[] for k in dc_chrinfo}\n",
    "for chr_id, chr_info in dc_chrinfo.items():\n",
    "     for scf_info in chr_info:\n",
    "          scf = scf_info[0]\n",
    "          dc_chr_sitesNoeIdentical[chr_id] += dc_scf_sitesNoeIdentical[scf]\n",
    "\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/sitesNonIdentical_6MostCommon.chr','w')\n",
    "for chr_id, s in dc_chr_sitesNoeIdentical.items():\n",
    "     fout.write(chr_id +'\\t')\n",
    "     fout.write(' '.join(str(e) for e in s))\n",
    "     fout.write('\\n')\n",
    "fout.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAxML scan window 10kb, 20kb, 50kb and 100kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get sequences\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen10000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen10000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen20000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen20000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen50000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen50000\n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate scripts  \n",
    "note, boostrap replicates is 1000 for 100frag_bpp. not shown in the scritps\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folders = '''/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen10000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen20000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen50000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen100000/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp/\n",
    "'''.split()\n",
    "outfolders = [e[:-1]+'_tree' for e in folders]\n",
    "outcommands = [e[:-1] + '.cmds' for e in folders]\n",
    "\n",
    "for folder, outfolder, outfile_cmds in zip(folders, outfolders, outcommands):\n",
    "#changed to add bootstrap\n",
    "    files = glob.glob(folder+'*')\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "    commandline = 'cd /dev/shm && cp {fullname} ./ && /home/xcao/p/RAxML/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3 -m GTRGAMMA -p 234 -s {basename} -n {basename} -o J_neildi,J_z_zonalis -x 234 -N 100 -f a && mv RAxML_bipartitions.{basename} {outfolder}/{basename} && rm RAxML*{basename} && rm {basename}.*'\n",
    "    open(outfile_cmds,'w').write('\\n'.join(commandline.format(outfolder=outfolder, fullname = e, basename=os.path.basename(e)) for e in files))\n",
    "```\n",
    "\n",
    "Run\n",
    "\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen10000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen20000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen50000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen100000.cmds\n",
    "python3 /home/xcao/p/xiaolongTools/multiThread.py 48 /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp.cmds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect trees and summarize results\n",
    "\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen10000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen20000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen50000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen100000_tree/\n",
    "/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp_tree/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree)\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "for fraglen in ['fraglen10000', 'fraglen20000', 'fraglen50000', 'fraglen100000']:\n",
    "    filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/{fraglen}_tree.sum'.format(fraglen=fraglen)\n",
    "    \n",
    "    txts = open(filename).readlines()\n",
    "    pool = Pool(32)\n",
    "    df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "    pool.close()\n",
    "    \n",
    "    df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "    df_tree = df_tree.reset_index()\n",
    "    \n",
    "    df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)]\n",
    "    df_tree_b75_O2 = df_tree_b75_O2.reset_index().copy()\n",
    "    def getTopology(tree):\n",
    "        tree = tree.copy()\n",
    "        tree.set_outgroup(tree.get_common_ancestor('J_z_zonalis','J_neildi'))\n",
    "        tree.sort_descendants()\n",
    "        return tree.write(format=9)\n",
    "    df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "    treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "    \n",
    "    fout = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/{fraglen}_tree.b75o2.names'.format(fraglen=fraglen),'w')\n",
    "    for chromosome, chrN in zip(df_tree_b75_O2['chr'],df_tree_b75_O2['chrN']):\n",
    "        fout.write('chr'+str(chromosome)+'_'+str(chrN)+'\\n')\n",
    "    fout.close() #store filenames of trees with minBoots >=75 and outgroupTogether\n",
    "    \n",
    "    treeTopology = {}\n",
    "    for chromosome in df_tree_b75_O2.chr.unique():\n",
    "        treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "    df_treeTopology = pd.DataFrame(treeTopology)\n",
    "    df_treeTopology = df_treeTopology.fillna(0)\n",
    "    df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "    df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "    df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "    df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/{fraglen}_tree.sum.b75o2.xlsx'.format(fraglen=fraglen)) #store the tree toplogy count in each chromosome\n",
    "    \n",
    "# '100frag_bpp'\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/{fraglen}_tree.sum'.format(fraglen='100frag_bpp')\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_type, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['type'] = tree_type\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','type','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "df_tree = df_tree.drop(columns='index')\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=35) & (df_tree['outgroupTogether']==True)].copy()\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp_tree.sum.b75o2.xlsx') #store the tree toplogy count in each chromosome\n",
    "\n",
    "for tree_type in 'cn':\n",
    "    df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=35) & (df_tree['outgroupTogether']==True) & (df_tree['type'] == tree_type)].copy()\n",
    "    df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "    treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "    treeTopology = {}\n",
    "    for chromosome in df_tree_b75_O2.chr.unique():\n",
    "        treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "    df_treeTopology = pd.DataFrame(treeTopology)\n",
    "    df_treeTopology = df_treeTopology.fillna(0)\n",
    "    df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "    df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "    df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "    df_treeTopology.to_excel('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp_tree.sum.b75o2.{tree_type}.xlsx'.format(tree_type=tree_type)) #store the tree toplogy count in each chromosome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1T2 caulcuate and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import glob\n",
    "import pandas as pd\n",
    "from ete3 import Tree\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils')\n",
    "import calculateT1T2FromMultipleFastaAlignment\n",
    "\n",
    "def processTree(file,folder,folder_fasta):\n",
    "    file_tree = os.path.join(folder,file)\n",
    "    file_alignment = os.path.join(folder_fasta,file)\n",
    "    tree = Tree(open(file_tree).read())\n",
    "    chromosome, chromosomeN = file.split('_')\n",
    "    ls_outinfo = []\n",
    "    def addNodeNames(tree):\n",
    "        '''\n",
    "        the internal nodes have no names. add unique names like node1, node2 to them\n",
    "        '''\n",
    "        n = 1\n",
    "        for node in tree.traverse():\n",
    "            if not node.is_leaf():\n",
    "                node.name = 'node' + str(n)\n",
    "                n += 1\n",
    "        return None\n",
    "    addNodeNames(tree)\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_neildi','J_z_zonalis'))\n",
    "    outgroups = ['J_neildi','J_z_zonalis']\n",
    "    testgroups = ['J_nigrosuffusaTX', 'J_c_grisea', 'J_c_coenia', 'J_nigrosuffusa']\n",
    "    for node in outgroups:\n",
    "        for testnode in testgroups:\n",
    "            dc = {}\n",
    "            dc['chr'] = chromosome\n",
    "            dc['chrN'] = chromosomeN\n",
    "            t2 = tree.copy()\n",
    "            for leaf in t2.iter_leaves():\n",
    "                if leaf.name == node or leaf.name == testnode:\n",
    "                    leaf.delete(preserve_branch_length=True)\n",
    "            #print(t2)\n",
    "            t2.sort_descendants()\n",
    "            sample_orders = []\n",
    "            for n2 in t2.traverse(strategy='levelorder'):\n",
    "                if n2.is_leaf():\n",
    "                    sample_orders.append(n2.name)\n",
    "            sample_orders.reverse()\n",
    "            #print(t2.write(format=9))\n",
    "            #print(calculateT1T2FromFastaAlignment(file_alignment,sample_orders))\n",
    "            T1T2=calculateT1T2FromMultipleFastaAlignment.calculateT1T2FromFastaAlignment(file_alignment, orders = sample_orders,minlen= 1000)\n",
    "            if T1T2 is not None:\n",
    "                T1, T2 = T1T2\n",
    "                dc['T1'] = T1\n",
    "                dc['T2'] = T2\n",
    "                dc['tree'] = t2.write(format=9)\n",
    "            ls_outinfo.append(dc)\n",
    "    return ls_outinfo\n",
    "\n",
    "fraglens = [10000,20000,50000,100000]\n",
    "for fraglen in fraglens:\n",
    "    folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen{fraglen}_tree/'.format(fraglen=fraglen)\n",
    "    folder_fasta = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen{fraglen}/'.format(fraglen=fraglen)\n",
    "    files = open('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen{fraglen}_tree.b75o2.names'.format(fraglen=fraglen)).read().split()\n",
    "    \n",
    "    pool = Pool(32)\n",
    "    results = pool.starmap(processTree,[[f, folder,folder_fasta] for f in files])\n",
    "    pool.close()\n",
    "    df_final = pd.DataFrame([j for i in results for j in i])\n",
    "    df_final.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/fraglen{fraglen}_tree.b75o2.summary'.format(fraglen=fraglen),sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run BPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get sequences\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_coding_100frag -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp \n",
    "\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_noncoding_100frag -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp2 \n",
    "\n",
    "mv /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp2/* /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181106MergeSampleMostCommon/100frag_bpp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to reuse the scripts, copy the files to the same folder of merged samples 6 species\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181112bpp/cmds/cmds'\n",
    "cmds = ['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runBPP_6mergedGenome_fraglen100000.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181112bpp/cmds/cmds'\n",
    "cmds = ['python /home2/s185491/p/xiaolongTools/project/Junonia/runBPP_6mergedGenome_frags100codingNonecoding.py -i '+f for f in files]\n",
    "open(file_cmds,'a').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 12 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze the results\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000/'\n",
    "folders_results = ['/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000BPP%d/'%n for n in [1,2,3]]\n",
    "folder_out = '/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000BPP/'\n",
    "if not os.path.exists(folder_out):\n",
    "    os.makedirs(folder_out)\n",
    "files_input = glob.glob(folder+'*')\n",
    "files_results = []\n",
    "for f in folders_results:\n",
    "     files_results += glob.glob(f+'*')\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_input:\n",
    "     dc_results[os.path.basename(f)] = []\n",
    "for f in files_results:\n",
    "     dc_results[os.path.basename(f)].append(f)\n",
    "\n",
    "Counter([len(e) for e in dc_results.values()])\n",
    "\n",
    "l_parameters = []\n",
    "for k in dc_results:\n",
    "     if len(dc_results[k]) !=0:\n",
    "          l_parameters.append([dc_results[k], 'J_neildi', 0.3, os.path.join(folder_out,k)])\n",
    "\n",
    "# run all\n",
    "pool = Pool(32)\n",
    "results = pool.starmap(bpp.combineBPPresultPartAOne, l_parameters)\n",
    "pool.close()\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000BPP/'\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "def processResultA(file):\n",
    "     '''\n",
    "     return a dc with tree as key and prob as value\n",
    "     '''\n",
    "     dc = {}\n",
    "     basename = os.path.basename(file)\n",
    "     dc['tree_id'] = basename\n",
    "     dc['chr'] = int(basename[3:].split('_')[0])\n",
    "     dc['chrN'] = int(basename[3:].split('_')[1])\n",
    "     for line in open(file):\n",
    "          tree, prob = line.strip().split('\\t')\n",
    "          prob = float(prob)\n",
    "          dc[tree] = prob\n",
    "     return dc\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(processResultA,files)\n",
    "pool.close()\n",
    "df_all = pd.DataFrame(results)\n",
    "df_all = df_all.fillna(0)\n",
    "columns = ['chr', 'chrN', 'tree_id'] + list(df_all.columns)[:-3]\n",
    "df_all = df_all.loc[:,columns]\n",
    "df_all = df_all.sort_values(by = ['chr','chrN'])\n",
    "df_all.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000.bpp.all',sep='\\t',index=None)\n",
    "\n",
    "df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "df_sum = df_sum.drop(columns=['chrN'])\n",
    "df_sum = df_sum.T\n",
    "df_sum['sum'] = df_sum.sum(axis=1)\n",
    "df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "df_sum = df_sum.drop(index=['chr'])\n",
    "df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "df_sum.to_excel('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.xlsx')\n",
    "\n",
    "df_all2 = df_all.copy()\n",
    "trees = list(df_all.columns[3:])\n",
    "#for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "def processLine(row):\n",
    "     dc = {}\n",
    "     dc['chr'] = row['chr']\n",
    "     dc['chrN'] = row['chrN']\n",
    "     dc['tree_id'] = row['tree_id']\n",
    "     row_trees = row[trees]\n",
    "     row_trees = row_trees.sort_values(ascending=False)\n",
    "     dc['max_prob'] = row_trees[0]\n",
    "     dc['max_tree'] = row_trees.index[0]\n",
    "     dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "     return dc\n",
    "\n",
    "l = df_all.apply(processLine, axis=1)\n",
    "l = list(l)\n",
    "df_sum2 = pd.DataFrame(l)\n",
    "df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000.bpp.bestTree',sep='\\t',index=None)\n",
    "\n",
    "df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000.bpp.bestTree.sum',sep='\\t')\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp/'\n",
    "folders_results = ['/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bppBPP%d/'%n for n in [1,2,3]]\n",
    "folder_out = '/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bppBPP/'\n",
    "if not os.path.exists(folder_out):\n",
    "    os.makedirs(folder_out)\n",
    "files_input = glob.glob(folder+'*')\n",
    "files_results = []\n",
    "for f in folders_results:\n",
    "     files_results += glob.glob(f+'*')\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_input:\n",
    "     dc_results[os.path.basename(f)] = []\n",
    "for f in files_results:\n",
    "     dc_results[os.path.basename(f)].append(f)\n",
    "\n",
    "Counter([len(e) for e in dc_results.values()])\n",
    "\n",
    "l_parameters = []\n",
    "for k in dc_results:\n",
    "     l_parameters.append([dc_results[k], 'J_neildi', 0.3, os.path.join(folder_out,k)])\n",
    "\n",
    "# run all\n",
    "pool = Pool(32)\n",
    "results = pool.starmap(bpp.combineBPPresultPartAOne, l_parameters)\n",
    "pool.close()\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/BPP')\n",
    "import bpp\n",
    "import glob,os\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bppBPP/'\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "def processResultA(file):\n",
    "     '''\n",
    "     return a dc with tree as key and prob as value\n",
    "     '''\n",
    "     dc = {}\n",
    "     basename = os.path.basename(file)\n",
    "     dc['tree_id'] = basename\n",
    "     dc['chr'] = int(basename[3:].split('_')[0])\n",
    "     dc['type'] = basename[3:].split('_')[1]\n",
    "     dc['chrN'] = int(basename[3:].split('_')[2])\n",
    "     for line in open(file):\n",
    "          tree, prob = line.strip().split('\\t')\n",
    "          prob = float(prob)\n",
    "          dc[tree] = prob\n",
    "     return dc\n",
    "\n",
    "pool = Pool(32)\n",
    "results = pool.map(processResultA,files)\n",
    "pool.close()\n",
    "df_all = pd.DataFrame(results)\n",
    "df_all = df_all.fillna(0)\n",
    "columns = ['chr', 'chrN', 'tree_id', 'type'] + list(df_all.columns)[:-4]\n",
    "df_all = df_all.loc[:,columns]\n",
    "df_all = df_all.sort_values(by = ['chr','type','chrN'])\n",
    "df_all.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.all',sep='\\t',index=None)\n",
    "\n",
    "df_all_ori = df_all.copy()\n",
    "df_all = df_all_ori.drop(columns=['type'])\n",
    "df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "df_sum = df_sum.drop(columns=['chrN'])\n",
    "df_sum = df_sum.T\n",
    "df_sum['sum'] = df_sum.sum(axis=1)\n",
    "df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "df_sum = df_sum.drop(index=['chr'])\n",
    "df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "df_sum.to_excel('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.xlsx')\n",
    "\n",
    "df_all2 = df_all.copy()\n",
    "trees = list(df_all.columns[3:])\n",
    "#for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "def processLine(row):\n",
    "     dc = {}\n",
    "     dc['chr'] = row['chr']\n",
    "     dc['chrN'] = row['chrN']\n",
    "     dc['tree_id'] = row['tree_id']\n",
    "     row_trees = row[trees]\n",
    "     row_trees = row_trees.sort_values(ascending=False)\n",
    "     dc['max_prob'] = row_trees[0]\n",
    "     dc['max_tree'] = row_trees.index[0]\n",
    "     dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "     return dc\n",
    "\n",
    "l = df_all.apply(processLine, axis=1)\n",
    "l = list(l)\n",
    "df_sum2 = pd.DataFrame(l)\n",
    "df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.bestTree',sep='\\t',index=None)\n",
    "\n",
    "df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.bestTree.sum',sep='\\t')\n",
    "\n",
    "#result for coding\n",
    "types = ['c','n']\n",
    "for t in types:\n",
    "     df_all = df_all_ori[df_all_ori['type']==t].drop(columns=['type'])\n",
    "     df_sum = df_all.groupby(['chr'], as_index=False).sum()\n",
    "     df_sum = df_sum.drop(columns=['chrN'])\n",
    "     df_sum = df_sum.T\n",
    "     df_sum['sum'] = df_sum.sum(axis=1)\n",
    "     df_sum = df_sum.sort_values(by=['sum'],ascending=False)\n",
    "     df_sum = df_sum.drop(index=['chr'])\n",
    "     df_sum.loc['sum_chr'] = df_sum.sum()\n",
    "     df_sum.to_excel('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.{t}.xlsx'.format(t=t))\n",
    "\n",
    "     df_all2 = df_all.copy()\n",
    "     trees = list(df_all.columns[3:])\n",
    "     #for each fragment, get max prob, tree with the max prob, and whether outgroup are together for that tree.\n",
    "     def processLine(row):\n",
    "          dc = {}\n",
    "          dc['chr'] = row['chr']\n",
    "          dc['chrN'] = row['chrN']\n",
    "          dc['tree_id'] = row['tree_id']\n",
    "          row_trees = row[trees]\n",
    "          row_trees = row_trees.sort_values(ascending=False)\n",
    "          dc['max_prob'] = row_trees[0]\n",
    "          dc['max_tree'] = row_trees.index[0]\n",
    "          dc['outTogether'] = row_trees.index[0].endswith('J_z_zonalis),J_neildi);')\n",
    "          return dc\n",
    "\n",
    "     l = df_all.apply(processLine, axis=1)\n",
    "     l = list(l)\n",
    "     df_sum2 = pd.DataFrame(l)\n",
    "     df_sum2.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.bestTree.{t}'.format(t=t),sep='\\t',index=None)\n",
    "\n",
    "     df_sum2good = df_sum2[df_sum2['outTogether'] & (df_sum2['max_prob'] > 0.5)]\n",
    "     df_sum3 = df_sum2good.groupby(['chr','max_tree'], as_index=False)['tree_id'].count()\n",
    "     df_sum4 = df_sum3.pivot(index='max_tree',columns='chr',values='tree_id')\n",
    "     df_sum4 = df_sum4.fillna(0)\n",
    "     df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "     df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "     df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "     df_sum4 = df_sum4.astype(int)\n",
    "     df_sum4.to_csv('/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp.bpp.bestTree.sum.{t}'.format(t=t),sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run MrBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate scripts and run MrBayes\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181112bpp/cmds/cmds'\n",
    "cmds = ['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bpp/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181112bpp/cmds/cmds'\n",
    "cmds = ['python /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'a').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 4 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MrBayes results\n",
    "```\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/work/biophysics/s185491/2018junonia/20181112bpp/100frag_bppNewick/\n",
    "/work/biophysics/s185491/2018junonia/20181112bpp/fraglen100000Newick/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree+'\\n')\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp'\n",
    "basename = 'fraglen100000Newick.sum'\n",
    "filename = os.path.join(folder,basename)\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)]\n",
    "df_tree_b75_O2 = df_tree_b75_O2.reset_index().copy()\n",
    "def getTopology(tree):\n",
    "    tree = tree.copy()\n",
    "    tree.set_outgroup(tree.get_common_ancestor('J_z_zonalis','J_neildi'))\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.xlsx')) #store the tree toplogy count in each chromosome\n",
    "\n",
    "# '100frag_bpp'\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181112bpp'\n",
    "basename = '100frag_bppNewick.sum'\n",
    "filename = os.path.join(folder,basename)\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_type, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['type'] = tree_type\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['outgroupTogether'] = checkIfTogether(tree, 'J_z_zonalis','J_neildi')\n",
    "    dc['tree'] = tree\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','type','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "df_tree = df_tree.drop(columns='index')\n",
    "\n",
    "df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True)].copy()\n",
    "df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "\n",
    "from collections import Counter\n",
    "treeTopology = {}\n",
    "for chromosome in df_tree_b75_O2.chr.unique():\n",
    "    treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "df_treeTopology = pd.DataFrame(treeTopology)\n",
    "df_treeTopology = df_treeTopology.fillna(0)\n",
    "df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.xlsx')) #store the tree toplogy count in each chromosome\n",
    "\n",
    "for tree_type in 'cn':\n",
    "    df_tree_b75_O2 = df_tree[(df_tree['minBoots']>=75) & (df_tree['outgroupTogether']==True) & (df_tree['type'] == tree_type)].copy()\n",
    "    df_tree_b75_O2['treeTopology'] = df_tree_b75_O2['tree'].apply(getTopology)\n",
    "    treeTopology = Counter(df_tree_b75_O2['treeTopology'])\n",
    "    treeTopology = {}\n",
    "    for chromosome in df_tree_b75_O2.chr.unique():\n",
    "        treeTopology['chr'+str(chromosome)] = Counter(df_tree_b75_O2[df_tree_b75_O2['chr']==chromosome]['treeTopology'])\n",
    "    df_treeTopology = pd.DataFrame(treeTopology)\n",
    "    df_treeTopology = df_treeTopology.fillna(0)\n",
    "    df_treeTopology['sum'] = df_treeTopology.sum(axis=1)\n",
    "    df_treeTopology = df_treeTopology.sort_values(by=['sum'],ascending=False)\n",
    "    df_treeTopology.loc['sum_chr'] = df_treeTopology.sum()\n",
    "    df_treeTopology.to_excel(os.path.join(folder,basename[:-4]+'.b75o2.{tree_type}.xlsx'.format(tree_type=tree_type))) #store the tree toplogy count in each chromosome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  20181213 analyze 10 species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The 10 species will be: J_c_coenia J_c_grisea J_neildi J_nigrosuffusa J_nigrosuffusaTX J_z_zonalis J_vestina J_wMex J_neildiTX J_MEXICANspecies\n",
    "\n",
    "1. J_c_coenia ['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "1. J_c_grisea ['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "1. J_neildi ['10385', '5332', '8206', '10350', '4806', '8205', '10396', '4818', '5330', '15112B02', '15101F01', '15117E05', '15117E06', '6652', '5327', '16106C08', '15112A11', '5325', '6650', '6651', '5742', '5750']\n",
    "1. J_nigrosuffusa ['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "1. J_nigrosuffusaTX ['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "1. J_z_zonalis ['8145', '8146', '8143', '10296', '8164', '8148', '8165', '8216', '8214', '10538', '8174', '10249', '7156', '15101E06', '7158', '15117E08', '7157', '7153', '16106C06', '15112B11', '7101', '6657', '7155', '5455', '7103', '7075', '5706', '5487']\n",
    "1. J_vestina ['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "1. J_wMex ['15112A12', '15112B01', '7080', '15117E11', '15117E10', '7076', '7077', '7079']\n",
    "1. J_neildiTX ['15117F02', '5709', '5500', '5387', '5390', '5392', '5437', '6801', '6804', '6817']\n",
    "1. J_MEXICANspecies ['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get merged sequences mostCommon method\n",
    "For each map file, the two strand were used. The output map file is with only one strand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import npMap\n",
    "\n",
    "J_c_coenia=['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "J_c_grisea=['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "J_neildi=['10385', '5332', '8206', '10350', '4806', '8205', '10396', '4818', '5330', '15112B02', '15101F01', '15117E05', '15117E06', '6652', '5327', '16106C08', '15112A11', '5325', '6650', '6651', '5742', '5750']\n",
    "J_nigrosuffusa=['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "J_nigrosuffusaTX=['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "J_z_zonalis=['8145', '8146', '8143', '10296', '8164', '8148', '8165', '8216', '8214', '10538', '8174', '10249', '7156', '15101E06', '7158', '15117E08', '7157', '7153', '16106C06', '15112B11', '7101', '6657', '7155', '5455', '7103', '7075', '5706', '5487']\n",
    "J_vestina=['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "J_wMex=['15112A12', '15112B01', '7080', '15117E11', '15117E10', '7076', '7077', '7079']\n",
    "J_neildiTX=['15117F02', '5709', '5500', '5387', '5390', '5392', '5437', '6801', '6804', '6817']\n",
    "J_MEXICANspecies=['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "\n",
    "dc = {}\n",
    "dc['J_c_coenia'] = J_c_coenia\n",
    "dc['J_c_grisea'] = J_c_grisea\n",
    "dc['J_neildi'] = J_neildi\n",
    "dc['J_nigrosuffusa'] = J_nigrosuffusa\n",
    "dc['J_nigrosuffusaTX'] = J_nigrosuffusaTX\n",
    "dc['J_z_zonalis'] = J_z_zonalis\n",
    "dc['J_vestina'] = J_vestina\n",
    "dc['J_wMex'] = J_wMex\n",
    "dc['J_neildiTX'] = J_neildiTX\n",
    "dc['J_MEXICANspecies'] = J_MEXICANspecies\n",
    "folder_map = '/archive/butterfly/maps/debiased/'\n",
    "ls_speciesMap = list(dc.items())\n",
    "\n",
    "for species, sample_prefix in ls_speciesMap:\n",
    "    files = ['{folder_map}{sample_id}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(folder_map=folder_map, sample_id = sample_id) for sample_id in sample_prefix]\n",
    "    npMap.combineMap(files,outfile='/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMap/{species}.map'.format(species=species), method=2, threads =32, AsOne=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transfer files to BioHPC\n",
    "```\n",
    "scp xcao@alea.swmed.edu:/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMap/* /work/biophysics/s185491/2018junonia/20181203MrBayes10species/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split files to 100kb scan window\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/mapfiles -t 32 -b /home/xcao/w/20180905Junonia_coenia/20180919Info/sitesKeep_2HmelChrOrder_fraglen100000 -o /home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000\n",
    "```\n",
    "```\n",
    "scp xcao@alea.swmed.edu:/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000/* /work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000/\n",
    "```\n",
    "or\n",
    "```\n",
    "python3 /home2/s185491/p/xiaolongTools/utils/getMultipleSequencesFromMapFilesInMultipleSites_small_memory.py -i /work/biophysics/s185491/2018junonia/20181203MrBayes10species/mapfiles -t 32 -b /work/biophysics/s185491/2018junonia/20181203MrBayes10species/sitesKeep_2HmelChrOrder_fraglen100000 -o /work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate scripts run MrBayes For scan window 100kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/cmds/cmds'\n",
    "cmds = ['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'w').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/100frag_bpp/'\n",
    "import glob\n",
    "files = glob.glob(folder+'*')\n",
    "file_cmds = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/cmds/cmds'\n",
    "cmds = ['python /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]\n",
    "open(file_cmds,'a').write('\\n'.join(cmds))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.system('python3 /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -N 32 -i '+file_cmds)\n",
    "\n",
    "file_qusb = ''\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name={n}\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 4 {file_cmds}.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    f = open(file_cmds+'.qsub%d'%n,'w')\n",
    "    f.write(txt_qsub.format(file_cmds=file_cmds,n=n))\n",
    "    f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect data and process result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# combine trees\n",
    "import glob\n",
    "import os\n",
    "folders = '''\n",
    "/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000Newick/\n",
    "'''.split()\n",
    "\n",
    "for folder in folders:\n",
    "    dirname = os.path.dirname(folder)\n",
    "    outfilename = dirname+'.sum'\n",
    "    files = glob.glob(folder +'*')\n",
    "    fout = open(outfilename,'w')\n",
    "    for f in files:\n",
    "        tree = open(f).read()\n",
    "        fout.write(os.path.basename(f) + '\\t' + tree+'\\n')\n",
    "    fout.close()\n",
    "\n",
    "#deal with trees\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    if 'J_vestina' in tree.get_leaf_names():\n",
    "        tree.set_outgroup('J_vestina')\n",
    "    tree.sort_descendants()\n",
    "    dc ={}\n",
    "    dc['tree_id'] = tree_id\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    dc['tree'] = tree.write(format=9)\n",
    "    return dc\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/'\n",
    "basename = 'fraglen100000Newick.sum'\n",
    "filename = os.path.join(folder,basename)\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "df_tree = df_tree.set_index('tree_id')\n",
    "df_tree = df_tree.reset_index()\n",
    "df_tree.to_csv(os.path.join(folder,basename[:-4]+'.all.csv'),sep='\\t')\n",
    "\n",
    "df_sum3 = df_tree.groupby(['chr','tree'], as_index=False)['tree_id'].count()\n",
    "df_sum4 = df_sum3.pivot(index='tree',columns='chr',values='tree_id')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv(os.path.join(folder,basename[:-4]+'.csv'),sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot some trees with R\n",
    "```\n",
    "filename <- '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000Newick.csv'\n",
    "df <- read.csv(filename,sep='\\t',stringsAsFactors = FALSE)\n",
    "library(ggtree)\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000Newick.sometree.pdf',width = 12, height = 6)\n",
    "atree = df[1,]\n",
    "atree.tree = read.newick(text=atree$tree)\n",
    "dd <- data.frame(taxa=atree.tree$tip.label, species = factor(atree.tree$tip.label),group=c('coenia','coenia','coenia','coenia','coenia','wMex','mangrove','mangrove','mangrove','outgroup'))\n",
    "for (n in c(1:80,185,204)){\n",
    "  atree = df[n,]\n",
    "  atree.tree = read.newick(text=atree$tree)\n",
    "  p <- ggtree(atree.tree,branch.length = 'none')+ geom_tiplab(hjust = 1,vjust=-0.5) +coord_cartesian(clip = 'off')\n",
    "  p <- p %<+% dd + geom_tippoint(aes(color=group), size=3) + ggtitle(paste0(\"tree count: \",atree$sum, ' each chr: ',paste(atree[2:23],collapse = ' ')))\n",
    "  print(p)\n",
    "}\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect tree with branch length\n",
    "20181212\n",
    "```\n",
    "file_treeTable = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000Newick.all.csv'\n",
    "file_treeTopology = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000Newick.csv'\n",
    "file_treeAll = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000Newick.sum'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dc_treeID2newick = {}#treeID is like \"chr16_126\", value is tree in newick format\n",
    "for line in open(file_treeAll):\n",
    "    k,v = line.split()\n",
    "    dc_treeID2newick[k] = v\n",
    "\n",
    "df_tree = pd.read_csv(file_treeTable, sep='\\t', index_col=0)\n",
    "df_topology = pd.read_csv(file_treeTopology, sep='\\t', index_col=0)\n",
    "\n",
    "import os\n",
    "from ete3 import Tree\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000NewickSum/individual/'\n",
    "for n,index in enumerate(df_topology.index):\n",
    "    if '(' not in index:#the index is \"sum_chr\", not a tree\n",
    "        continue\n",
    "    d = df_tree[df_tree['tree'] == index]['tree_id']\n",
    "    if d.shape[0] <5:\n",
    "        continue\n",
    "    f = open(os.path.join(outfolder,str(n)),'w')\n",
    "    for t in d:\n",
    "        t = Tree(dc_treeID2newick[t])\n",
    "        t.set_outgroup('J_vestina')\n",
    "        t.sort_descendants()\n",
    "        total_dist = 0\n",
    "        for i in t.traverse():\n",
    "            total_dist += i.dist\n",
    "        for i in t.traverse():\n",
    "            i.dist = i.dist / total_dist\n",
    "        f.write(t.write()+'\\n')\n",
    "    f.close()\n",
    "\n",
    "# sumtrees and change format\n",
    "import dendropy\n",
    "from multiprocessing import Pool\n",
    "def processOneFile(f):\n",
    "    '''\n",
    "    f is files in outfolder, get nex and newick trees\n",
    "    '''\n",
    "    if '.' in f:\n",
    "        return None\n",
    "    file1 = os.path.join(outfolder,f)\n",
    "    commandline = '''/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 {file1} >{file1}.nex'''.format(file1=file1)\n",
    "    os.system(commandline)\n",
    "    mle = dendropy.Tree.get(path=\"{file1}.nex\".format(file1=file1), schema=\"nexus\")\n",
    "    mle.write(path=\"{file1}.new\".format(file1=file1), schema=\"newick\")\n",
    "    \n",
    "\n",
    "files = os.listdir(outfolder)\n",
    "pool = Pool(32)\n",
    "pool.map(processOneFile,files)\n",
    "pool.close()\n",
    "\n",
    "# generate new df_topology, use tree with length as key\n",
    "df_topology_branch = df_topology[df_topology['sum']>=5][:-1].copy()\n",
    "index = []\n",
    "for n,i in enumerate(df_topology_branch.index):\n",
    "    tree = open(os.path.join(outfolder,str(n)+'.new')).read().split()[1]\n",
    "    t = Tree(tree)\n",
    "    #t.unroot()\n",
    "    t.set_outgroup('J_vestina')\n",
    "    t.sort_descendants()\n",
    "    index.append(t.write())\n",
    "df_topology_branch.index = index\n",
    "\n",
    "df_topology_branch.to_csv('/work/biophysics/s185491/2018junonia/20181203MrBayes10species/fraglen100000Newick.sum.branch',sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize result by selecting 5 species or 6 species\n",
    "```\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000Newick.csv'\n",
    "\n",
    "import pandas as pd\n",
    "from ete3 import Tree\n",
    "\n",
    "df = pd.read_csv(filename,sep='\\t', index_col=0)\n",
    "ls_trees = [Tree(e) for e in df.index[:-1]]\n",
    "\n",
    "species5 = 'J_c_coenia J_c_grisea J_nigrosuffusaTX J_nigrosuffusa J_vestina'.split()\n",
    "species6 = 'J_c_coenia J_c_grisea J_nigrosuffusaTX J_nigrosuffusa J_neildi J_z_zonalis'.split()\n",
    "def prune(tree, speciesKeep, outgroup=False):\n",
    "    tree = tree.copy()\n",
    "    tree.prune(speciesKeep)\n",
    "    if outgroup:\n",
    "        tree.set_outgroup(outgroup)\n",
    "    tree.sort_descendants()\n",
    "    return tree\n",
    "\n",
    "ls_trees5 = [prune(tree,species5,outgroup='J_vestina') for tree in ls_trees]\n",
    "ls_trees6 = [prune(tree,species6,outgroup='J_z_zonalis') for tree in ls_trees]\n",
    "\n",
    "df5 = df.iloc[:-1].copy()\n",
    "df5['tree5'] = [t.write(format=9) for t in ls_trees5]\n",
    "df5 = df5.groupby('tree5').sum()\n",
    "df5 = df5.sort_values(by='sum',ascending=False)\n",
    "df5.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000Newick_5species_J_vestina.csv', sep='\\t')\n",
    "\n",
    "df6 = df.iloc[:-1].copy()\n",
    "df6['tree6'] = [t.write(format=9) for t in ls_trees6]\n",
    "df6 = df6.groupby('tree6').sum()\n",
    "df6 = df6.sort_values(by='sum',ascending=False)\n",
    "df6.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181203MergedMostCommon10species/fraglen100000Newick_6species_J_neildi_J_z_zonalis.csv', sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densitree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 301 files\n",
    "#### check existence of files\n",
    "\n",
    "conclusion: '3935' do not have fasta file, but the vcf and map files all exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sequenced genomes is 301\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/xcao/w/20180905Junonia_coenia/junonia_coenia_sequenced_genomes_prefixes.txt'\n",
    "l_junoniaPrefixes = open(filename).read().split()\n",
    "print('total number of sequenced genomes is', len(l_junoniaPrefixes))\n",
    "#convert names like 'NVG-15112B04' to '15112B04'\n",
    "l_junoniaPrefixes = [e if '-' not in e else e.split('-')[-1] for e in l_junoniaPrefixes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of fastq files is 23005\n",
      "number of Junonia samples with fastq is 300\n",
      "the missing file is ['3935']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files_fastqs = glob.glob('/archive/butterfly/ready_fastq/*.*')\n",
    "print('total number of fastq files is',len(files_fastqs))\n",
    "files_fastqs = set(files_fastqs)\n",
    "l_junonia_withfastq = [e for e in l_junoniaPrefixes \\\n",
    "                       if '/archive/butterfly/ready_fastq/'+e+'_R1.fastq' in files_fastqs and \\\n",
    "                      '/archive/butterfly/ready_fastq/'+e+'_R2.fastq' in files_fastqs]\n",
    "print('number of Junonia samples with fastq is', len(l_junonia_withfastq))\n",
    "print('the missing file is', [e for e in l_junoniaPrefixes \\\n",
    "                       if '/archive/butterfly/ready_fastq/'+e+'_R1.fastq' not in files_fastqs or \\\n",
    "                      '/archive/butterfly/ready_fastq/'+e+'_R2.fastq' not in files_fastqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/SNP_results/debiased/7888_Lerema_accius_assembly_V1.1_withMito/', '/archive/butterfly/SNP_results/debiased/7412_Lerema_accius_assembly_V1.1_withMito/']\n",
      "number of vfc folders 7680\n",
      "number of Junonia samples with VFC is 301\n",
      "the following Junonia sample do not have a VFC file against Junonia_coenia_JC_v1.0.scaffolds\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "folders_vfc = glob.glob('/archive/butterfly/SNP_results/debiased/*/')\n",
    "print(folders_vfc[:2])\n",
    "folders_vfc = set(folders_vfc)\n",
    "print('number of vfc folders',len(folders_vfc))\n",
    "l_junoia_withVFC = [e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/SNP_results/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds/' in folders_vfc]\n",
    "print('number of Junonia samples with VFC is', len(l_junoia_withVFC))\n",
    "print('the following Junonia sample do not have a VFC file against Junonia_coenia_JC_v1.0.scaffolds')\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/SNP_results/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds/' not in folders_vfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/archive/butterfly/maps/debiased/3284_pxu_genome_snp_step2.map', '/archive/butterfly/maps/debiased/15109G07_3574_assembly_v1_withMito_snp_step2.map']\n",
      "number of map files 13329\n",
      "number of Junonia samples with Map file is 301\n",
      "the following Junonia sample do not have a map file against Junonia_coenia_JC_v1.0.scaffolds\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files_map = glob.glob('/archive/butterfly/maps/debiased/*.map')\n",
    "print(files_map[:2])\n",
    "files_map = set(files_map)\n",
    "print('number of map files',len(files_map))\n",
    "l_junoia_withMap = ['/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'\\\n",
    "                    for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' in files_map]\n",
    "print('number of Junonia samples with Map file is', len(l_junoia_withMap))\n",
    "print('the following Junonia sample do not have a map file against Junonia_coenia_JC_v1.0.scaffolds')\n",
    "print([e for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' not in files_map])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of 301 .map files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of map files 13329\n",
      "number of Junonia samples with Map file is 301 ['/archive/butterfly/maps/debiased/15102E08_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map', '/archive/butterfly/maps/debiased/5490_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map']\n",
      "number of selected map files 301\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files_map = glob.glob('/archive/butterfly/maps/debiased/*.map')\n",
    "files_map = set(files_map)\n",
    "print('number of map files',len(files_map))\n",
    "l_junoia_withMap = ['/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'\\\n",
    "                    for e in l_junoniaPrefixes \\\n",
    "                   if '/archive/butterfly/maps/debiased/'+e+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' in files_map]\n",
    "print('number of Junonia samples with Map file is', len(l_junoia_withMap),l_junoia_withMap[:2])\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt','w').write('\\n'.join(l_junoia_withMap))\n",
    "print('number of selected map files', len(l_junoia_withMap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count gaps\n",
    "\n",
    "A lot of files need to be re-generated, since the total number of files were changed.\n",
    "\n",
    "```\n",
    "python3 /home/xcao/p/xiaolongTools/utils/countGapEachPositionFromMapFiles.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -o /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileStrand0.GapCounts -t 32 -s 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate site filtered files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genome with different coverage\n",
    "```{python}\n",
    "# genome sites to keep\n",
    "non_gap_ratios = [0.4,0.5,0.6,0.7,0.8,0.90]\n",
    "for non_gap_ratio in non_gap_ratios:\n",
    "    number_map_files = 301\n",
    "    number_map_max = int(number_map_files * (1-non_gap_ratio))\n",
    "    file_GapCounts = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileStrand0.GapCounts'\n",
    "    file_siteKeep = '/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_rmGap_'+str(non_gap_ratio)\n",
    "\n",
    "    fout = open(file_siteKeep,'w')\n",
    "    for n,v in enumerate(open(file_GapCounts)):\n",
    "        if int(v.replace('\\n','')) < number_map_max:\n",
    "            fout.write(str(n)+'\\n')\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDS_noZ, interGene_noZ, intron_noZ, UTR_noZ, Z with different coverage\n",
    "```\n",
    "import os\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919wholeGenomeTree'\n",
    "files_sitesKeep = '''sitesKeep_CDS_noZ\n",
    "sitesKeep_interGene_noZ\n",
    "sitesKeep_intron_noZ\n",
    "sitesKeep_UTR_noZ\n",
    "sitesKeep_Z\n",
    "'''.split()\n",
    "\n",
    "coverages = [0.5,0.8]\n",
    "base_groups = files_sitesKeep\n",
    "\n",
    "for coverage in coverages:\n",
    "    file_rmGap = '/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_rmGap_'+str(coverage)\n",
    "    st_rmGap = set(open(file_rmGap).read().split())\n",
    "    for base_group in base_groups:\n",
    "        file_group = os.path.join(folder,base_group)\n",
    "        outfile_rmGap = '/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_{group}_rmGap{coverage}'.format(group=base_group,coverage=coverage)\n",
    "        fout = open(outfile_rmGap,'w')\n",
    "        for e in open(file_group):\n",
    "            if e.strip('\\n') in st_rmGap:\n",
    "                fout.write(e)\n",
    "        fout.close()\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get fasta sequences\n",
    "\n",
    "run the code generated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_CDS_noZ_rmGap0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_interGene_noZ_rmGap0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_intron_noZ_rmGap0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_UTR_noZ_rmGap0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_Z_rmGap0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_rmGap_0.8 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_rmGap_0.8.fa -t 32 -s 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree'\n",
    "files_sitesKeep = '''sitesKeep_filter_sitesKeep_CDS_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_interGene_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_intron_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_UTR_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_Z_rmGap0.8\n",
    "sitesKeep_rmGap_0.8\n",
    "'''.split()\n",
    "\n",
    "txt = '''python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b {folder}/{filename} -o {folder}/20180928junonia_301_0_{keywords}.fa -t 32 -s 0'''\n",
    "for filename in files_sitesKeep:\n",
    "    keywords = filename.split('_',1)[1].replace('filter_','')\n",
    "    print(txt.format(folder = folder, filename=filename, keywords=keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_CDS_noZ_rmGap0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.5.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_interGene_noZ_rmGap0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.5.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_intron_noZ_rmGap0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.5.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_UTR_noZ_rmGap0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.5.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_filter_sitesKeep_Z_rmGap0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_sitesKeep_Z_rmGap0.5.fa -t 32 -s 0\n",
      "python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/sitesKeep_rmGap_0.5 -o /home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/20180928junonia_301_0_rmGap_0.5.fa -t 32 -s 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree'\n",
    "files_sitesKeep = '''sitesKeep_filter_sitesKeep_CDS_noZ_rmGap0.5\n",
    "sitesKeep_filter_sitesKeep_interGene_noZ_rmGap0.5\n",
    "sitesKeep_filter_sitesKeep_intron_noZ_rmGap0.5\n",
    "sitesKeep_filter_sitesKeep_UTR_noZ_rmGap0.5\n",
    "sitesKeep_filter_sitesKeep_Z_rmGap0.5\n",
    "sitesKeep_rmGap_0.5\n",
    "'''.split()\n",
    "\n",
    "txt = '''python3 /home/xcao/p/xiaolongTools/utils/getSequencesFromMapFilesInSites.py -i /home/xcao/w/20180905Junonia_coenia/20180919Info/20180928mapFileLocations.txt -b {folder}/{filename} -o {folder}/20180928junonia_301_0_{keywords}.fa -t 32 -s 0'''\n",
    "for filename in files_sitesKeep:\n",
    "    keywords = filename.split('_',1)[1].replace('filter_','')\n",
    "    print(txt.format(folder = folder, filename=filename, keywords=keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate PCA input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload files\n",
    "```\n",
    "cd /work/biophysics/s185491/2018junonia/alignments\n",
    "scp xcao@alea.swmed.edu:/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/*rmGAP0.8.fa ./\n",
    "```\n",
    "\n",
    "note:\n",
    "`sshpass -p 'passwd' scp xcao@alea.swmed.edu:/home/xcao/w/20180905Junonia_coenia/20180928wholeGenomeTree/{filename} ./` to save the time of using passwd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.geno -a 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.snp -b 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.ind -o 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.pca -p 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.plot -e 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.eval -l 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.log -snpweightoutname 20180928junonia_301_0_sitesKeep_CDS_noZ_rmGap0.8.fa.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.geno -a 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.snp -b 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.ind -o 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.pca -p 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.plot -e 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.eval -l 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.log -snpweightoutname 20180928junonia_301_0_sitesKeep_interGene_noZ_rmGap0.8.fa.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.geno -a 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.snp -b 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.ind -o 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.pca -p 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.plot -e 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.eval -l 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.log -snpweightoutname 20180928junonia_301_0_sitesKeep_intron_noZ_rmGap0.8.fa.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.geno -a 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.snp -b 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.ind -o 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.pca -p 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.plot -e 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.eval -l 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.log -snpweightoutname 20180928junonia_301_0_sitesKeep_UTR_noZ_rmGap0.8.fa.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.geno -a 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.snp -b 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.ind -o 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.pca -p 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.plot -e 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.eval -l 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.log -snpweightoutname 20180928junonia_301_0_sitesKeep_Z_rmGap0.8.fa.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/20180928junonia_301_0_rmGap_0.8.fa  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20180928junonia_301_0_rmGap_0.8.fa.geno -a 20180928junonia_301_0_rmGap_0.8.fa.snp -b 20180928junonia_301_0_rmGap_0.8.fa.ind -o 20180928junonia_301_0_rmGap_0.8.fa.pca -p 20180928junonia_301_0_rmGap_0.8.fa.plot -e 20180928junonia_301_0_rmGap_0.8.fa.eval -l 20180928junonia_301_0_rmGap_0.8.fa.log -snpweightoutname 20180928junonia_301_0_rmGap_0.8.fa.snp_weights.txt &\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "cd /work/biophysics/s185491/2018junonia/20180928PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/biophysics/s185491/2018junonia/alignments/{filename}  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i {filename}.geno -a {filename}.snp -b {filename}.ind -o {filename}.pca -p {filename}.plot -e {filename}.eval -l {filename}.log -snpweightoutname {filename}.snp_weights.txt &\n",
    "'''\n",
    "files_sitesKeep = '''sitesKeep_filter_sitesKeep_CDS_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_interGene_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_intron_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_UTR_noZ_rmGap0.8\n",
    "sitesKeep_filter_sitesKeep_Z_rmGap0.8\n",
    "sitesKeep_rmGap_0.8\n",
    "'''.split()\n",
    "for _f in files_sitesKeep:\n",
    "    keywords = _f.split('_',1)[1].replace('filter_','')\n",
    "    filename = '20180928junonia_301_0_{keywords}.fa'.format(keywords=keywords)\n",
    "    print(txt.format(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  High quality 263 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run PCA with the code below.\n",
    "Don't forget the code below\n",
    "\n",
    "```\n",
    "export PATH=$PATH:/home2/wli/local/EIG6.1.1/bin;\n",
    "module add gsl/1.15;\n",
    "module add openblas/intel/0.2.14;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cd /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181011PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_CDS  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20181010junoniaSeq_CDS.geno -a 20181010junoniaSeq_CDS.snp -b 20181010junoniaSeq_CDS.ind -o 20181010junoniaSeq_CDS.pca -p 20181010junoniaSeq_CDS.plot -e 20181010junoniaSeq_CDS.eval -l 20181010junoniaSeq_CDS.log -snpweightoutname 20181010junoniaSeq_CDS.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181011PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_whole_max30Gap  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20181010junoniaSeq_whole_max30Gap.geno -a 20181010junoniaSeq_whole_max30Gap.snp -b 20181010junoniaSeq_whole_max30Gap.ind -o 20181010junoniaSeq_whole_max30Gap.pca -p 20181010junoniaSeq_whole_max30Gap.plot -e 20181010junoniaSeq_whole_max30Gap.eval -l 20181010junoniaSeq_whole_max30Gap.log -snpweightoutname 20181010junoniaSeq_whole_max30Gap.snp_weights.txt &\n",
      "\n",
      "\n",
      "cd /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181011PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181010junoniaSeq_z  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i 20181010junoniaSeq_z.geno -a 20181010junoniaSeq_z.snp -b 20181010junoniaSeq_z.ind -o 20181010junoniaSeq_z.pca -p 20181010junoniaSeq_z.plot -e 20181010junoniaSeq_z.eval -l 20181010junoniaSeq_z.log -snpweightoutname 20181010junoniaSeq_z.snp_weights.txt &\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "cd /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/20181011PCA/  &&  python /home2/s185491/p/xiaolongTools/WenlinTools.Python3/scripts/fasta2EIGinput.py /work/archive/biophysics/Nick_lab/shared/Xiaolong/20181010Tree/{filename}  &&  /home2/wli/local/EIG6.1.1/bin/smartpca.pl -i {filename}.geno -a {filename}.snp -b {filename}.ind -o {filename}.pca -p {filename}.plot -e {filename}.eval -l {filename}.log -snpweightoutname {filename}.snp_weights.txt &\n",
    "'''\n",
    "files_sitesKeep = '''20181010junoniaSeq_CDS\n",
    "20181010junoniaSeq_whole_max30Gap\n",
    "20181010junoniaSeq_z\n",
    "'''.split()\n",
    "for _f in files_sitesKeep:\n",
    "    print(txt.format(filename=_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot with R\n",
    "```\n",
    "filename <- 'C:\\\\Users\\\\K\\\\OneDrive\\\\Lab\\\\UTSW\\\\2018JunoniaProject_Xiaolong\\\\summary\\\\20181003Junonia-all-sample-summary.xlsx'\n",
    "\n",
    "mydata1 <- read_excel(filename,sheet = 'all')\n",
    "PCAs = c('PCA_263_CDS_PC1',\n",
    "         'PCA_263_CDS_PC2',\n",
    "         'PCA_263_CDS_PC3',\n",
    "         'PCA_263_whole_PC1',\n",
    "         'PCA_263_whole_PC2',\n",
    "         'PCA_263_whole_PC3',\n",
    "         'PCA_263_z_PC1',\n",
    "         'PCA_263_z_PC2',\n",
    "         'PCA_263_z_PC3')\n",
    "group_titles = c('Filter_7USA', 'Filter4groups_withMX', 'Filter_4groups','Filter_keep_all_available')\n",
    "\n",
    "pdf('C:\\\\Users\\\\K\\\\OneDrive\\\\Lab\\\\UTSW\\\\2018JunoniaProject_Xiaolong\\\\20180922PCA\\\\20181016PCA_3baseTypes_4Groups.pdf',width = 30, height = 18)\n",
    "\n",
    "for (group_title in group_titles){\n",
    "    mydata <- mydata1[mydata1[,group_title] == 1,]\n",
    "    for (n in 1:(length(PCAs)/3)){\n",
    "        p <- ggplot(mydata, aes_string(x=PCAs[3*n-2],y=PCAs[3*n-1])) + geom_point(aes(color=Species, size = 10))  + geom_label_repel(aes(label=Number),label.size=NA,fill=NA) + ggtitle(group_title)\n",
    "        print(p)\n",
    "    }\n",
    "}\n",
    "\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introgression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J_vestina as outgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ABBA/BABA test\n",
    "combine ABBA with RAxML. Calculate all possible topologies with three species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate D-statistics\n",
    "```\n",
    "import glob\n",
    "import os,sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/seq')\n",
    "import ABBA_BABA\n",
    "import itertools\n",
    "import pandas as pd\n",
    "#calculate D for all possible permutations\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina/'\n",
    "files = glob.glob(folder+'*')\n",
    "workfolder = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181126Introgression'\n",
    "outfile = os.path.join(workfolder,'20181126fastaFilesLoc.J_vestina')\n",
    "open(outfile,'w').write('\\n'.join(files))\n",
    "ls_species = 'J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_vestina'.split()\n",
    "file_results_pos = []\n",
    "file_results = []\n",
    "for ss in itertools.permutations(ls_species[:-1],3):\n",
    "    ls_orders = list(ss) + ['J_vestina']\n",
    "    s = '.'.join(ls_orders)\n",
    "    output = os.path.join(workfolder,'20181126Jvestina_D_4species.'+s)\n",
    "    file_results_pos.append(s)\n",
    "    file_results.append(output)\n",
    "    ABBA_BABA.calculate_Dstatistics_FromMultipleFastaAlignment(outfile, ls_orders, minlen = 5000, threads = 32, output=output)\n",
    "\n",
    "#collect data\n",
    "fastafiles = open(os.path.join(workfolder,'20181126fastaFilesLoc.J_vestina')).readlines()\n",
    "filenames = [os.path.basename(e.strip()) for e in fastafiles]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['chr_ID'] = filenames\n",
    "df['chr'] = df['chr_ID'].apply(lambda x:int(x[3:].split('_')[0]))\n",
    "df['chrN'] = df['chr_ID'].apply(lambda x:int(x[3:].split('_')[1]))\n",
    "df = df.set_index('chr_ID')\n",
    "\n",
    "ls_species = 'J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX'.split()\n",
    "for s, file_D in zip(file_results_pos,file_results):\n",
    "    df[s] = 0\n",
    "    for line in open(file_D):\n",
    "        chr_ID, D = line.strip().split(':')\n",
    "        chr_ID = os.path.basename(chr_ID)\n",
    "        D = float(D)\n",
    "        df.loc[chr_ID,s] = D\n",
    "\n",
    "df = df.sort_values(by=['chr','chrN'])\n",
    "df = df.reset_index()\n",
    "df.to_csv(os.path.join(workfolder,'20181126Jvestina_D_4species.csv'), sep='\\t')\n",
    "\n",
    "df2 = pd.read_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree.all',sep='\\t',index_col=0)\n",
    "df2 = df2.set_index('chr_ID')\n",
    "df2 = df2.drop(['chr', 'chrN', 'minBoots'],axis=1)\n",
    "\n",
    "df3 = df.join(df2,on='chr_ID')\n",
    "df3.to_csv(os.path.join(workfolder,'20181126Jvestina_D_4species.csv'), sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAxML result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree.sum'\n",
    "ls_species = 'J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX'.split()\n",
    "outgroup = 'J_vestina'\n",
    "\n",
    "def getlowestBootstrap(tree):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "def checkOneNode(tree,name1,name2):\n",
    "    children = tree.children\n",
    "    if len(children) !=2:\n",
    "        print('children number not 2',tree)\n",
    "        return False\n",
    "    c1,c2 = children\n",
    "    if c1.name == name1 and c2.name == name2:\n",
    "        return True\n",
    "    if c1.name == name2 and c2.name == name1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkIfTogether(tree, name1,name2):\n",
    "    '''\n",
    "    check if name1 and name2 forms a group in tree\n",
    "    '''\n",
    "    tree = tree.copy()\n",
    "\n",
    "    for t in tree.iter_descendants():\n",
    "        if not t.is_leaf():\n",
    "            t2 = tree.copy()\n",
    "            t2.sort_descendants()\n",
    "            t2.set_outgroup(t.name)\n",
    "            #print(t.name, t2)\n",
    "            for c in t2.children:\n",
    "                if len(c.get_children()) == 2:\n",
    "                    if checkOneNode(c,name1,name2):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def processTXT(txt):\n",
    "    tree_id, tree_txt = txt.split()\n",
    "    tree_chr, tree_chrN = tree_id.split('_')\n",
    "    tree = Tree(tree_txt)\n",
    "    tree.unroot()\n",
    "    node_name = 1#add name for all trees\n",
    "    for t in tree.traverse():\n",
    "        if t.name == '':\n",
    "            t.name = 'node'+str(node_name)\n",
    "            node_name += 1\n",
    "    dc ={}\n",
    "    dc['chr_ID'] = tree_id\n",
    "    dc['chr'] = int(tree_chr[3:])\n",
    "    dc['chrN'] = int(tree_chrN)\n",
    "    dc['minBoots'] = getlowestBootstrap(tree)\n",
    "    for ss in itertools.permutations(ls_species,3):\n",
    "        ls_orders = list(ss) + ['J_vestina']\n",
    "        s = '.'.join(ls_orders)\n",
    "        tree_temp = tree.copy()\n",
    "        for leaf in tree_temp.iter_leaves():\n",
    "            if leaf.name not in ls_orders:\n",
    "                leaf.delete()\n",
    "        dc[s+'.top'] = checkIfTogether(tree_temp,ss[0],ss[1])\n",
    "        #print(tree_temp,ss[0],ss[1])\n",
    "    tree.set_outgroup('J_vestina')\n",
    "    tree.sort_descendants()\n",
    "    dc['tree'] = tree.write(format=9)\n",
    "    return dc\n",
    "\n",
    "txts = open(filename).readlines()\n",
    "pool = Pool(32)\n",
    "df_tree = pd.DataFrame(pool.map(processTXT,txts))\n",
    "pool.close()\n",
    "\n",
    "df_tree = df_tree.set_index('chr_ID')\n",
    "df_tree = df_tree.sort_values(by=['chr','chrN'])\n",
    "df_tree = df_tree.reset_index()\n",
    "columns = ['chr_ID','chr','chrN', 'minBoots','tree']\n",
    "columns_top = [e for e in df_tree.columns if e not in columns]\n",
    "for c in columns_top:\n",
    "    df_tree[c] = df_tree[c].astype(int)\n",
    "df_tree = df_tree.loc[:,columns+columns_top]\n",
    "df_tree.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree.all',sep='\\t')\n",
    "\n",
    "df_sum3 = df_tree.groupby(['chr','tree'], as_index=False)['chr_ID'].count()\n",
    "df_sum4 = df_sum3.pivot(index='tree',columns='chr',values='chr_ID')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree.bestTree.sum',sep='\\t')\n",
    "df_sum3 = df_tree[df_tree['minBoots']>80].groupby(['chr','tree'], as_index=False)['chr_ID'].count()\n",
    "df_sum4 = df_sum3.pivot(index='tree',columns='chr',values='chr_ID')\n",
    "df_sum4 = df_sum4.fillna(0)\n",
    "df_sum4['sum'] = df_sum4.sum(axis=1)\n",
    "df_sum4 = df_sum4.sort_values(by=['sum'],ascending=False)\n",
    "df_sum4.loc['sum_chr'] = df_sum4.sum()\n",
    "df_sum4 = df_sum4.astype(int)\n",
    "df_sum4.to_csv('/home/xcao/w/20180905Junonia_coenia/20181010Trees/20181121MergedSample/fraglen100000_J_vestina5_tree.bestTree.minBoots80.sum',sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine ABBA with RAxML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new analysis 20190131\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify types for Junonia by making CDS trees of whole genome and zChr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get fasta alignments 20190131\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import glob\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "files = glob.glob(folder+'*')\n",
    "print('total map npInt8 files', len(files))\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/Junonia324.zChr'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilter'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/Junonia324.zChrCDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilterCDS'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/Junonia324.CDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gff.0inter.2intron.6CDS.14UTR'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=lambda x:x==6, merged=True)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep nongap ratio 0.8 for CDS and zChrCDS\n",
    "```\n",
    "cd /home/xcao/w/20180905Junonia_coenia/20190131New/phylo && python /home/xcao/p/xiaolongTools/utils/fastaAlignmentFilter.py -i Junonia324.CDS -o Junonia324.CDS.filter0.8 -t 32 -b 0 -g 0.8 -B 0\n",
    "cd /home/xcao/w/20180905Junonia_coenia/20190131New/phylo && python /home/xcao/p/xiaolongTools/utils/fastaAlignmentFilter.py -i Junonia324.zChrCDS -o Junonia324.zChrCDS.filter0.8 -t 32 -b 0 -g 0.8 -B 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run ExaML\n",
    "for zChrCDS, run RAxML with test  \n",
    "for CDS, split to 30 parts and run ExaML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split sequence\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -m 1 -N 30 -i /home2/s185491/w/2018junonia/20190131ExaML/seqs/Junonia324.CDS.filter0.8 -o /home2/s185491/w/2018junonia/20190131ExaML/seqs/\n",
    "```\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=zCDS\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "cd /work/biophysics/s185491/2018junonia/20190131ExaML/seqs\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-PTHREADS-AVX -m GTRGAMMA -p 234 -s Junonia324.zChrCDS.filter0.8 -n Junonia324.zChrCDS.filter0.8 -x 234 -N 100 -f a -T 32\n",
    "```\n",
    "\n",
    "```\n",
    "txt = '''#!/bin/bash\n",
    "#SBATCH --job-name=zCDS\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "cd /work/biophysics/s185491/2018junonia/20190131ExaML/seqs\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-PTHREADS-AVX -m GTRGAMMA -p 234 -s Junonia324.CDS.filter0.8.split30.{N} -n Junonia324.CDS.filter0.8.split30.{N} -T 32 -t /work/biophysics/s185491/2018junonia/20180921ExaML/ExaML_result.junonia_whole_genome_0_filter_CDS_noZ_rmGap0.85.ExaML'''\n",
    "\n",
    "folder_cmds = '/work/biophysics/s185491/2018junonia/20190131ExaML/cmds/'\n",
    "for N in range(30):\n",
    "    fout = open(folder_cmds+'qsub_junonia.'+str(N),'w')\n",
    "    fout.write(txt.format(N=N))\n",
    "    fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cat /work/biophysics/s185491/2018junonia/20190131ExaML/seqs/RAxML_bestTree.Junonia324.CDS.filter0.8.split30.* >/work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.all\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.all >/work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.sum\n",
    " python /home2/s185491/p/xiaolongTools/utils/trees/sumtreesNEXUS2newick.py -i /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.sum\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.sum.nwk\n",
    "\n",
    "java -Xmx3000m -jar /home2/s185491/p/Astral/astral.5.6.2.jar -i /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.all -o /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.astral -t 2 \n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py /work/biophysics/s185491/2018junonia/20190131ExaML/Junonia324.CDS.astral\n",
    "\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py /work/biophysics/s185491/2018junonia/20190131ExaML/zChr/ExaML_result.Junonia324.zChrCDS.filter0.8\n",
    "\n",
    "#zChrCDS, combine bootstrap with tree\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z RAxML_bootstrap.Junonia324.zChrCDS.filter0.8 -t RAxML_bestTree.ExaML_result.Junonia324.zChrCDS.filter0.8.boots -m GTRGAMMA -n ExaML_result.Junonia324.zChrCDS.filter0.8.B\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py /work/biophysics/s185491/2018junonia/20190131ExaML/zChr/RAxML_bipartitions.ExaML_result.Junonia324.zChrCDS.filter0.8.B\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find z chromosome based on coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_summary = '/home/xcao/w/20180905Junonia_coenia/20180919Info/20190130Junonia-all-sample-summary.xlsx'\n",
    "df_summary = pd.read_excel(file_summary,sheet_name='all')\n",
    "df_summary['Number'] = df_summary['Number'].astype(str)\n",
    "sample_prefix = [e.split('-')[-1] for e in df_summary['Number']]\n",
    "print(len(sample_prefix))\n",
    "files_bam = ['/archive/butterfly/SNP_results/debiased/{sample}_Junonia_coenia_JC_v1.0.scaffolds/realigned_reads_step2.bam'.format(sample=e) for e in sample_prefix]\n",
    "files_bam = [e for e in files_bam if os.path.exists(e)]\n",
    "\n",
    "workfolder = '/home/xcao/w/20180905Junonia_coenia/20190131New/zChr/bam/'\n",
    "def processBam(filename):\n",
    "    outname = os.path.basename(os.path.dirname(filename))\n",
    "    cmd = '''cd {workfolder} &&\\\n",
    "    ln -s {filename} {outname}.bam &&\\\n",
    "    samtools index {outname}.bam &&\\\n",
    "    samtools idxstats {outname}.bam >{outname}.idxstats\n",
    "    '''.format(workfolder=workfolder,outname=outname,filename=filename)\n",
    "    os.system(cmd)\n",
    "    \n",
    "pool = Pool(32)\n",
    "pool.map(processBam, files_bam)\n",
    "pool.close()\n",
    "\n",
    "files_idxstats = glob.glob(workfolder+'*.idxstats')\n",
    "def readIdxstats(filename):\n",
    "    '''\n",
    "    return a dataframe\n",
    "    '''\n",
    "    df = pd.read_csv(filename,sep='\\t',dtype=str, header=None)\n",
    "    df = df.iloc[:-1,:-1]\n",
    "    df.columns = ['scf','scf_len','reads']\n",
    "    df['reads'] = df['reads'].astype(int)\n",
    "    df['scf_len'] = df['scf_len'].astype(int)\n",
    "    return df\n",
    "\n",
    "dc_df_idxstats = {os.path.basename(f).split('_')[0]: readIdxstats(f) for f in files_idxstats}\n",
    "#get median number of total reads\n",
    "median_reads_count = np.median([df['reads'].sum() for df in dc_df_idxstats.values()])\n",
    "#get samples with reads count greater than median_reads_count\n",
    "ls_sample_good = [e for e in dc_df_idxstats if dc_df_idxstats[e]['reads'].sum() >= median_reads_count]\n",
    "#get depth for each scf in each sample\n",
    "df_depth = dc_df_idxstats[ls_sample_good[0]].copy()\n",
    "df_depth = df_depth[['scf','scf_len']]\n",
    "for key in ls_sample_good:\n",
    "    df = dc_df_idxstats[key]\n",
    "    average_depth = df['reads'].sum()/df['scf_len'].sum()\n",
    "    df_depth[key] = df['reads'] / df['scf_len'] / average_depth\n",
    "\n",
    "samples_female = [e.split('-')[-1] for e in df_summary[df_summary['Sex'] == 'F']['Number'] if '.' not in e]#keep only female libaries\n",
    "samples_male = [e.split('-')[-1] for e in df_summary[df_summary['Sex'] == 'M']['Number'] if '.' not in e]#keep only male libaries\n",
    "\n",
    "df_depth['female_average'] = df_depth[[e for e in samples_female if e in ls_sample_good]].mean(axis=1)\n",
    "df_depth['male_average'] = df_depth[[e for e in samples_male if e in ls_sample_good]].mean(axis=1)\n",
    "def getSex(t):\n",
    "    if t in samples_female:\n",
    "        return 'female'\n",
    "    if t in samples_male:\n",
    "        return 'male'\n",
    "    return None\n",
    "df_depth.loc['sex'] = [getSex(t) for t in df_depth.columns]\n",
    "\n",
    "df_depth.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/zChr/20190201junonia_scf_depth.xlsx')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysizing new hybrids 20190214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences\n",
    "99 samples\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import itertools\n",
    "import mapfileIO\n",
    "\n",
    "J_c_coenia=['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "J_c_grisea=['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "J_nigrosuffusa=['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "J_nigrosuffusaTX=['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "J_MEXICANspecies=['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "J_vestina=['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "J_new = ['18098H06', '18098H07', '18098H08', '18098H09', '18098H10', '18098H11', '18098H12', '18126E01', '18126E02', '18126E03', '18126E04', '18126E05', '18126E06', '18126E07', '18126E08', '18126E09']\n",
    "\n",
    "dc = {}\n",
    "dc['J_c_coenia'] = J_c_coenia\n",
    "dc['J_c_grisea'] = J_c_grisea\n",
    "dc['J_nigrosuffusa'] = J_nigrosuffusa\n",
    "dc['J_nigrosuffusaTX'] = J_nigrosuffusaTX\n",
    "dc['J_vestina'] = J_vestina\n",
    "dc['J_MEXICANspecies'] = J_MEXICANspecies\n",
    "dc['J_new'] = J_new\n",
    "samples = list(itertools.chain.from_iterable(dc.values()))\n",
    "\n",
    "#get npMap files to work with\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "files = [folder + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "print('total map npInt8 files', len(files))\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid99.zChr'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilter'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid99.zChrCDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilterCDS'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid99.CDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gff.0inter.2intron.6CDS.14UTR'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=lambda x:x==6, merged=True)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get trees\n",
    "transfer\n",
    "```\n",
    "scp /home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid99.* s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/20190214ExaML/seqs/\n",
    "```\n",
    "\n",
    "run with full sequences.\n",
    "Also, split files to add bootstrap values for full sequence trees\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.CDS -N 100\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChr -N 100\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChrCDS -N 5\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split/ -m 2 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChrCDS -N 5\n",
    "\n",
    "```\n",
    "run for each individual\n",
    "\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split/'\n",
    "folder_cmd = '/work/biophysics/s185491/2018junonia/20190214ExaML/cmds/'\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name=zChr\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/python3 /home2/s185491/p/xiaolongTools/utils/runExaMLForIndividualSmallFiles.py -i {filename} -T 32 -r /work/biophysics/s185491/2018junonia/20190214ExaML/JunoniaHybrid99.zChrCDS.NJ\n",
    "'''\n",
    "\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    open(os.path.join(folder_cmd,'qsub_individual_'+basename),'w').write(txt_qsub.format(filename = f))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine trees and rename trees\n",
    "```\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split.ExaMLbestTree/JunoniaHybrid99.CDS.split100.* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.CDS.splitTrees\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split.ExaMLbestTree/JunoniaHybrid99.zChr.split100.* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChr.splitTrees\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/split.ExaMLbestTree/JunoniaHybrid99.zChrCDS* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChrCDS.splitTrees\n",
    "\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid99.CDS.splitTrees -t ExaML_result.JunoniaHybrid99.CDS -m GTRGAMMA -n JunoniaHybrid99.CDS.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid99.CDS.boots\n",
    "\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid99.zChr.splitTrees -t ExaML_result.JunoniaHybrid99.zChr -m GTRGAMMA -n JunoniaHybrid99.zChr.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid99.zChr.boots\n",
    "\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid99.zChrCDS.splitTrees -t ExaML_result.JunoniaHybrid99.zChrCDS -m GTRGAMMA -n JunoniaHybrid99.zChrCDS.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid99.zChrCDS.boots\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 JunoniaHybrid99.CDS.splitTrees >JunoniaHybrid99.CDS.splitTrees.sum\n",
    "python /home2/s185491/p/xiaolongTools/utils/trees/sumtreesNEXUS2newick.py -i JunoniaHybrid99.CDS.splitTrees.sum\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py JunoniaHybrid99.CDS.splitTrees.sum.nwk\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 JunoniaHybrid99.zChr.splitTrees >JunoniaHybrid99.zChr.splitTrees.sum\n",
    "python /home2/s185491/p/xiaolongTools/utils/trees/sumtreesNEXUS2newick.py -i JunoniaHybrid99.zChr.splitTrees.sum\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py JunoniaHybrid99.zChr.splitTrees.sum.nwk\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/sumtrees.py -f 0.00 JunoniaHybrid99.zChrCDS.splitTrees >JunoniaHybrid99.zChrCDS.splitTrees.sum\n",
    "python /home2/s185491/p/xiaolongTools/utils/trees/sumtreesNEXUS2newick.py -i JunoniaHybrid99.zChrCDS.splitTrees.sum\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py JunoniaHybrid99.zChrCDS.splitTrees.sum.nwk\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences for structure\n",
    "remove J_vestina\n",
    "```\n",
    "files = '''/work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.CDS\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChr\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seqs/JunoniaHybrid99.zChrCDS\n",
    "'''.split()\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/'\n",
    "\n",
    "J_vestina=['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "def cleanSample(filename):\n",
    "    '''\n",
    "    remove J_vestina\n",
    "    '''\n",
    "    basename = os.path.basename(filename)\n",
    "    basename = basename.replace('99','93')\n",
    "    outfile = os.path.join(outfolder, basename)\n",
    "    fout = open(outfile,'w')\n",
    "    for s in SeqIO.parse(filename,'fasta'):\n",
    "        if s.id not in J_vestina:\n",
    "            fout.write('>'+s.id+'\\n'+str(s.seq)+'\\n')\n",
    "    fout.close()\n",
    "    return None\n",
    "\n",
    "for filename in files:\n",
    "    cleanSample(filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run structure\n",
    "split file\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.CDS -N 10\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 32 -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.zChr -N 10\n",
    "\n",
    "```\n",
    "\n",
    "run\n",
    "```\n",
    "files = '''\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.CDS.split10.0\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.CDS.split10.1\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.CDS.split10.2\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.zChr.split10.0\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.zChr.split10.1\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.zChr.split10.2\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/JunoniaHybrid93.zChrCDS\n",
    "'''.split()\n",
    "\n",
    "\n",
    "commandline = 'python /home2/s185491/p/xiaolongTools/utils/STRUCTURE/fasta2STRUCTUREinput_haploid_nt.py -i {file_in} -o {file_out} -t 32 -g {gap} -N 4 -H True'\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/'\n",
    "import os\n",
    "\n",
    "for file in files:\n",
    "    gap = 0.8\n",
    "    os.system(commandline.format(file_in = file,gap=gap, file_out = os.path.join(outfolder, os.path.basename(file))))\n",
    "    \n",
    "    \n",
    "    \n",
    "files = '''\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.CDS.split10.0\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.CDS.split10.1\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.CDS.split10.2\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.zChr.split10.0\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.zChr.split10.1\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.zChr.split10.2\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/structure/JunoniaHybrid93.zChrCDS\n",
    "'''.split()\n",
    "\n",
    "snps = '''\n",
    "111281\n",
    "110636\n",
    "111171\n",
    "74537\n",
    "75077\n",
    "74819\n",
    "34605\n",
    "'''.split()\n",
    "\n",
    "commandline = 'python /home2/s185491/p/xiaolongTools/utils/STRUCTURE/runSTRUCTUREwithSTRUCTUREinput.py -i {file_in} -I 93 -L {snp_len} -s 1000 -P 2,3,4,5,6,7 -o /work/biophysics/s185491/2018junonia/20190214ExaML/cmds/structrue_jobs'\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/'\n",
    "\n",
    "for file_in, snp_len in zip(files,snps):\n",
    "    os.system(commandline.format(file_in=file_in, snp_len=snp_len))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect structure data\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/STRUCTURE')\n",
    "import extractStructureTableFromCommandlineOutput\n",
    "folder_structure = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/'\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/results/'\n",
    "files_structure = glob.glob(folder_structure +'runSTRUCTURE*/*_f')\n",
    "for f in files_structure:\n",
    "    key = os.path.basename(f)\n",
    "    outfile = os.path.join(outfolder, key)\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_structure:\n",
    "    key = os.path.basename(f)\n",
    "    outfile = os.path.join(outfolder, key)\n",
    "    dc_results[key] = extractStructureTableFromCommandlineOutput.extractStructureTableFromCommandlineOutput(filename=f, output=outfile, processLineForR=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot structure\n",
    "```\n",
    "library(readxl)\n",
    "library(tidyr)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "\n",
    "folder_files <- '/home/xcao/w/20180905Junonia_coenia/20190131New/structure/results/'\n",
    "result_files <- list.files(folder_files,full.names = FALSE)\n",
    "basetypes <- c(\"zChrCDS\",\"CDS\",\"zChr\")\n",
    "\n",
    "f.getbasetype <- function(x){\n",
    "  for (keyword in basetypes){\n",
    "    if (grepl(keyword,x)) return(keyword)\n",
    "  }\n",
    "}\n",
    "\n",
    "f.getSplitgroup <- function(x){\n",
    "  if (grepl('.split',x)) {\n",
    "    return(as.integer(gsub(\"(.*split\\\\d*\\\\.)|(\\\\.STRUCTUREresult.*)\",\"\",x)))\n",
    "  } else return(0)\n",
    "}\n",
    "\n",
    "f.getRandint <- function(x){\n",
    "  #x=\"Calephelis266.zChr.split5.4.STRUCTUREresult.s1000.pop8_f\" return 1000\n",
    "  return(as.integer(gsub('(.*STRUCTUREresult.s)|(.pop.*_f)','',x)))\n",
    "}\n",
    "\n",
    "f.getPopulation <- function(x) {\n",
    "  return(as.integer(gsub('(.*STRUCTUREresult.s\\\\d*.pop)|(_f)','',x)))\n",
    "}\n",
    "\n",
    "df_files <- data.frame(baseType=sapply(result_files, f.getbasetype),\n",
    "                       splitID=sapply(result_files, f.getSplitgroup),\n",
    "                       seed=sapply(result_files, f.getRandint),\n",
    "                       population=sapply(result_files,f.getPopulation),\n",
    "                       filename=file.path(folder_files,result_files), stringsAsFactors = FALSE)\n",
    "\n",
    "df_files <- df_files[order(df_files$baseType, df_files$population,df_files$seed,df_files$splitID),]\n",
    "\n",
    "F.renameHead <- function(x){\n",
    "  x[1] = 'sample'\n",
    "  x[2:length(x)] = paste0('pop',1:(length(x)-1))\n",
    "  return(x)\n",
    "}\n",
    "\n",
    "filename2 <- '/home/xcao/w/20180905Junonia_coenia/20180919Info/20190130Junonia-all-sample-summary.xlsx'\n",
    "xdf_info <- read_excel(filename2,sheet = 'Sheet1')\n",
    "# xdf_info <- xdf_info[!is.na(xdf_info$order_266mitoTree),]\n",
    "# v_id2Treename <- xdf_info$Treename\n",
    "# names(v_id2Treename) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "v_id2species <- xdf_info$`Taxon name`\n",
    "names(v_id2species) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "species_in_order <- xdf_info[order(xdf_info$species),]$ID\n",
    "species_in_order <- substr(species_in_order,2,length(species_in_order))\n",
    "species_in_order <- species_in_order[species_in_order %in% xdf_s$sample]\n",
    "\n",
    "\n",
    "oneline = df_files[1,]\n",
    "xdf_s <- read.csv(oneline$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "#xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species)\n",
    "xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "\n",
    "p <- ggplot(xdf_s.clear,aes(x=sample,y=fraction,fill=pop)) + geom_bar(stat = 'identity') + scale_x_discrete(limits=species_in_order)\n",
    "\n",
    "\n",
    "\n",
    "F.plotOneline <- function(oneline,xtext=FALSE){\n",
    "  xdf_s <- read.csv(oneline$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "  colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "  xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "  #xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "  xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species)\n",
    "  xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "  \n",
    "  p <- ggplot(xdf_s.clear,aes(x=sample,y=fraction,fill=pop)) + geom_bar(stat = 'identity') + scale_x_discrete(limits=species_in_order[species_in_order %in% xdf_s$sample]) + ylab(paste0('seed',oneline$seed,'_',oneline$baseType,'_K=',oneline$population,\" \",oneline$splitID))\n",
    "  if (xtext) {\n",
    "    p <- p + geom_text(aes(x=sample,y=0,label=paste(sample,species)),angle=90,hjust=0)+ theme(legend.position=\"none\", axis.text.x= element_blank(), axis.title.x = element_blank())\n",
    "    #p <- p + theme(legend.position=\"none\", axis.text.x= element_text(angle = 90, hjust = 1), axis.title.x = element_blank())\n",
    "  }\n",
    "  else {\n",
    "    p <-  p + theme(legend.position=\"none\", axis.text.x= element_blank(), axis.title.x = element_blank())\n",
    "  }\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "F.plotMultipleLines <- function(filelines,xtext=FALSE){\n",
    "  p <- list()\n",
    "  for (i in 1:nrow(filelines)){\n",
    "    p[[i]] <- F.plotOneline(filelines[i,],xtext = xtext)\n",
    "  }\n",
    "  #p[[i+1]] <- F.plotOneline(filelines[i+1,], xtext = TRUE)\n",
    "  return(grid.arrange(grobs=p,ncol=1))\n",
    "}\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20190131New/structure/Junonia93structure.pdf',width = 24, height = 24)\n",
    "filelines = df_files[df_files$baseType == 'CDS' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "filelines = df_files[df_files$baseType == 'zChrCDS' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "filelines = df_files[df_files$baseType == 'zChr' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict father sequences\n",
    "```\n",
    "mother = '18098H11'\n",
    "children = ['18098H12', '18126E01', '18126E02', '18126E03', '18126E04', '18126E05', '18126E06', '18126E07', '18126E08', '18126E09']\n",
    "child = children[0]\n",
    "\n",
    "folder1 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "folder2 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def getFatherBase(m1, m2, c1, c2):\n",
    "    '''\n",
    "    A 65, T 84, C 67, G 71, - 45\n",
    "    if \n",
    "    '''\n",
    "    m_ori = [m1, m2]\n",
    "    c_ori = [c1, c2]\n",
    "    #remove gap sites\n",
    "    m_ori = [e for e in m_ori if e != 45]\n",
    "    c_ori = [e for e in c_ori if e != 45]\n",
    "    mol = len(m_ori)\n",
    "    col = len(c_ori)\n",
    "    #remove gap sites and keep unique bases\n",
    "    m = list(set(m_ori))\n",
    "    c = list(set(c_ori))\n",
    "    ml = len(m)\n",
    "    cl = len(c)\n",
    "    #preserve original order if no duplicated values in mother or child\n",
    "    if ml == mol:\n",
    "        m = m_ori\n",
    "    if cl == col:\n",
    "        c = c_ori\n",
    "    #mother is unknown\n",
    "    if ml == 0:\n",
    "        return 45\n",
    "    #child is unknown\n",
    "    if cl == 0:\n",
    "        return 45\n",
    "    #child unique bases\n",
    "    cx = [e for e in c if e not in m]\n",
    "    cxl = len(cx)\n",
    "    if cxl == 0:\n",
    "        return c[-1]\n",
    "    else:\n",
    "        return cx[-1]\n",
    "\n",
    "def getFatherNp(npMother1,npMother2,npChild1,npChild2):\n",
    "    '''\n",
    "    return npFather use the function getFatherBase\n",
    "    '''\n",
    "    size = len(npMother1)\n",
    "    npFather = np.ones(size, dtype=np.uint8) * 45\n",
    "    N = 0\n",
    "    for m1, m2, c1, c2 in zip(npMother1,npMother2,npChild1,npChild2):\n",
    "        npFather[N] = getFatherBase(m1, m2, c1, c2)\n",
    "        N += 1\n",
    "    return npFather\n",
    "\n",
    "\n",
    "def getFather(mother, child, threads = 32):\n",
    "    '''\n",
    "    given the sample ID of mother and child, return a numpy array of father\n",
    "    '''\n",
    "    post_fix = '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'\n",
    "    f_mother1 = os.path.join(folder1,mother + post_fix)\n",
    "    f_mother2 = os.path.join(folder2,mother + post_fix)\n",
    "    f_child1 = os.path.join(folder1,child + post_fix)\n",
    "    f_child2 = os.path.join(folder2,child + post_fix)\n",
    "    npMother1 = pickle.load(open(f_mother1,'rb'))\n",
    "    npMother2 = pickle.load(open(f_mother2,'rb'))\n",
    "    npChild1 = pickle.load(open(f_child1,'rb'))\n",
    "    npChild2 = pickle.load(open(f_child2,'rb'))\n",
    "    \n",
    "    ls_np = [npMother1, npMother2, npChild1, npChild2]\n",
    "    lsls_np = [np.array_split(e, threads) for e in ls_np]\n",
    "    lsls_np = list(zip(*lsls_np))\n",
    "    \n",
    "    pool = Pool(threads)\n",
    "    ls_npFather = pool.starmap(getFatherNp, lsls_np)\n",
    "    pool.close()\n",
    "    npFather = np.concatenate(ls_npFather)\n",
    "    \n",
    "    outfile = os.path.join(folder1, child+'F'+post_fix)\n",
    "    pickle.dump(npFather, open(outfile,'wb',))\n",
    "    return npFather\n",
    "\n",
    "for child in children:\n",
    "    getFather(mother, child, threads = 32)\n",
    "\n",
    "#getFather(mother, children[0], threads = 32)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences for 109 samples\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import itertools\n",
    "import mapfileIO\n",
    "\n",
    "J_c_coenia=['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "J_c_grisea=['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "J_nigrosuffusa=['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "J_nigrosuffusaTX=['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "J_MEXICANspecies=['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "J_vestina=['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "J_new = ['18098H06', '18098H07', '18098H08', '18098H09', '18098H10', '18098H11', '18098H12', '18126E01', '18126E02', '18126E03', '18126E04', '18126E05', '18126E06', '18126E07', '18126E08', '18126E09']\n",
    "J_father = ['18098H12F', '18126E01F', '18126E02F', '18126E03F', '18126E04F', '18126E05F', '18126E06F', '18126E07F', '18126E08F', '18126E09F']\n",
    "\n",
    "dc = {}\n",
    "dc['J_c_coenia'] = J_c_coenia\n",
    "dc['J_c_grisea'] = J_c_grisea\n",
    "dc['J_nigrosuffusa'] = J_nigrosuffusa\n",
    "dc['J_nigrosuffusaTX'] = J_nigrosuffusaTX\n",
    "dc['J_vestina'] = J_vestina\n",
    "dc['J_MEXICANspecies'] = J_MEXICANspecies\n",
    "dc['J_new'] = J_new\n",
    "dc['J_father'] = J_father\n",
    "samples = list(itertools.chain.from_iterable(dc.values()))\n",
    "\n",
    "#get npMap files to work with\n",
    "import os\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "files = [folder + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "files = [e for e in files if os.path.exists(e)]\n",
    "print('total map npInt8 files', len(files))\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid109.zChr'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilter'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid109.zChrCDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilterCDS'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid109.CDS'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/gff.0inter.2intron.6CDS.14UTR'\n",
    "mapfileIO.npInt8stoFastas(files, threads=32, output=outfile, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=lambda x:x==6, merged=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get trees for 109 sequences\n",
    "transfer to bioHPC\n",
    "```\n",
    " scp /home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid109.* s185491@nucleus.biohpc.swmed.edu:/work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/\n",
    "\n",
    "```\n",
    "\n",
    "run ExaML\n",
    "```\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 56 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.CDS -N 100\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 56 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChr -N 100\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 56 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split/ -m 1 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChrCDS -N 5\n",
    "python /home2/s185491/p/xiaolongTools/utils/splitFastaAlignments2NPartsContinuingEqualStepOrRandom.py -t 56 -o /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split/ -m 2 -i /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChrCDS -N 5\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split/'\n",
    "folder_cmd = '/work/biophysics/s185491/2018junonia/20190214ExaML/cmds/'\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(folder+'*')\n",
    "\n",
    "txt_qsub = '''#!/bin/bash\n",
    "#SBATCH --job-name=zChr\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "\n",
    "/home2/s185491/p/anaconda3/anaconda520/bin/python3 /home2/s185491/p/xiaolongTools/utils/runExaMLForIndividualSmallFiles.py -i {filename} -T 32 -r /work/biophysics/s185491/2018junonia/20190214ExaML/JunoniaHybrid109.zChrCDS.NJ\n",
    "'''\n",
    "\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    open(os.path.join(folder_cmd,'qsub_individual_'+basename),'w').write(txt_qsub.format(filename = f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get trees\n",
    "```\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split.ExaMLbestTree/JunoniaHybrid109.CDS.split100.* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.CDS.splitTrees\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split.ExaMLbestTree/JunoniaHybrid109.zChr.split100.* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChr.splitTrees\n",
    "cat /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/split.ExaMLbestTree/JunoniaHybrid109.zChrCDS* > /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChrCDS.splitTrees\n",
    "\n",
    "cd /work/biophysics/s185491/2018junonia/20190214ExaML/seqs109\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid109.CDS.splitTrees -t ExaML_result.JunoniaHybrid109.CDS -m GTRGAMMA -n JunoniaHybrid109.CDS.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid109.CDS.boots\n",
    "\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid109.zChr.splitTrees -t ExaML_result.JunoniaHybrid109.zChr -m GTRGAMMA -n JunoniaHybrid109.zChr.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid109.zChr.boots\n",
    "\n",
    "/home2/s185491/p/raxml/standard-RAxML-master/raxmlHPC-AVX -f b -z JunoniaHybrid109.zChrCDS.splitTrees -t ExaML_result.JunoniaHybrid109.zChrCDS -m GTRGAMMA -n JunoniaHybrid109.zChrCDS.boots\n",
    "/home2/mtang/anaconda2/bin/python /work/biophysics/mtang/SNP_calling/scripts/rename_tree_fullname.py RAxML_bipartitions.JunoniaHybrid109.zChrCDS.boots\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run STRUCTURE 109 samples\n",
    "get sequences for structure\n",
    "remove J_vestina\n",
    "```\n",
    "files = '''/work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.CDS\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChr\n",
    "/work/biophysics/s185491/2018junonia/20190214ExaML/seqs109/JunoniaHybrid109.zChrCDS\n",
    "'''.split()\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/seq4structure/'\n",
    "\n",
    "J_vestina=['5660', '15117F01', '5658', '5659', '5661', '15113A12']\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "def cleanSample(filename):\n",
    "    '''\n",
    "    remove J_vestina\n",
    "    '''\n",
    "    basename = os.path.basename(filename)\n",
    "    basename = basename.replace('99','93')\n",
    "    outfile = os.path.join(outfolder, basename)\n",
    "    fout = open(outfile,'w')\n",
    "    for s in SeqIO.parse(filename,'fasta'):\n",
    "        if s.id not in J_vestina:\n",
    "            fout.write('>'+s.id+'\\n'+str(s.seq)+'\\n')\n",
    "    fout.close()\n",
    "    return None\n",
    "\n",
    "for filename in files:\n",
    "    cleanSample(filename)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect structure data and plot.  \n",
    "Almost the same as the scripts before for 99 samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect structure data\n",
    "```\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/STRUCTURE')\n",
    "import extractStructureTableFromCommandlineOutput\n",
    "folder_structure = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/'\n",
    "outfolder = '/work/biophysics/s185491/2018junonia/20190214ExaML/structure/results/'\n",
    "files_structure = glob.glob(folder_structure +'runSTRUCTURE*/*_f')\n",
    "for f in files_structure:\n",
    "    key = os.path.basename(f)\n",
    "    outfile = os.path.join(outfolder, key)\n",
    "\n",
    "dc_results = {}\n",
    "for f in files_structure:\n",
    "    key = os.path.basename(f)\n",
    "    outfile = os.path.join(outfolder, key)\n",
    "    dc_results[key] = extractStructureTableFromCommandlineOutput.extractStructureTableFromCommandlineOutput(filename=f, output=outfile, processLineForR=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot structure\n",
    "```\n",
    "library(readxl)\n",
    "library(tidyr)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "\n",
    "folder_files <- '/home/xcao/w/20180905Junonia_coenia/20190131New/structure/results/'\n",
    "result_files <- list.files(folder_files,full.names = FALSE)\n",
    "basetypes <- c(\"zChrCDS\",\"CDS\",\"zChr\")\n",
    "\n",
    "f.getbasetype <- function(x){\n",
    "  for (keyword in basetypes){\n",
    "    if (grepl(keyword,x)) return(keyword)\n",
    "  }\n",
    "}\n",
    "\n",
    "f.getSplitgroup <- function(x){\n",
    "  if (grepl('.split',x)) {\n",
    "    return(as.integer(gsub(\"(.*split\\\\d*\\\\.)|(\\\\.STRUCTUREresult.*)\",\"\",x)))\n",
    "  } else return(0)\n",
    "}\n",
    "\n",
    "f.getRandint <- function(x){\n",
    "  #x=\"Calephelis266.zChr.split5.4.STRUCTUREresult.s1000.pop8_f\" return 1000\n",
    "  return(as.integer(gsub('(.*STRUCTUREresult.s)|(.pop.*_f)','',x)))\n",
    "}\n",
    "\n",
    "f.getPopulation <- function(x) {\n",
    "  return(as.integer(gsub('(.*STRUCTUREresult.s\\\\d*.pop)|(_f)','',x)))\n",
    "}\n",
    "\n",
    "df_files <- data.frame(baseType=sapply(result_files, f.getbasetype),\n",
    "                       splitID=sapply(result_files, f.getSplitgroup),\n",
    "                       seed=sapply(result_files, f.getRandint),\n",
    "                       population=sapply(result_files,f.getPopulation),\n",
    "                       filename=file.path(folder_files,result_files), stringsAsFactors = FALSE)\n",
    "\n",
    "df_files <- df_files[order(df_files$baseType, df_files$population,df_files$seed,df_files$splitID),]\n",
    "\n",
    "F.renameHead <- function(x){\n",
    "  x[1] = 'sample'\n",
    "  x[2:length(x)] = paste0('pop',1:(length(x)-1))\n",
    "  return(x)\n",
    "}\n",
    "\n",
    "filename2 <- '/home/xcao/w/20180905Junonia_coenia/20180919Info/20190130Junonia-all-sample-summary.xlsx'\n",
    "xdf_info <- read_excel(filename2,sheet = 'Sheet1')\n",
    "# xdf_info <- xdf_info[!is.na(xdf_info$order_266mitoTree),]\n",
    "# v_id2Treename <- xdf_info$Treename\n",
    "# names(v_id2Treename) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "v_id2species <- xdf_info$`Taxon name`\n",
    "names(v_id2species) <- sapply(xdf_info$ID, function(x)substring(x,2))\n",
    "species_in_order <- xdf_info[order(xdf_info$species),]$ID\n",
    "species_in_order <- substr(species_in_order,2,length(species_in_order))\n",
    "species_in_order <- species_in_order[species_in_order %in% xdf_s$sample]\n",
    "\n",
    "\n",
    "oneline = df_files[1,]\n",
    "xdf_s <- read.csv(oneline$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "#xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species)\n",
    "xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "\n",
    "p <- ggplot(xdf_s.clear,aes(x=sample,y=fraction,fill=pop)) + geom_bar(stat = 'identity') + scale_x_discrete(limits=species_in_order)\n",
    "\n",
    "\n",
    "\n",
    "F.plotOneline <- function(oneline,xtext=FALSE){\n",
    "  xdf_s <- read.csv(oneline$filename,header = FALSE,sep = '\\t', stringsAsFactors = FALSE)\n",
    "  colnames(xdf_s) <- F.renameHead(colnames(xdf_s))\n",
    "  xdf_s$species <- v_id2species[xdf_s$sample]\n",
    "  #xdf_s$treename <- v_id2Treename[xdf_s$sample]\n",
    "  xdf_s.clear <- gather(xdf_s,key = 'pop',value = 'fraction',-sample,-species)\n",
    "  xdf_s.clear <- xdf_s.clear[order(xdf_s.clear$species,xdf_s.clear$pop,-xdf_s.clear$fraction),]\n",
    "  \n",
    "  p <- ggplot(xdf_s.clear,aes(x=sample,y=fraction,fill=pop)) + geom_bar(stat = 'identity') + scale_x_discrete(limits=species_in_order[species_in_order %in% xdf_s$sample]) + ylab(paste0('seed',oneline$seed,'_',oneline$baseType,'_K=',oneline$population,\" \",oneline$splitID))\n",
    "  if (xtext) {\n",
    "    p <- p + geom_text(aes(x=sample,y=0,label=paste(sample,species)),angle=90,hjust=0)+ theme(legend.position=\"none\", axis.text.x= element_blank(), axis.title.x = element_blank())\n",
    "    #p <- p + theme(legend.position=\"none\", axis.text.x= element_text(angle = 90, hjust = 1), axis.title.x = element_blank())\n",
    "  }\n",
    "  else {\n",
    "    p <-  p + theme(legend.position=\"none\", axis.text.x= element_blank(), axis.title.x = element_blank())\n",
    "  }\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "F.plotMultipleLines <- function(filelines,xtext=FALSE){\n",
    "  p <- list()\n",
    "  for (i in 1:nrow(filelines)){\n",
    "    p[[i]] <- F.plotOneline(filelines[i,],xtext = xtext)\n",
    "  }\n",
    "  #p[[i+1]] <- F.plotOneline(filelines[i+1,], xtext = TRUE)\n",
    "  return(grid.arrange(grobs=p,ncol=1))\n",
    "}\n",
    "\n",
    "pdf('/home/xcao/w/20180905Junonia_coenia/20190131New/structure/Junonia93structure.pdf',width = 24, height = 24)\n",
    "filelines = df_files[df_files$baseType == 'CDS' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "filelines = df_files[df_files$baseType == 'zChrCDS' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "filelines = df_files[df_files$baseType == 'zChr' &df_files$splitID==0,]\n",
    "F.plotMultipleLines(filelines = filelines,xtext = TRUE)\n",
    "dev.off()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if new hybrids have same mother by comparing mitochondria genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assemble mitochondria\n",
    "```\n",
    "txt = '''cd /home/xcao/w/20180905Junonia_coenia/20181004mitochondrion && mkdir {name} && cd {name} && /home/xcao/p/mira/mira_4.0.2_linux-gnu_x86_64_static/bin/mirabait /home/xcao/w/genomes/mitochondrion/20181004mitochondrionGenome406butterflies.fasta /archive/butterfly/ready_fastq/{name}_R1.fastq {name}_R1.fastq && /home/xcao/p/mira/mira_4.0.2_linux-gnu_x86_64_static/bin/mirabait /home/xcao/w/genomes/mitochondrion/20181004mitochondrionGenome406butterflies.fasta /archive/butterfly/ready_fastq/{name}_R2.fastq {name}_R2.fastq && /home/xcao/p/SPAdes/SPAdes-3.13.0-Linux/bin/spades.py -k 33,55,77,99 -t 4 --s1 {name}_R1.fastq --s2 {name}_R2.fastq -o . && mv contigs.fasta ../{name}_mito.fa && rm -rf /home/xcao/w/20180905Junonia_coenia/20181004mitochondrion/{name}\n",
    "'''\n",
    "\n",
    "sample_prefix = ['18098H06', '18098H07', '18098H08', '18098H09', '18098H10', '18098H11', '18098H12', '18126E01', '18126E02', '18126E03', '18126E04', '18126E05', '18126E06', '18126E07', '18126E08', '18126E09']\n",
    "open('/home/xcao/w/20180905Junonia_coenia/20180913scripts/2019junonia_assemble_mitochondria.cmds','w').write(''.join(txt.format(name = name) for name in sample_prefix))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect mitochondria sequences  \n",
    "only collect those longer than 1000bp, sequences in each sample as sample name `_N`. keep those with total length > 10k\n",
    "```\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20181004mitochondrion/'\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/mito/20190219Junonia_mito.fa'\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "def collectFa(filename):\n",
    "    '''\n",
    "    filename is a assemble result of mitochondria\n",
    "    return a txt for writing, in fasta format\n",
    "    '''\n",
    "    seqkeep = []\n",
    "    total_len = 0\n",
    "    for s in SeqIO.parse(filename,'fasta'):\n",
    "        if len(s.seq) > 500:\n",
    "            seqkeep.append(s)\n",
    "            total_len += len(s.seq)\n",
    "    \n",
    "    if len(seqkeep) == 0 or total_len < 10000:\n",
    "        return ''\n",
    "    \n",
    "    sample_name = os.path.basename(filename).split('_')[0]\n",
    "    ls_txt = []\n",
    "    for n,s in enumerate(seqkeep):\n",
    "        ls_txt.append('>' + sample_name + '_' + str(n) + '\\n' + str(s.seq) +'\\n')\n",
    "    \n",
    "    return ''.join(ls_txt)\n",
    "\n",
    "files = os.listdir(folder)\n",
    "fout = open(outfile,'w')\n",
    "for f in files:\n",
    "    fout.write(collectFa(os.path.join(folder,f)))\n",
    "fout.close()\n",
    "\n",
    "#get mitochondria sequence based on reference mito\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20190131New/mito/20190219Junonia_mito.fa'\n",
    "reference = '/home/xcao/w/20180905Junonia_coenia/20190131New/mito/reference.fa'\n",
    "\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "from Bio import SeqIO\n",
    "import io\n",
    "import copy\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def muscleAlignment(seqs, muscle_exe = \"muscle\"):\n",
    "    '''\n",
    "    align sequences with muscle\n",
    "    given a list of seqs in SeqIO format,\n",
    "    return a aligned seqs in SeqIO format\n",
    "    '''\n",
    "    f_mem = io.StringIO()\n",
    "    SeqIO.write(seqs,f_mem,'fasta')\n",
    "    data = f_mem.getvalue()\n",
    "    muscle_cline = MuscleCommandline(muscle_exe)\n",
    "    stdout, stderr = muscle_cline(stdin=data)\n",
    "    return list(SeqIO.parse(io.StringIO(stdout),'fasta'))\n",
    "\n",
    "def getAlignLen(seq, minFrag = 10):\n",
    "    '''\n",
    "    seq is like 'ATAAA---A-GG-ATCG'\n",
    "    return alignment length\n",
    "    do not count fragments shorter than minFrag\n",
    "    '''\n",
    "    l = re.split('-+',seq)\n",
    "    l = [e for e in l if len(e) >= minFrag]\n",
    "    return sum([len(e) for e in l])\n",
    "\n",
    "def getAlign(seq1,refseq):\n",
    "    '''\n",
    "    seq1 is a Bio.Seq object, refseq is a reference Bio.Seq object\n",
    "    return str with the same length of refseq, based on alignment result of muslce\n",
    "    refseq can form a circle, so need to make a new two concatenated copied of reference\n",
    "    '''\n",
    "    seqlen = len(refseq.seq)\n",
    "    refstr = str(refseq.seq).upper()\n",
    "    r = copy.deepcopy(refseq)\n",
    "    r.seq = r.seq+r.seq\n",
    "    aligns = muscleAlignment([seq1, r])\n",
    "    s1 = str(aligns[0].seq).upper()\n",
    "    s2 = str(aligns[1].seq).upper()\n",
    "    \n",
    "    seq2 = seq1.reverse_complement()\n",
    "    aligns = muscleAlignment([seq2, r])\n",
    "    v1 = str(aligns[0].seq).upper()\n",
    "    v2 = str(aligns[1].seq).upper()\n",
    "    \n",
    "    if getAlignLen(s1) < getAlignLen(v1):\n",
    "        s1 = v1\n",
    "        s2 = v2\n",
    "    \n",
    "    result = ['-' for e in range(seqlen)]\n",
    "    N = 0\n",
    "    for b1, b2 in zip(s1,s2):\n",
    "        if b2 == '-':\n",
    "            continue\n",
    "        if N >= seqlen:\n",
    "            N = N-seqlen\n",
    "        if b1 != '-':\n",
    "            if result[N] == '-':\n",
    "                result[N] = b1\n",
    "            elif result[N] != refstr[N]:\n",
    "                result[N] = b1\n",
    "        N += 1\n",
    "    return ''.join(result)\n",
    "    \n",
    "    \n",
    "def getMitoBasedRef(dc_seqs, sample_id, refseq):\n",
    "    '''\n",
    "    seqs is a list of SeqIO sequences of one sample\n",
    "    return a sequence based on refseq, a str in fasta format that is ready to write\n",
    "    refseq is a mito sequences together\n",
    "    '''\n",
    "    seqs = dc_seqs[sample_id]\n",
    "    refstr = str(refseq.seq).upper()\n",
    "    seqlen = len(refseq.seq)\n",
    "    ls_align = [getAlign(e,refseq) for e in seqs]\n",
    "    result = ['-' for e in range(seqlen)]\n",
    "    for N, bases in enumerate(zip(*ls_align)):\n",
    "        bases = [e for e in bases if e != '-']\n",
    "        bases = Counter(bases).most_common()\n",
    "        if len(bases) != 0:\n",
    "            if refstr[N] in [e[0] for e in bases]:\n",
    "                result[N] = refstr[N]\n",
    "            else:\n",
    "                result[N] = bases[0][0]\n",
    "    seq = ''.join(result)\n",
    "    print(sample_id,'finished')\n",
    "    return '>'+sample_id+'\\n' + seq+'\\n'\n",
    "\n",
    "seqs = list(SeqIO.parse(filename,'fasta'))\n",
    "refseq = SeqIO.read(reference,'fasta')\n",
    "\n",
    "dc_seqs = {}\n",
    "for s in seqs:\n",
    "    key = s.id.split('_')[0]\n",
    "    if key not in dc_seqs:\n",
    "        dc_seqs[key] = []\n",
    "    dc_seqs[key].append(s)\n",
    "print('total samples to analyze',len(dc_seqs))\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(48)\n",
    "results = pool.starmap(getMitoBasedRef,[[dc_seqs, k, refseq] for k in dc_seqs])\n",
    "pool.close()\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/mito/20190219Junonia_mito.align.fa'\n",
    "open(outfile,'w').write(''.join(results))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get trees with ExaML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check mitochondria alignment by map file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run unbiased pipeline to get mitochondria alignment.\n",
    "\n",
    "```\n",
    "samples = '''NVG-18098H11\n",
    "NVG-18098H12\n",
    "NVG-18126E01\n",
    "NVG-18126E02\n",
    "NVG-18126E03\n",
    "NVG-18126E04\n",
    "NVG-18126E05\n",
    "NVG-18126E06\n",
    "NVG-18126E07\n",
    "NVG-18126E08\n",
    "NVG-18126E09\n",
    "'''.replace('NVG-','').split()\n",
    "\n",
    "files = ['/archive/butterfly/maps/debiased/{e}_NC_028207_snp_step2.map'.format(e=e) for e in samples]\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "import os\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/mito/20190225mitoAlignment.fa'\n",
    "mapfileIO.mapFilestoFastas(files, threads=32, output=outfile, headerFun=lambda x:os.path.basename(x).split('_')[0], npFilter=None, npFilterFun=None, merged=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check mitochondria quality by viewing reads alignment\n",
    "```\n",
    "txt = '''\n",
    "minimap2 -a /home/xcao/w/genomes/Junonia_coenia/NC_028207.fa /home/xcao/w/20180905Junonia_coenia/20190131New/mito/{test}.fas >/home/xcao/w/20180905Junonia_coenia/20190131New/mito/{test}.sam\n",
    "samtools sort /home/xcao/w/20180905Junonia_coenia/20190131New/mito/{test}.sam -o /home/xcao/w/20180905Junonia_coenia/20190131New/mito/{test}.bam\n",
    "samtools index /home/xcao/w/20180905Junonia_coenia/20190131New/mito/{test}.bam\n",
    "'''.format(test='test')\n",
    "print(txt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get alignment of zChr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "samples = ['18098H11', '18098H12', '18126E03', '18126E04', '18126E07', '18126E08', '18126E09', '18126E01', '18126E02', '18126E05', '18126E06']\n",
    "\n",
    "folder1 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "folder2 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import os\n",
    "import mapfileIO\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "files1 = [folder1 + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "files2 = [folder2  + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "outfile1 = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.zChr1'\n",
    "file_filter = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/zChrFilter'\n",
    "mapfileIO.npInt8stoFastas(files1, threads=32, output=outfile1, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "outfile2 = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.zChr2'\n",
    "mapfileIO.npInt8stoFastas(files2, threads=32, output=outfile2, headerFun=lambda x:x.split('_')[0], npFilter=file_filter, npFilterFun=None, merged=True)\n",
    "\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.zChr'\n",
    "fout = open(outfile,'w')\n",
    "for s1, s2 in zip(SeqIO.parse(outfile1,'fasta'), SeqIO.parse(outfile2,'fasta')):\n",
    "    fout.write('>'+s1.id+'_1\\n'+str(s1.seq)+'\\n')\n",
    "    fout.write('>'+s2.id+'_2\\n'+str(s2.seq)+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze SNPs in the whole genome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "samples = ['18098H11', '18098H12', '18126E03', '18126E04', '18126E07', '18126E08', '18126E09', '18126E01', '18126E02', '18126E05', '18126E06']\n",
    "\n",
    "folder1 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'\n",
    "folder2 = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import os\n",
    "import mapfileIO\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "threads = 32\n",
    "files1 = [folder1 + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "files2 = [folder2  + e + '_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map' for e in samples]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for f in files1:\n",
    "    df[os.path.basename(f).split('_')[0]+'_0'] = mapfileIO.loadMapBinary(f)\n",
    "for f in files2:\n",
    "    df[os.path.basename(f).split('_')[0]+'_1'] = mapfileIO.loadMapBinary(f)\n",
    "\n",
    "np2d = df.values\n",
    "\n",
    "def goodSNPsites(n):\n",
    "    '''\n",
    "    n is index number in np2d\n",
    "    return True or False to indicate if the index is a good snp site\n",
    "    a good snp site: with no gap, the bases are not identical, the any base exist at least twice\n",
    "    '''\n",
    "    bases = np2d[n,]\n",
    "    if 45 in bases:\n",
    "        return False\n",
    "    base_type, base_count = np.unique(bases, return_counts=True)\n",
    "    if len(base_type) == 1:\n",
    "        return False\n",
    "    if min(base_count) == 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "pool = Pool(threads)\n",
    "sites = np.array(pool.map(goodSNPsites,range(df.shape[0])))\n",
    "pool.close()\n",
    "\n",
    "np2d_snp = np2d[sites,]\n",
    "df_snp = pd.DataFrame(np2d_snp,columns=df.columns)\n",
    "\n",
    "dc_snp = dict([[e,df_snp[e]] for e in df_snp.columns])\n",
    "fout = open('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.snp','w')\n",
    "for key,seq in dc_snp.items():\n",
    "    fout.write('>'+key+'\\n'+mapfileIO.npInt8toSeq(seq)+'\\n')\n",
    "fout.close()\n",
    "\n",
    "\n",
    "dc_snp = {}\n",
    "for seq in SeqIO.parse('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.snp','fasta'):\n",
    "    dc_snp[seq.id] = np.fromstring(str(seq.seq),dtype=np.uint8)\n",
    "\n",
    "#re-arrange SNP in children so that one strand is more similar to mother\n",
    "mother = '18098H11'\n",
    "children = ['18098H12', '18126E03', '18126E04', '18126E07', '18126E08', '18126E09', '18126E01', '18126E02', '18126E05', '18126E06']\n",
    "\n",
    "#get father snp\n",
    "def getFather(child):\n",
    "    '''\n",
    "    mother is '18098H11'\n",
    "    given a child, re-arrange SNP in two strand so that strand0 is more similar to mother\n",
    "    '''\n",
    "    mother0 = dc_snp['18098H11_0']\n",
    "    mother1 = dc_snp['18098H11_1']\n",
    "    child0 = dc_snp[child+'_0']\n",
    "    child1 = dc_snp[child+'_1']\n",
    "    seqlen = len(mother0)\n",
    "    father =[]\n",
    "    for n in range(seqlen):\n",
    "        ms = [mother0[n],mother1[n]]\n",
    "        c0 = child0[n]\n",
    "        c1 = child1[n]\n",
    "        if c0 in ms and c1 not in ms:\n",
    "            father.append(c1)\n",
    "        elif c0 not in ms and c1 in ms:\n",
    "            father.append(c0)\n",
    "        else:\n",
    "            father.append(45)\n",
    "    father = np.array(father,dtype=np.uint8)\n",
    "    return father\n",
    "\n",
    "\n",
    "pool = Pool(threads)\n",
    "ls_father = pool.map(getFather,children)\n",
    "pool.close()\n",
    "dc_father = dict(zip(children,ls_father))\n",
    "\n",
    "def getAlignmentNoGapNoIdentical(dc):\n",
    "    '''\n",
    "    return two dict, one is alignment with nogap, one is with nogap and noIdentical\n",
    "    '''\n",
    "    keys = list(dc.keys())\n",
    "    seq2d = np.array(list(dc.values()))\n",
    "    seq2d = seq2d.T\n",
    "    seq2d_nogap = seq2d[list(map(lambda x:45 not in x,seq2d))]\n",
    "    seq2d_noIdentical = seq2d_nogap[list(map(lambda x:len(set(x))>1, seq2d_nogap))]\n",
    "    return dict(zip(keys, seq2d_nogap.T)), dict(zip(keys,seq2d_noIdentical.T))\n",
    "\n",
    "dc_fatherNoGap, dc_fatherNoIdentical = getAlignmentNoGapNoIdentical(dc_father)\n",
    "\n",
    "def writeDcSeq(dc,outfile):\n",
    "    '''\n",
    "    write dc seq to outfile in fasta format\n",
    "    '''\n",
    "    fout = open(outfile,'w')\n",
    "    for k,v in dc.items():\n",
    "        fout.write('>'+k+'\\n')\n",
    "        fout.write(''.join(chr(e) for e in v))\n",
    "        fout.write('\\n')\n",
    "    fout.close()\n",
    "\n",
    "writeDcSeq(dc_fatherNoGap,'/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.father.noGap')\n",
    "writeDcSeq(dc_fatherNoIdentical,'/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.father.noIdentical')\n",
    "\n",
    "df_similar = pd.DataFrame()\n",
    "for col, index in itertools.product(children, repeat=2):\n",
    "    df_similar.loc[index,col] = ((dc_father[index] == dc_father[col]) & (dc_father[index] != 45)).sum()\n",
    "\n",
    "df_similar.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.father.xlsx')\n",
    "\n",
    "\n",
    "def getFather2(child):\n",
    "    '''\n",
    "    mother is '18098H11'\n",
    "    given a child, re-arrange SNP in two strand so that strand0 is more similar to mother\n",
    "    '''\n",
    "    mother0 = dc_snp['18098H11_0']\n",
    "    mother1 = dc_snp['18098H11_1']\n",
    "    child0 = dc_snp[child+'_0']\n",
    "    child1 = dc_snp[child+'_1']\n",
    "    seqlen = len(mother0)\n",
    "    father =[]\n",
    "    for n in range(seqlen):\n",
    "        c0,c1,m0,m1 = child0[n],child1[n],mother0[n],mother1[n]\n",
    "        if c0 == c1 and m0 == m1:\n",
    "            if c0 != m0:\n",
    "                father.append(c0)\n",
    "            else:\n",
    "                father.append(45)\n",
    "        else:\n",
    "            father.append(45)\n",
    "    father = np.array(father,dtype=np.uint8)\n",
    "    return father\n",
    "\n",
    "\n",
    "pool = Pool(threads)\n",
    "ls_father = pool.map(getFather2,children)\n",
    "pool.close()\n",
    "dc_father = dict(zip(children,ls_father))\n",
    "\n",
    "df_similar = pd.DataFrame()\n",
    "for col, index in itertools.product(children, repeat=2):\n",
    "    df_similar.loc[index,col] = ((dc_father[index] == dc_father[col]) & (dc_father[index] != 45)).sum()\n",
    "\n",
    "df_similar.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.father2.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "def getMother(child):\n",
    "    '''\n",
    "    mother is '18098H11'\n",
    "    given a child, re-arrange SNP in two strand so that strand0 is more similar to mother\n",
    "    '''\n",
    "    mother0 = dc_snp['18098H11_0']\n",
    "    mother1 = dc_snp['18098H11_1']\n",
    "    child0 = dc_snp[child+'_0']\n",
    "    child1 = dc_snp[child+'_1']\n",
    "    seqlen = len(mother0)\n",
    "    mother =[]\n",
    "    for n in range(seqlen):\n",
    "        ms = [mother0[n],mother1[n]]\n",
    "        c0 = child0[n]\n",
    "        c1 = child1[n]\n",
    "        if ms[0] == ms[1]:\n",
    "            mother.append(45)\n",
    "        else:\n",
    "            if c0 in ms and c1 not in ms:\n",
    "                mother.append(c0)\n",
    "            elif c0 not in ms and c1 in ms:\n",
    "                mother.append(c1)\n",
    "#            elif c0 in ms and c1 in ms:\n",
    "#                mother.append(c0)\n",
    "            else:\n",
    "                mother.append(45)\n",
    "    mother = np.array(mother,dtype=np.uint8)\n",
    "    return mother\n",
    "\n",
    "\n",
    "pool = Pool(threads)\n",
    "ls_mother = pool.map(getMother,children)\n",
    "pool.close()\n",
    "dc_mother = dict(zip(children,ls_mother))\n",
    "\n",
    "dc_motherNoGap, dc_motherNoIdentical = getAlignmentNoGapNoIdentical(dc_mother)\n",
    "\n",
    "writeDcSeq(dc_fatherNoGap,'/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.mother.noGap')\n",
    "writeDcSeq(dc_fatherNoIdentical,'/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.mother.noIdentical')\n",
    "\n",
    "df_similar = pd.DataFrame()\n",
    "for col, index in itertools.product(children, repeat=2):\n",
    "    df_similar.loc[index,col] = ((dc_mother[index] == dc_mother[col]) & (dc_mother[index] != 45)).sum()\n",
    "\n",
    "df_similar.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.mother.xlsx')\n",
    "\n",
    "def getMother2(child):\n",
    "    '''\n",
    "    mother is '18098H11'\n",
    "    given a child, re-arrange SNP in two strand so that strand0 is more similar to mother\n",
    "    '''\n",
    "    mother0 = dc_snp['18098H11_0']\n",
    "    mother1 = dc_snp['18098H11_1']\n",
    "    child0 = dc_snp[child+'_0']\n",
    "    child1 = dc_snp[child+'_1']\n",
    "    seqlen = len(mother0)\n",
    "    mother =[]\n",
    "    for n in range(seqlen):\n",
    "        c0,c1,m0,m1 = child0[n],child1[n],mother0[n],mother1[n]\n",
    "        if m0 == m1 and c0 == c1:\n",
    "            if c0 == m0:\n",
    "                mother.append(c0)\n",
    "            else:\n",
    "                mother.append(45)\n",
    "        else:\n",
    "            mother.append(45)\n",
    "    mother = np.array(mother,dtype=np.uint8)\n",
    "    return mother\n",
    "\n",
    "\n",
    "pool = Pool(threads)\n",
    "ls_mother = pool.map(getMother2,children)\n",
    "pool.close()\n",
    "dc_mother = dict(zip(children,ls_mother))\n",
    "\n",
    "df_similar = pd.DataFrame()\n",
    "for col, index in itertools.product(children, repeat=2):\n",
    "    df_similar.loc[index,col] = ((dc_mother[index] == dc_mother[col]) & (dc_mother[index] != 45)).sum()\n",
    "\n",
    "df_similar.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.mother2.xlsx')\n",
    "\n",
    "def getNovo(child):\n",
    "    '''\n",
    "    mother is '18098H11'\n",
    "    given a child, re-arrange SNP in two strand so that strand0 is more similar to mother\n",
    "    '''\n",
    "    mother0 = dc_snp['18098H11_0']\n",
    "    mother1 = dc_snp['18098H11_1']\n",
    "    child0 = dc_snp[child+'_0']\n",
    "    child1 = dc_snp[child+'_1']\n",
    "    seqlen = len(mother0)\n",
    "    novo =[]\n",
    "    for n in range(seqlen):\n",
    "        ms = [mother0[n],mother1[n]]\n",
    "        c0 = child0[n]\n",
    "        c1 = child1[n]\n",
    "        if c0 not in ms and c1 not in ms:\n",
    "            novo.append(random.choice([c0,c1]))\n",
    "        else:\n",
    "            novo.append(45)\n",
    "    novo = np.array(novo,dtype=np.uint8)\n",
    "    return novo\n",
    "\n",
    "\n",
    "pool = Pool(threads)\n",
    "ls_novo = pool.map(getNovo,children)\n",
    "pool.close()\n",
    "dc_novo = dict(zip(children,ls_novo))\n",
    "\n",
    "df_similar = pd.DataFrame()\n",
    "for col, index in itertools.product(children, repeat=2):\n",
    "    df_similar.loc[index,col] = ((dc_novo[index] == dc_novo[col]) & (dc_novo[index] != 45)).sum()\n",
    "\n",
    "df_similar.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/phylo/JunoniaHybrid11.novo.xlsx')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get gene trees with gene sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get a dictionary with gene name as ID and position in map file as value. positions stored in np.int array\n",
    "```\n",
    "#get a dictionary with gene name as ID and position in map file as value. positions stored in np.int array\n",
    "\n",
    "file_scflen = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.scaffolds.fa.len'\n",
    "file_gff = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.gff3'\n",
    "outfile = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/genes.locs'\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "dc_scf_start = {}\n",
    "l = open(file_scflen).readlines()\n",
    "n = 0\n",
    "for e in l:\n",
    "    es = e.split()\n",
    "    if len(es) != 2:\n",
    "        continue\n",
    "    scf, scf_len = es\n",
    "    scf_len = int(scf_len)\n",
    "    dc_scf_start[scf] = n\n",
    "    n += scf_len\n",
    "total_len = n\n",
    "\n",
    "dc_gene2maploc = {}\n",
    "l = open(file_gff).readlines()\n",
    "l = [e for e in l if '\\tgene\\t' in e]\n",
    "for e in l:\n",
    "    es = e.split()\n",
    "    if len(es) != 9:\n",
    "        continue\n",
    "    scf = es[0]\n",
    "    map_start = int(es[3]) - 1 + dc_scf_start[scf]\n",
    "    map_end = int(es[4]) + dc_scf_start[scf]\n",
    "    gene = es[8].replace('ID=','')\n",
    "    dc_gene2maploc[gene] = np.array(range(map_start,map_end))\n",
    "\n",
    "pickle.dump(dc_gene2maploc,open(outfile,'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences for 10 merged species\n",
    "```\n",
    "files = '''/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_c_coenia.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_c_grisea.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_MEXICANspecies.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_neildi.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_neildiTX.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_nigrosuffusa.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_nigrosuffusaTX.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_vestina.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_wMex.map\n",
    "/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/J_z_zonalis.map\n",
    "'''.split()\n",
    "\n",
    "file_dc = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/genes.locs'\n",
    "outputfolder = '/home/xcao/w/20180905Junonia_coenia/20190131New/20190225geneTree/seqs10species/'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import mapfileIO\n",
    "\n",
    "mapfileIO.npInt8stoMultipleFastas(npInt8s=files, threads=32, outputfolder=outputfolder, headerFun=lambda x:os.path.basename(x).replace('.map',''), npFilter=file_dc, minlen=0.5, minseq=5)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make trees with MrBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect trees to a single file\n",
    "```\n",
    "folder = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/seqs10speciesNewick/'\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(folder +'*')\n",
    "outfile = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/20190301allTrees.txt'\n",
    "fout = open(outfile,'w')\n",
    "for f in files:\n",
    "    fout.write('{gene_id}:{tree}\\n'.format(gene_id = os.path.basename(f), tree=open(f).read()))\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter trees with J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_neildi and use J_neildi as outgroup and summarize different topology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get tree topology and GO\n",
    "\n",
    "```\n",
    "filename = '/home/xcao/w/20180905Junonia_coenia/20190131New/20190225geneTree/20190301allTrees.txt'\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "genes = []\n",
    "tree_txts = []\n",
    "for line in open(filename):\n",
    "    gene, tree_txt = line.strip().split(':',1)\n",
    "    genes.append(gene)\n",
    "    tree_txts.append(tree_txt)\n",
    "df['gene'] = genes\n",
    "df['tree_txt'] = tree_txts\n",
    "\n",
    "# get tree topology with neildi as outgroup and with all 5 species J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_neildi\n",
    "species = 'J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_neildi'.split()\n",
    "def getTopology(tree_txt):\n",
    "    '''\n",
    "    given a tree_txt, return a tree_txt with only the species, and neildi as outgroup.\n",
    "    if some species is missing, return None\n",
    "    '''\n",
    "    tree = Tree(tree_txt)\n",
    "    if not all([leaf in tree.get_leaf_names() for leaf in species]):\n",
    "        return None\n",
    "    for leaf in tree.iter_leaves():\n",
    "        if leaf.name not in species:\n",
    "            leaf.delete()\n",
    "    tree.set_outgroup('J_neildi')\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "\n",
    "df['topology'] = df['tree_txt'].apply(getTopology)\n",
    "\n",
    "# get chr for each gene\n",
    "# get gene order based on chr and scaffold\n",
    "# get gene GO term and annotation\n",
    "file_gff = '/home/xcao/w/genomes/Junonia_coenia/Junonia_coenia_JC_v1.0.gff3'\n",
    "file_scforder = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.tab'\n",
    "df_scf = pd.read_csv(file_scforder,header=None,sep='\\t')\n",
    "df_scf.columns = ['chr','scf','start','end']\n",
    "df_gene = pd.DataFrame()\n",
    "genes = []\n",
    "scfs = []\n",
    "starts = []\n",
    "for line in open(file_gff):\n",
    "    es = line.split()\n",
    "    if len(es) < 9:\n",
    "        continue\n",
    "    if es[2] == 'gene':\n",
    "        genes.append(es[-1].replace('ID=',''))\n",
    "        scfs.append(es[0])\n",
    "        starts.append(int(es[3]))\n",
    "df_gene['gene'] = genes\n",
    "df_gene['scf'] = scfs\n",
    "df_gene['start'] = starts\n",
    "dc_scf2chr = dict(zip(df_scf['scf'], df_scf['chr']))\n",
    "df_gene['chr'] = df_gene['scf'].apply(lambda x:dc_scf2chr[x])\n",
    "dc_scf2order = dict(zip(df_scf['scf'], df_scf.index))\n",
    "df_gene['scf_order'] = df_gene['scf'].apply(lambda x:dc_scf2order[x])\n",
    "df_gene = df_gene.sort_values(by=['scf_order','start'])\n",
    "df_gene = df_gene.reset_index(drop=True)\n",
    "df_gene['gene_order'] = df_gene.index\n",
    "\n",
    "file_annotation = '/home/xcao/w/genomes/Junonia_coenia/20190301Junonia_proteinAnno.tsv'\n",
    "df_anno = pd.read_csv(file_annotation,sep='\\t',dtype=str)\n",
    "df_anno['proteinID'] = df_anno['proteinID'].apply(lambda x:x.split('-')[0])\n",
    "dc_gene2anno = dict(zip(df_anno['proteinID'], df_anno['annotation']))\n",
    "dc_gene2go = dict(zip(df_anno['proteinID'], df_anno['go-terms']))\n",
    "df_gene['anno'] = df_gene['gene'].apply(lambda x:dc_gene2anno[x])\n",
    "df_gene['GO'] = df_gene['gene'].apply(lambda x:dc_gene2go[x])\n",
    "df_gene.to_csv('/home/xcao/w/genomes/Junonia_coenia/20190301Junonia_gene_info.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "folder = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/'\n",
    "threads = 48\n",
    "import glob\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from multiprocessing import Pool\n",
    "files = glob.glob(folder+'seqs10species/*')\n",
    "target_species = 'J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX'.split()\n",
    "outgroups = 'J_neildi J_MEXICANspecies J_vestina J_wMex J_z_zonalis'.split()\n",
    "\n",
    "def filterSeqs(filename,ids_keep,outfile):\n",
    "    '''\n",
    "    filename is where sequences are stored. outfile is where to write the output file\n",
    "    write the output file only if all ids_keep were identified\n",
    "    only seqs in ids_keep were used\n",
    "    '''\n",
    "    seqs = list(SeqIO.parse(filename,'fasta'))\n",
    "    seqs = [e for e in seqs if e.id in ids_keep]\n",
    "    seq_ids = [e.id for e in seqs]\n",
    "    if all([e in seq_ids for e in ids_keep]):\n",
    "        outfolder = os.path.dirname(outfile)\n",
    "        if not os.path.exists(outfolder):\n",
    "            os.makedirs(outfolder)\n",
    "        fout = open(outfile,'w')\n",
    "        for e in seqs:\n",
    "            fout.write('>'+e.id+'\\n'+str(e.seq)+'\\n')\n",
    "        fout.close()\n",
    "\n",
    "parameters = []\n",
    "for outgroup in outgroups:\n",
    "    outfolder = folder+'seqs5'+outgroup\n",
    "    ids_keep = target_species.copy()\n",
    "    ids_keep.append(outgroup)\n",
    "    for file in files:\n",
    "        outfile = os.path.join(outfolder,os.path.basename(file))\n",
    "        parameters.append([file,ids_keep,outfile])\n",
    "\n",
    "pool = Pool(threads)\n",
    "pool.starmap(filterSeqs,parameters)\n",
    "pool.close()\n",
    "\n",
    "#generate scripts for mrbayes\n",
    "folders = [folder+'seqs5'+outgroup for outgroup in outgroups]\n",
    "files = []\n",
    "for f in folders:\n",
    "    files += glob.glob(f+'/*')\n",
    "\n",
    "cmds = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/cmds/cmds'\n",
    "open(cmds,'w').write('\\n'.join(['python3 /home2/s185491/p/xiaolongTools/project/Junonia/runMrBayes.py -i '+f for f in files]))\n",
    "os.system('python /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -i /work/biophysics/s185491/2018junonia/20190226GeneTrees/cmds/cmds -N 32')\n",
    "\n",
    "txt = '''#!/bin/bash\n",
    "#SBATCH --job-name=0\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 3 /work/biophysics/s185491/2018junonia/20190226GeneTrees/cmds/cmds.split{n}\n",
    "\n",
    "'''\n",
    "for n in range(32):\n",
    "    open('/work/biophysics/s185491/2018junonia/20190226GeneTrees/cmds/cmds.qsub'+str(n),'w').write(txt.format(n=n))\n",
    "\n",
    "\n",
    "# collect data\n",
    "folder_up = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/'\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "for outgroup in outgroups:\n",
    "    folder = folder_up+'seqs5'+outgroup+'Newick/'\n",
    "    files = glob.glob(folder +'*')\n",
    "    outfile = '/work/biophysics/s185491/2018junonia/20190226GeneTrees/20190304Trees5{outgroup}.txt'.format(outgroup=outgroup)\n",
    "    fout = open(outfile,'w')\n",
    "    for f in files:\n",
    "        fout.write('{gene_id}:{tree}\\n'.format(gene_id = os.path.basename(f), tree=open(f).read()))\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "folder = '/home/xcao/w/20180905Junonia_coenia/20190131New/20190225geneTree/'\n",
    "outgroups = 'J_neildi J_MEXICANspecies J_vestina J_wMex J_z_zonalis'.split()\n",
    "target_species = ['J_c_coenia', 'J_c_grisea', 'J_nigrosuffusa', 'J_nigrosuffusaTX']\n",
    "file_geneinfo = '/home/xcao/w/genomes/Junonia_coenia/20190301Junonia_gene_info.tsv'\n",
    "\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "dc_file = {e:os.path.join(folder,'20190304Trees5{outgroup}.txt'.format(outgroup=e)) for e in outgroups}\n",
    "\n",
    "\n",
    "\n",
    "def getTopology(tree_txt,target_species,outgroup):\n",
    "    '''\n",
    "    given a tree_txt, return a tree_txt by re-root the tree with outgroup, set outgroup name to 'outgroup', and order leaves\n",
    "    if some species is missing, return None\n",
    "    '''\n",
    "    species = target_species.copy()\n",
    "    species.append(outgroup)\n",
    "    \n",
    "    tree = Tree(tree_txt)\n",
    "    if not all([leaf in tree.get_leaf_names() for leaf in species]):\n",
    "        return None\n",
    "    for node in tree.iter_leaves():\n",
    "        if node.name == outgroup:\n",
    "            node.name = 'O'\n",
    "    \n",
    "    tree.set_outgroup('O')\n",
    "    tree.sort_descendants()\n",
    "    return tree.write(format=9)\n",
    "\n",
    "\n",
    "#get a df store the tree_txt\n",
    "df = pd.DataFrame()\n",
    "for outgroup, filename in dc_file.items():\n",
    "    for line in open(filename):\n",
    "        gene, tree_txt = line.strip().split(':',1)\n",
    "        df.loc[gene,outgroup] = getTopology(tree_txt,target_species, outgroup)\n",
    "\n",
    "df = df.sort_index()\n",
    "df = df.applymap(str)\n",
    "\n",
    "#change 'nan' to 'noSeq', and non-binary trees to 'badTop'\n",
    "def cleanTree(tree_txt):\n",
    "    '''\n",
    "    change 'nan' to 'noSeq'\n",
    "    non-binary tree to 'badTop'\n",
    "    '''\n",
    "    if tree_txt == 'nan':\n",
    "        return 'noSeq'\n",
    "    tree = Tree(tree_txt)\n",
    "    for node in tree.traverse():\n",
    "        if not node.is_leaf():\n",
    "            if len(node.get_children()) != 2:\n",
    "                return 'badTop'\n",
    "    return tree_txt\n",
    "\n",
    "df1 = df.applymap(cleanTree)\n",
    "\n",
    "topologies = df.values.flatten()\n",
    "topologies = np.unique(topologies,return_counts=True)\n",
    "\n",
    "topologies1 = np.unique(df1.values.flatten(), return_counts=True)\n",
    "ls_topology = list(zip(topologies1[0], topologies1[1]))\n",
    "ls_topology = [e for e in ls_topology if e[0] not in ['badTop','noSeq']]\n",
    "ls_topology.sort(key=lambda x:x[1],reverse=True)\n",
    "dc_topology = {}\n",
    "for n,top in enumerate(ls_topology):\n",
    "    dc_topology[top[0]] = n+1\n",
    "\n",
    "def topology2number(topology):\n",
    "    '''\n",
    "    given a topology txt, return a number based on dc_topology\n",
    "    '''\n",
    "    if topology in dc_topology:\n",
    "        return dc_topology[topology]\n",
    "    return None\n",
    "\n",
    "for outgroup in outgroups:\n",
    "    df1['N_'+outgroup] = df1[outgroup].apply(topology2number)\n",
    "\n",
    "df_gene = pd.read_csv(file_geneinfo,sep='\\t',index_col=0)\n",
    "df_gene = df_gene.set_index('gene')\n",
    "\n",
    "df1 = df1.join(df_gene,how='left')\n",
    "df1 = df1.sort_values(by=['gene_order'])\n",
    "\n",
    "def getlowestBootstrap(tree_txt):\n",
    "    '''\n",
    "    return the lowest bootstrap value for a tree\n",
    "    '''\n",
    "    supports = []\n",
    "    tree = Tree(tree_txt)\n",
    "    for t in tree.traverse():\n",
    "        if t.support >1:\n",
    "            supports.append(t.support)\n",
    "    if len(supports) != 0:\n",
    "        return min(supports)\n",
    "    return 0\n",
    "\n",
    "for outgroup, filename in dc_file.items():\n",
    "    for line in open(filename):\n",
    "        gene, tree_txt = line.strip().split(':',1)\n",
    "        if df1.loc[gene,outgroup] not in ['badTop','noSeq']:\n",
    "            df1.loc[gene,outgroup +'_minBoots'] = getlowestBootstrap(tree_txt)\n",
    "        \n",
    "df1.to_excel('/home/xcao/w/20180905Junonia_coenia/20190131New/20190225geneTree/20190304Trees5.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20190307 check each sample, scan window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split scaffold to fragments about 100kb\n",
    "```\n",
    "\n",
    "filename = '/home/xcao/w/genomes/Junonia_coenia/20181005JunoniaPositionHeliconiusChrOrder.tab'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(filename,sep='\\t',header=None)\n",
    "df.columns = ['chr','scf','start','end']\n",
    "df['scf_len'] = df['end'] - df['start']\n",
    "\n",
    "fraglenlimit = 100000\n",
    "getFragCount = lambda x:int(np.ceil(x/fraglenlimit))\n",
    "df['frag_count'] = df['scf_len'].apply(getFragCount)\n",
    "df['frag_step'] = df.apply(lambda row:int(np.ceil(row['scf_len'] / row['frag_count'])), axis='columns')\n",
    "#remove scf shorter than half of fraglenlimit\n",
    "df = df[df['scf_len'] >= fraglenlimit/2]\n",
    "\n",
    "\n",
    "ls_frag = []\n",
    "for r, row in df.iterrows():\n",
    "    for n in range(row['start'], row['end'], row['frag_step']):\n",
    "        ls_frag.append( [row['chr'], row['scf'], n, min(n + row['frag_step'], row['end'])])\n",
    "\n",
    "df_frag = pd.DataFrame(ls_frag, columns = ['chr','scf','start','end'])\n",
    "\n",
    "dc_frag = {}\n",
    "for r, row in df_frag.iterrows():\n",
    "    chromosome = row['chr']\n",
    "    scf = row['scf']\n",
    "    key = f'{r}_chr{chromosome}_{scf}'\n",
    "    dc_frag[key] = range(row['start'],row['end'])\n",
    "\n",
    "df_frag.to_csv('/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/scf_frag100000.csv',sep='\\t')\n",
    "pickle.dump(dc_frag, open('/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/map.sitesInfo/scf_frag100000','wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re-order SNPs in two strand of SNP call so that strand0 is more similar to its species (most-common bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import numpy as np\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "J_c_coenia=['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "J_c_grisea=['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "J_neildi=['10385', '5332', '8206', '10350', '4806', '8205', '10396', '4818', '5330', '15112B02', '15101F01', '15117E05', '15117E06', '6652', '5327', '16106C08', '15112A11', '5325', '6650', '6651', '5742', '5750']\n",
    "J_nigrosuffusa=['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "J_nigrosuffusaTX=['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "J_z_zonalis=['8145', '8146', '8143', '10296', '8164', '8148', '8165', '8216', '8214', '10538', '8174', '10249', '7156', '15101E06', '7158', '15117E08', '7157', '7153', '16106C06', '15112B11', '7101', '6657', '7155', '5455', '7103', '7075', '5706', '5487']\n",
    "J_wMex=['15112A12', '15112B01', '7080', '15117E11', '15117E10', '7076', '7077', '7079']\n",
    "J_neildiTX=['15117F02', '5709', '5500', '5387', '5390', '5392', '5437', '6801', '6804', '6817']\n",
    "J_MEXICANspecies=['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "\n",
    "dc = {}\n",
    "dc['J_c_coenia'] = J_c_coenia\n",
    "dc['J_c_grisea'] = J_c_grisea\n",
    "dc['J_neildi'] = J_neildi\n",
    "dc['J_nigrosuffusa'] = J_nigrosuffusa\n",
    "dc['J_nigrosuffusaTX'] = J_nigrosuffusaTX\n",
    "dc['J_z_zonalis'] = J_z_zonalis\n",
    "dc['J_wMex'] = J_wMex\n",
    "dc['J_neildiTX'] = J_neildiTX\n",
    "dc['J_MEXICANspecies'] = J_MEXICANspecies\n",
    "\n",
    "dc_file = {}\n",
    "for species in dc:\n",
    "    dc_file[species] = '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/merged/'+species+'.map'\n",
    "    for sample in dc[species]:\n",
    "        dc_file[sample] = ['/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand0/'+sample+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map', '/home/xcao/w/20180905Junonia_coenia/20180919Info/HelperFileMapInNumpyArray/strand1/'+sample+'_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map']\n",
    "\n",
    "params = []\n",
    "for species in dc:\n",
    "    for sample in dc[species]:\n",
    "        file_ref = dc_file[species]\n",
    "        file_query1, file_query2 = dc_file[sample]\n",
    "        file_out1 = file_query1.replace('/strand0/','/bias0/')\n",
    "        file_out2 = file_query2.replace('/strand1/','/bias1/')\n",
    "        params.append([file_ref, file_query1, file_query2,file_out1,file_out2])\n",
    "\n",
    "def biasSeq(file_ref, file_query1, file_query2,file_out1,file_out2):\n",
    "    '''\n",
    "    re-arrange SNPs in file_query1/2, so that file_query1 is more similar to file_ref\n",
    "    '''\n",
    "    np_ref = pickle.load(open(file_ref,'rb'))\n",
    "    np_q1 = pickle.load(open(file_query1,'rb'))\n",
    "    np_q2 = pickle.load(open(file_query2,'rb'))\n",
    "    changed_sites = 0\n",
    "    for n in range(len(np_ref)):\n",
    "        r,q1,q2 = np_ref[n], np_q1[n], np_q2[n]\n",
    "        if q2 == r and q1 != r:\n",
    "            np_q1[n] = q2\n",
    "            np_q2[n] = q1\n",
    "            changed_sites += 1\n",
    "    print(os.path.basename(file_query1).split('_')[0], 'SNP changed', changed_sites,)\n",
    "    pickle.dump(np_q1, open(file_out1,'wb'))\n",
    "    pickle.dump(np_q2, open(file_out2,'wb'))\n",
    "    return None\n",
    "\n",
    "pool = Pool(32)\n",
    "pool.starmap(biasSeq, params)\n",
    "pool.close()\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sequences for each sample\n",
    "transfer file to bioHPC\n",
    "```\n",
    "\n",
    "\n",
    "#transfer files to bioHPC\n",
    "import sys\n",
    "sys.path.append('/home/xcao/p/xiaolongTools/utils/mapFile')\n",
    "import numpy as np\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "J_c_coenia=['3935', '8142', '8215', '4256', '8340', '4879', '4758', '5256', '5258', '5235', '8163', '5417', '5436', '5422', '6862', '7171', '5490', '5707', '7579']\n",
    "J_c_grisea=['6049', '6048', '6704', 'PAO33', '8278', '5989', '5962', '8266', '15105C07', '15101E07', '5704', '5582', '5705', '5583', '5517']\n",
    "J_neildi=['10385', '5332', '8206', '10350', '4806', '8205', '10396', '4818', '5330', '15112B02', '15101F01', '15117E05', '15117E06', '6652', '5327', '16106C08', '15112A11', '5325', '6650', '6651', '5742', '5750']\n",
    "J_nigrosuffusa=['15117D12', '5950', '5982', '15117E01', '5990', '5667', '5993', '15117E04', '5668', '15117F07', '15117E03', '16106B05', '16106B06', '16106B09', '16106B07', '5669', '15101E04', '15117E02', '15112B03', '15101E05']\n",
    "J_nigrosuffusaTX=['5397', '5438', '5581', '4507', '5409', '5470', '5424', '5451', '6658']\n",
    "J_z_zonalis=['8145', '8146', '8143', '10296', '8164', '8148', '8165', '8216', '8214', '10538', '8174', '10249', '7156', '15101E06', '7158', '15117E08', '7157', '7153', '16106C06', '15112B11', '7101', '6657', '7155', '5455', '7103', '7075', '5706', '5487']\n",
    "J_wMex=['15112A12', '15112B01', '7080', '15117E11', '15117E10', '7076', '7077', '7079']\n",
    "J_neildiTX=['15117F02', '5709', '5500', '5387', '5390', '5392', '5437', '6801', '6804', '6817']\n",
    "J_MEXICANspecies=['5477', '16106B11', '16106B12', '15113B10', '15117F06', '7083', '7086', '5492', '5516', '5476', '16106C01', '16106C03', '5708', '7087']\n",
    "\n",
    "dc = {}\n",
    "dc['J_c_coenia'] = J_c_coenia\n",
    "dc['J_c_grisea'] = J_c_grisea\n",
    "dc['J_neildi'] = J_neildi\n",
    "dc['J_nigrosuffusa'] = J_nigrosuffusa\n",
    "dc['J_nigrosuffusaTX'] = J_nigrosuffusaTX\n",
    "dc['J_z_zonalis'] = J_z_zonalis\n",
    "dc['J_wMex'] = J_wMex\n",
    "dc['J_neildiTX'] = J_neildiTX\n",
    "dc['J_MEXICANspecies'] = J_MEXICANspecies\n",
    "\n",
    "sys.path.append('/home2/s185491/p/xiaolongTools/utils/mapFile')\n",
    "params = []\n",
    "threads = 56\n",
    "headerFun=lambda x:x.split('_')[0]\n",
    "\n",
    "def headerFun(x):\n",
    "    if 'Junonia_coenia_JC' in x:\n",
    "        return os.path.basename(x).split('_')[0]\n",
    "    else:\n",
    "        return os.path.basename(x).replace('.map','')\n",
    "\n",
    "npFilter='/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/scf_frag100000'\n",
    "minlen= 60000\n",
    "minseq = 6\n",
    "npInt8s = []\n",
    "for species in dc:\n",
    "    npInt8s.append('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/merged/{species}.map'.format(species=species))\n",
    "for species in dc:\n",
    "    for sample in dc[species]:\n",
    "        npInt8s_0 = npInt8s.copy()\n",
    "        npInt8s_1 = npInt8s.copy()\n",
    "        npInt8s_0.append('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/bias0/{}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(sample))\n",
    "        npInt8s_1.append('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/bias1/{}_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.map'.format(sample))\n",
    "        outputfolder_0= os.path.join('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/samples/',sample+'_0')\n",
    "        outputfolder_1= os.path.join('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/samples/',sample+'_1')\n",
    "        params.append([npInt8s_0, threads, outputfolder_0, headerFun, npFilter, minlen, minseq, [sample]])\n",
    "        params.append([npInt8s_1, threads, outputfolder_1, headerFun, npFilter, minlen, minseq, [sample]])\n",
    "\n",
    "import mapfileIO\n",
    "\n",
    "mapfileIO.npInt8stoMultipleFastas(*params[0])\n",
    "mapfileIO.npInt8stoMultipleFastas(*params[1])\n",
    "\n",
    "mapfileIO.npInt8stoMultipleFastas(*params[2])\n",
    "mapfileIO.npInt8stoMultipleFastas(*params[3])\n",
    "\n",
    "for n in range(4,20):\n",
    "    mapfileIO.npInt8stoMultipleFastas(*params[n])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run RAxML\n",
    "```\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folders = glob.glob('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/samples/*')\n",
    "folders = [e for e in folders if not any(e.endswith(s) for s in ['MrBayes', 'Newick', 'Temp'])]\n",
    "\n",
    "files = []\n",
    "for folder in folders:\n",
    "    files += glob.glob(folder+'/*')\n",
    "\n",
    "folder_cmd = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/cmds/'\n",
    "open(os.path.join(folder_cmd,'cmds'),'w').write('\\n'.join('python3 /home2/s185491/p/xiaolongTools/utils/runRAxMLForIndividualSmallFiles.py -T 2 -N 50 -i '+e for e in files))\n",
    "\n",
    "os.system('python /home2/s185491/p/xiaolongTools/utils/splitFiles2Nparts.py -i /work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/cmds/cmds -N 32')\n",
    "\n",
    "txt = '''#!/bin/bash\n",
    "#SBATCH --job-name=0\n",
    "#SBATCH --partition=32GB\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --output=0.%j.out\n",
    "#SBATCH --mail-user=xiaolong.cao@utsouthwestern.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module load openmpi/intel/3.1.1\n",
    "module load beagle-lib/2.1.2\n",
    "\n",
    "python3 /home2/s185491/p/xiaolongTools/multiThread.py 16 /work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/cmds/cmds.split{n}\n",
    "'''\n",
    "for n in range(32):\n",
    "    open(os.path.join(folder_cmd,'qsub_cmds'+str(n)),'w').write(txt.format(n=n))\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folders = glob.glob('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/samples/*.RAxMLbestTree')\n",
    "folders2 = [e.replace('.RAxMLbestTree','') for e in folders]\n",
    "for folder in folders:\n",
    "    files = glob.glob(os.path.join(folder,'*'))\n",
    "    fout = open('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/trees/'+os.path.basename(folder).replace('.RAxMLbestTree',''), 'w')\n",
    "    for file in files:\n",
    "        fout.write(os.path.basename(file)+'\\t')\n",
    "        fout.write(open(file).read())\n",
    "    fout.close()\n",
    "\n",
    "for folder1,folder2 in zip(folders,folders2):\n",
    "    os.system('rm -rf '+folder1+'&')\n",
    "    os.system('rm -rf '+folder2+'&')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "file_scf = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/scf_frag100000.csv'\n",
    "folder_trees = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/trees/'\n",
    "file_order = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/sample_order.txt'\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from ete3 import Tree\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(folder_trees,'*'))\n",
    "species = '''J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_MEXICANspecies J_z_zonalis J_neildi J_neildiTX J_wMex'''.split()\n",
    "dc_species = {e:n for n,e in enumerate(species)}\n",
    "\n",
    "df = pd.read_csv(file_scf,sep='\\t',index_col=0)\n",
    "\n",
    "def processFile(file,dc_species=dc_species, treeCountMin=1000):\n",
    "    '''\n",
    "    return a library name and a dictionary of with tree_id as key, and the closest species number (defined in dc_species) as value\n",
    "    if the number of trees is less than treeCountMin, return None\n",
    "    '''\n",
    "    lines = open(file).readlines()\n",
    "    if len(lines) < treeCountMin:\n",
    "        return None\n",
    "    \n",
    "    lib = os.path.basename(file)\n",
    "    sample = lib.split('_')[0]\n",
    "    \n",
    "    dc = {}\n",
    "    for line in lines:\n",
    "        key, tree_txt = line.strip().split('\\t')\n",
    "        key = key.split('_')[0]\n",
    "        tree = Tree(tree_txt)\n",
    "        leaves = [e for e in tree.iter_leaf_names() if e != sample]\n",
    "        distances = []\n",
    "        for leaf in leaves:\n",
    "            distances.append([leaf,tree.get_distance(sample, leaf)])\n",
    "            distances.sort(key=lambda x:x[1])\n",
    "            dc[key] = dc_species[distances[0][0]]\n",
    "    \n",
    "    return lib, dc\n",
    "\n",
    "pool = Pool(32)\n",
    "ls_results = pool.map(processFile,files)\n",
    "pool.close()\n",
    "\n",
    "ls_results = [e for e in ls_results if e is not None]\n",
    "\n",
    "for lib, dc in ls_results:\n",
    "    df[lib] = np.array([dc[str(e)] if str(e) in dc else np.nan for e in df.index])\n",
    "\n",
    "l = open(file_order).read().split()\n",
    "dc_order = {e:n for n,e in enumerate(l)}\n",
    "\n",
    "\n",
    "def getOrder(k):\n",
    "    if k.endswith('_0'):\n",
    "        return dc_order[k.split('_')[0]]\n",
    "    elif k.endswith('_1'):\n",
    "        return dc_order[k.split('_')[0]]+0.5\n",
    "    return -1\n",
    "\n",
    "columns = list(df.columns[4:])\n",
    "columns.sort(key=lambda x:getOrder(x))\n",
    "df1 = df[list(df.columns[:4])+columns]\n",
    "df1.to_excel('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/20190311SampleFragSpecies.xlsx')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hybrids\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "filename = '/work/biophysics/s185491/2018junonia/20190312HybridsVCF/18098H11_Junonia_coenia_JC_v1.0.scaffolds_snp_step2.vcf.gz'\n",
    "\n",
    "import vcf\n",
    "vcf_reader = vcf.Reader(filename=filename)\n",
    "n = 0\n",
    "records = []\n",
    "for record in vcf_reader:\n",
    "    records.append(record)\n",
    "    n += 1\n",
    "    print(record)\n",
    "    if n >100:\n",
    "        break\n",
    "\n",
    "file_scf = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/scf_frag100000.csv'\n",
    "folder_trees = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/trees/'\n",
    "file_order = '/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/sample_order.txt'\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from ete3 import Tree\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(folder_trees,'*'))\n",
    "J_new = ['18098H11', '18098H12', '18126E01', '18126E02', '18126E03', '18126E04', '18126E05', '18126E06', '18126E07', '18126E08', '18126E09']\n",
    "files = [os.path.join(folder_trees,e) + '_0' for e in J_new] + [os.path.join(folder_trees,e) + '_1' for e in J_new]\n",
    "species = '''J_c_coenia J_c_grisea J_nigrosuffusa J_nigrosuffusaTX J_MEXICANspecies J_z_zonalis J_neildi J_neildiTX J_wMex'''.split()\n",
    "dc_species = {e:n for n,e in enumerate(species)}\n",
    "\n",
    "df = pd.read_csv(file_scf,sep='\\t',index_col=0)\n",
    "\n",
    "def processFile(file,dc_species=dc_species, treeCountMin=1000):\n",
    "    '''\n",
    "    return a library name and a dictionary of with tree_id as key, and the closest species number (defined in dc_species) as value\n",
    "    if the number of trees is less than treeCountMin, return None\n",
    "    '''\n",
    "    lines = open(file).readlines()\n",
    "    if len(lines) < treeCountMin:\n",
    "        return None\n",
    "\n",
    "    lib = os.path.basename(file)\n",
    "    sample = lib.split('_')[0]\n",
    "\n",
    "    dc = {}\n",
    "    for line in lines:\n",
    "        key, tree_txt = line.strip().split('\\t')\n",
    "        key = key.split('_')[0]\n",
    "        tree = Tree(tree_txt)\n",
    "        leaves = [e for e in tree.iter_leaf_names() if e != sample]\n",
    "        distances = []\n",
    "        for leaf in leaves:\n",
    "            distances.append([leaf,tree.get_distance(sample, leaf)])\n",
    "            distances.sort(key=lambda x:x[1])\n",
    "            dc[key] = dc_species[distances[0][0]]\n",
    "\n",
    "    return lib, dc\n",
    "\n",
    "pool = Pool(32)\n",
    "ls_results = pool.map(processFile,files)\n",
    "pool.close()\n",
    "\n",
    "ls_results = [e for e in ls_results if e is not None]\n",
    "\n",
    "for lib, dc in ls_results:\n",
    "    df[lib] = np.array([dc[str(e)] if str(e) in dc else np.nan for e in df.index])\n",
    "\n",
    "l = open(file_order).read().split()\n",
    "dc_order = {e:n for n,e in enumerate(l)}\n",
    "\n",
    "\n",
    "def getOrder(k):\n",
    "    if k.endswith('_0'):\n",
    "        return dc_order[k.split('_')[0]]\n",
    "    elif k.endswith('_1'):\n",
    "        return dc_order[k.split('_')[0]]+0.5\n",
    "    return -1\n",
    "\n",
    "columns = list(df.columns[4:])\n",
    "columns.sort()\n",
    "df1 = df[list(df.columns[:4])+columns]\n",
    "df1.to_excel('/work/biophysics/s185491/2018junonia/20190307SampleGeneTrees/20190314HybridsFragSpecies.grisea.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "362px",
    "width": "319px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "553px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
